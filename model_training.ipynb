{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4512677c",
   "metadata": {},
   "source": [
    "### Template notebook for finetuning MaskDINO on different datasets.\n",
    "\n",
    "Notebook to finetune MaskDINO to detect and segment object classes relevant for the Storing Groceries task at RoboCup@Home2023. To finetune for other tasks, just select relevant datasets and labels as exemplified here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dae044a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "import fiftyone.zoo as foz\n",
    "from fiftyone import ViewField as F\n",
    "\n",
    "import torch\n",
    "import detectron2\n",
    "from detectron2.utils.logger import setup_logger\n",
    "\n",
    "setup_logger()\n",
    "\n",
    "# import some common libraries\n",
    "import numpy as np\n",
    "import os, shutil, json, random, cv2\n",
    "from datetime import datetime\n",
    "\n",
    "# import some common detectron2 utilities\n",
    "from detectron2.structures import BoxMode\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from detectron2.data import MetadataCatalog, DatasetCatalog\n",
    "from detectron2.projects.deeplab import add_deeplab_config\n",
    "#from detectron2.data.datasets import register_coco_instances\n",
    "\n",
    "import sys\n",
    "sys.path.append('contrib/MaskDINO')\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "from helpers.util import get_timestamp, select_classes, merge_to_superclass, concat_datasets, make_archive\n",
    "from helpers.fiftyone_detectron2_bridge import clean_instances, detectron_to_fo, get_fiftyone_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49e206c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = 'example_detector'\n",
    "model_type = 'swin' #'r50', 'swin' -> r50 uses fewer resources, swin is more performant"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df11210",
   "metadata": {},
   "source": [
    "#### Dataset creation\n",
    "\n",
    "Usually we want to add the base dataset from Bonn or the day 1 dataset from Bordeaux, the older robotics hall dataset at AIS and probably also at least one more dataset created in Bordeaux. For some tasks it might be beneficial to add some of the auxiliary image search datasets as well. Locally it can make sense to select a different validation set as well. The available datasets and the object classes they cover are listed in *dataset_overview.ipynb*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a29be42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% |█████████████████| 100/100 [2.2s elapsed, 0s remaining, 44.6 samples/s]        \n",
      " 100% |███████████████████| 25/25 [653.9ms elapsed, 0s remaining, 38.3 samples/s]      \n",
      "Found train dataset labels:\n",
      "['apple', 'bin', 'chair', 'chocolate_pudding_box', 'cleanser_bottle', 'coffee_can', 'couch', 'couch_table', 'cupboard', 'dishwasher', 'door', 'dresser', 'fork', 'fridge', 'fridge_handle', 'jacket_stand', 'knife', 'lamp', 'mustard_bottle', 'person', 'pringles', 'rubiks_cube', 'shelf', 'sink', 'spoon', 'strawberry_gelatin_box', 'sugar_box', 'table', 'tv']\n",
      "Found valid dataset labels:\n",
      "['apple', 'bin', 'chair', 'chocolate_pudding_box', 'cleanser_bottle', 'coffee_can', 'couch', 'couch_table', 'cupboard', 'dishwasher', 'door', 'fork', 'knife', 'lamp', 'mustard_bottle', 'person', 'pringles', 'rubiks_cube', 'shelf', 'sink', 'spoon', 'strawberry_gelatin_box', 'sugar_box', 'table', 'tv']\n"
     ]
    }
   ],
   "source": [
    "# add the base dataset\n",
    "# name for the dataset\n",
    "name = \"robocup23_day1_coco\"\n",
    "\n",
    "# The directory containing the dataset to import\n",
    "data_dir = \"data/robocup_bordeaux_2023/robocup_data/coco_datasets/Bordeaux/bordeaux_day_1_coco\"\n",
    "labels_path = \"data/robocup_bordeaux_2023/robocup_data/coco_datasets/Bordeaux/bordeaux_day_1_coco/annotations\"\n",
    "train_dir = data_dir + '/train'\n",
    "valid_dir = data_dir + '/val'\n",
    "train_labels_path = labels_path + '/instances_Train.json'\n",
    "valid_labels_path = labels_path + '/instances_Validation.json'\n",
    "\n",
    "# setup the fiftyone dataset\n",
    "dataset_type = fo.types.COCODetectionDataset\n",
    "\n",
    "train_dataset = fo.Dataset.from_dir(\n",
    "    dataset_type=dataset_type,\n",
    "    data_path=train_dir,\n",
    "    labels_path=train_labels_path,\n",
    "    name=name+'_train',\n",
    ")\n",
    "\n",
    "valid_dataset = fo.Dataset.from_dir(\n",
    "    dataset_type=dataset_type,\n",
    "    data_path=valid_dir,\n",
    "    labels_path=valid_labels_path,\n",
    "    name=name+'_valid',\n",
    ")\n",
    "\n",
    "# most of the time we use all classes here and filter the total dataset at the end instead\n",
    "relevant_classes = ['apple', 'bin', 'chair', 'chocolate_pudding_box', 'cleanser_bottle', 'coffee_can', 'couch', \n",
    "                    'couch_table', 'cupboard', 'dishwasher', 'door', 'dresser', 'fork', 'fridge', 'fridge_handle', \n",
    "                    'jacket_stand', 'knife', 'lamp', 'mustard_bottle', 'person', 'pringles', 'rubiks_cube', 'shelf', \n",
    "                    'sink', 'spoon', 'strawberry_gelatin_box', 'sugar_box', 'table', 'tv']\n",
    "\n",
    "train_dataset = select_classes(train_dataset, labels=relevant_classes)\n",
    "valid_dataset = select_classes(valid_dataset, labels=relevant_classes)\n",
    "\n",
    "print('Found train dataset labels:')\n",
    "print(train_dataset.distinct(\"segmentations.detections.label\"))\n",
    "print('Found valid dataset labels:')\n",
    "print(valid_dataset.distinct(\"segmentations.detections.label\"))\n",
    "\n",
    "# merge training and validation data here (because at competition time we are greedy)\n",
    "# we will create a validation set further down the line\n",
    "combined_train_dataset = concat_datasets([train_dataset, valid_dataset])\n",
    "valid_dataset = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75dfe17e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% |█████████████████| 100/100 [1.3s elapsed, 0s remaining, 78.2 samples/s]         \n",
      " 100% |███████████████████| 50/50 [611.2ms elapsed, 0s remaining, 81.8 samples/s]      \n",
      "Adding dataset robocup23_local_furniture...\n",
      "Found train dataset labels:\n",
      "['bed', 'chair', 'couch', 'couch_table', 'cupboard', 'dishwasher', 'dishwasher_slot', 'dishwasher_tray', 'door', 'door_handle', 'dresser', 'fridge', 'fridge_handle', 'jacket_stand', 'lamp', 'microwave_oven', 'person', 'shelf', 'sink', 'table', 'tv']\n",
      "Found valid dataset labels:\n",
      "['bed', 'bin', 'chair', 'couch', 'couch_table', 'cupboard', 'dishwasher', 'dishwasher_slot', 'dishwasher_tray', 'door', 'door_handle', 'dresser', 'fridge', 'fridge_handle', 'jacket_stand', 'lamp', 'person', 'shelf', 'sink', 'table', 'tv']\n"
     ]
    }
   ],
   "source": [
    "# we continue by adding the dataset containing furniture from the arenas.\n",
    "# again use the validation set for training instead\n",
    "combined_train_dataset = combined_train_dataset\n",
    "\n",
    "USE_LOCAL_FURNITURE = True\n",
    "\n",
    "if USE_LOCAL_FURNITURE:\n",
    "    # add the older robotics hall dataset\n",
    "    name = \"robocup23_local_furniture\"\n",
    "\n",
    "    # The directory containing the dataset to import\n",
    "    data_dir = \"data/robocup_bordeaux_2023/robocup_data/coco_datasets/Bordeaux/arena_furniture_coco\"\n",
    "    labels_path = \"data/robocup_bordeaux_2023/robocup_data/coco_datasets/Bordeaux/arena_furniture_coco/annotations\"\n",
    "\n",
    "    train_dir = data_dir + '/train'\n",
    "    valid_dir = data_dir + '/val'\n",
    "    train_labels_path = labels_path + '/instances_Train.json'\n",
    "    valid_labels_path = labels_path + '/instances_Validation.json'\n",
    "\n",
    "    # The type of the dataset being imported\n",
    "    dataset_type = fo.types.COCODetectionDataset  # for example\n",
    "\n",
    "    train_dataset_add = fo.Dataset.from_dir(\n",
    "        dataset_type=dataset_type,\n",
    "        data_path=train_dir,\n",
    "        labels_path=train_labels_path,\n",
    "        name=name+'_train',\n",
    "    )\n",
    "\n",
    "    valid_dataset_add = fo.Dataset.from_dir(\n",
    "        dataset_type=dataset_type,\n",
    "        data_path=valid_dir,\n",
    "        labels_path=valid_labels_path,\n",
    "        name=name+'_valid',\n",
    "    )\n",
    "\n",
    "    print('Adding dataset ' + name + '...')\n",
    "    print('Found train dataset labels:')\n",
    "    print(train_dataset_add.distinct(\"segmentations.detections.label\"))\n",
    "    print('Found valid dataset labels:')\n",
    "    print(valid_dataset_add.distinct(\"segmentations.detections.label\"))\n",
    "\n",
    "    # most of the time we use all classes here and filter the total dataset at the end instead\n",
    "    relevant_classes = ['chair', 'couch', 'couch_table', 'cupboard', 'dishwasher', 'dishwasher_slot', \n",
    "                        'dishwasher_tray', 'door', 'door_handle', 'dresser', 'fridge', 'fridge_handle', 'jacket_stand', \n",
    "                        'lamp', 'person', 'shelf', 'sink', 'table', 'tv']\n",
    "\n",
    "    train_dataset_add = select_classes(train_dataset_add, labels=relevant_classes)\n",
    "    valid_dataset_add = select_classes(valid_dataset_add, labels=relevant_classes)\n",
    "\n",
    "    combined_train_dataset = concat_datasets([combined_train_dataset, train_dataset_add])\n",
    "    combined_train_dataset = concat_datasets([combined_train_dataset, valid_dataset_add])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "930f38be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% |█████████████████| 253/253 [5.2s elapsed, 0s remaining, 38.1 samples/s]      \n",
      " 100% |███████████████████| 32/32 [572.5ms elapsed, 0s remaining, 55.9 samples/s]      \n",
      "Adding dataset bordeaux_day2...\n",
      "Found train dataset labels:\n",
      "['apple', 'bag', 'banana', 'bed', 'bin', 'bowl', 'candle', 'cereals', 'chair', 'chocolate_pudding_box', 'cola', 'couch', 'couch_table', 'cracker_box', 'cupboard', 'decoration', 'dice', 'dishwasher', 'dishwasher_tray', 'door', 'door_handle', 'dresser', 'fridge', 'iced_tea', 'jacket_stand', 'juice_pack', 'lamp', 'lemon', 'milk_bottle', 'mustard_bottle', 'orange', 'orange_juice', 'peach', 'pear', 'person', 'plum', 'potted_meat_can', 'pringles', 'red_wine', 'rubiks_cube', 'shelf', 'shelf_door', 'sink', 'soccer_ball', 'sponge', 'strawberry', 'strawberry_gelatin_box', 'sugar_box', 'table', 'tennis_ball', 'tomato_soup_can', 'tray', 'tropical_juice', 'tuna_fish_can', 'tv']\n",
      "Found valid dataset labels:\n",
      "['banana', 'bed', 'bin', 'bowl', 'candle', 'cereals', 'chair', 'chocolate_pudding_box', 'cola', 'couch_table', 'cracker_box', 'cupboard', 'decoration', 'dishwasher', 'door', 'door_handle', 'fridge', 'iced_tea', 'jacket_stand', 'juice_pack', 'lamp', 'lemon', 'milk_bottle', 'mustard_bottle', 'orange', 'orange_juice', 'pear', 'person', 'potted_meat_can', 'red_wine', 'shelf', 'shelf_door', 'sponge', 'strawberry', 'strawberry_gelatin_box', 'sugar_box', 'table', 'tomato_soup_can', 'tray', 'tropical_juice', 'tuna_fish_can', 'tv']\n"
     ]
    }
   ],
   "source": [
    "# now we add our dataset from day 2 in Bordeaux\n",
    "# at this point we actually initialize the validation set cleanly\n",
    "combined_train_dataset = combined_train_dataset\n",
    "\n",
    "USE_DAY2 = True\n",
    "if USE_DAY2:\n",
    "    # add the older robotics hall dataset\n",
    "    name = \"bordeaux_day2\"\n",
    "\n",
    "    # The directory containing the dataset to import\n",
    "    data_dir = \"data/robocup_bordeaux_2023/robocup_data/coco_datasets/Bordeaux/bordeaux_day_2_coco\"\n",
    "    labels_path = \"data/robocup_bordeaux_2023/robocup_data/coco_datasets/Bordeaux/bordeaux_day_2_coco/annotations\"\n",
    "\n",
    "    train_dir = data_dir + '/train'\n",
    "    valid_dir = data_dir + '/val'\n",
    "    train_labels_path = labels_path + '/instances_Train.json'\n",
    "    valid_labels_path = labels_path + '/instances_Validation.json'\n",
    "\n",
    "    # The type of the dataset being imported\n",
    "    dataset_type = fo.types.COCODetectionDataset  # for example\n",
    "\n",
    "    train_dataset_add = fo.Dataset.from_dir(\n",
    "        dataset_type=dataset_type,\n",
    "        data_path=train_dir,\n",
    "        labels_path=train_labels_path,\n",
    "        name=name+'_train',\n",
    "    )\n",
    "    \n",
    "    valid_dataset_add = fo.Dataset.from_dir(\n",
    "    dataset_type=dataset_type,\n",
    "    data_path=valid_dir,\n",
    "    labels_path=valid_labels_path,\n",
    "    name=name+'_valid',\n",
    "    )\n",
    "\n",
    "    print('Adding dataset ' + name + '...')\n",
    "    print('Found train dataset labels:')\n",
    "    print(train_dataset_add.distinct(\"segmentations.detections.label\"))\n",
    "    print('Found valid dataset labels:')\n",
    "    print(valid_dataset_add.distinct(\"segmentations.detections.label\"))\n",
    "\n",
    "    # most of the time we use all classes here and filter the total dataset at the end instead\n",
    "    relevant_classes = ['apple', 'bag', 'banana', 'bed', 'bin', 'bowl', 'candle', 'cereals', 'chair',\n",
    "                        'chocolate_pudding_box', 'cola', 'couch', 'couch_table', 'cracker_box', 'cupboard', \n",
    "                        'decoration', 'dishwasher', 'dishwasher_tray', 'door', 'door_handle', 'dresser', 'fridge', \n",
    "                        'iced_tea', 'jacket_stand', 'juice_pack', 'lamp', 'lemon', 'milk_bottle', 'mustard_bottle', \n",
    "                        'orange', 'orange_juice', 'peach', 'pear', 'person', 'plum', 'potted_meat_can', 'pringles', \n",
    "                        'red_wine', 'rubiks_cube', 'shelf', 'shelf_door', 'sink', 'soccer_ball', 'sponge', 'strawberry', \n",
    "                        'strawberry_gelatin_box', 'sugar_box', 'table', 'tennis_ball', 'tomato_soup_can', 'tray', \n",
    "                        'tropical_juice', 'tuna_fish_can', 'tv']\n",
    "\n",
    "    train_dataset_add = select_classes(train_dataset_add, labels=relevant_classes)\n",
    "    valid_dataset_add = select_classes(valid_dataset_add, labels=relevant_classes)\n",
    "\n",
    "    combined_train_dataset = concat_datasets([combined_train_dataset, train_dataset_add])\n",
    "    combined_valid_dataset = valid_dataset_add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1dbd11de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% |█████████████████| 300/300 [8.8s elapsed, 0s remaining, 35.2 samples/s]      \n",
      " 100% |███████████████████| 30/30 [692.4ms elapsed, 0s remaining, 43.4 samples/s]      \n",
      "Adding dataset robocup23_ais_coco...\n",
      "Found train dataset labels:\n",
      "['almdudler', 'apple', 'apple_juice', 'bag', 'banana', 'baseball', 'booster', 'bowl', 'cereals', 'chair', 'chocolate_pudding_box', 'cleanser_bottle', 'coffee_can', 'coke', 'cracker_box', 'dice', 'dishwasher_tab', 'fork', 'golf_ball', 'iso_drink', 'knife', 'lemon', 'milk_bottle', 'milk_carton', 'mug', 'mustard_bottle', 'orange', 'orange_juice', 'peach', 'pear', 'person', 'plate', 'plum', 'potted_meat_can', 'pringles', 'racquet_ball', 'rubiks_cube', 'soccer_ball', 'soft_ball', 'spoon', 'strawberry', 'strawberry_gelatin_box', 'sugar_box', 'table', 'tennis_ball', 'tomato_soup_can', 'tuna_fish_can']\n",
      "Found valid dataset labels:\n",
      "['almdudler', 'apple', 'apple_juice', 'bag', 'banana', 'baseball', 'booster', 'bowl', 'cereals', 'chair', 'chocolate_pudding_box', 'cleanser_bottle', 'coffee_can', 'coke', 'cracker_box', 'dice', 'dishwasher_tab', 'fork', 'golf_ball', 'iso_drink', 'knife', 'lemon', 'milk_bottle', 'milk_carton', 'mug', 'mustard_bottle', 'orange', 'orange_juice', 'peach', 'pear', 'person', 'plate', 'plum', 'potted_meat_can', 'pringles', 'racquet_ball', 'rubiks_cube', 'soccer_ball', 'soft_ball', 'spoon', 'strawberry', 'strawberry_gelatin_box', 'sugar_box', 'table', 'tennis_ball', 'tomato_soup_can', 'tuna_fish_can']\n"
     ]
    }
   ],
   "source": [
    "# now we add a rather large dataset from Bonn. \n",
    "# here the orange juice looks a bit like the tropical juice at robocup, so we just remap that class name\n",
    "# the validation set is combined with the validation set from this particular dataset\n",
    "\n",
    "USE_AIS_OBJECTS = True\n",
    "if USE_AIS_OBJECTS:\n",
    "    # add the older robotics hall dataset\n",
    "    name = \"robocup23_ais_coco\"\n",
    "\n",
    "    # The directory containing the dataset to import\n",
    "    data_dir = \"data/robocup_bordeaux_2023/robocup_data/coco_datasets/Bonn/robocup23_ais_coco\"\n",
    "    labels_path = \"data/robocup_bordeaux_2023/robocup_data/coco_datasets/Bonn/robocup23_ais_coco/annotations\"\n",
    "\n",
    "    train_dir = data_dir + '/train'\n",
    "    valid_dir = data_dir + '/val'\n",
    "    train_labels_path = labels_path + '/instances_Train.json'\n",
    "    valid_labels_path = labels_path + '/instances_Validation.json'\n",
    "\n",
    "    dataset_type = fo.types.COCODetectionDataset  # for example\n",
    "\n",
    "    train_dataset_add = fo.Dataset.from_dir(\n",
    "        dataset_type=dataset_type,\n",
    "        data_path=train_dir,\n",
    "        labels_path=train_labels_path,\n",
    "        name=name+'_train',\n",
    "    )\n",
    "\n",
    "    valid_dataset_add = fo.Dataset.from_dir(\n",
    "        dataset_type=dataset_type,\n",
    "        data_path=valid_dir,\n",
    "        labels_path=valid_labels_path,\n",
    "        name=name+'_valid',\n",
    "    )\n",
    "\n",
    "    print('Adding dataset ' + name + '...')\n",
    "    print('Found train dataset labels:')\n",
    "    print(train_dataset_add.distinct(\"segmentations.detections.label\"))\n",
    "    print('Found valid dataset labels:')\n",
    "    print(valid_dataset_add.distinct(\"segmentations.detections.label\"))\n",
    "    \n",
    "\n",
    "\n",
    "    # most of the time we use all classes here and filter the total dataset at the end instead\n",
    "    relevant_classes = ['apple', 'bag', 'banana', 'baseball', 'bowl', 'cereals',\n",
    "                        'chair', 'chocolate_pudding_box', 'cleanser_bottle', 'coffee_can', 'cracker_box', 'dice', \n",
    "                        'dishwasher_tab', 'fork', 'knife', 'lemon', 'milk_bottle', 'mug', 'mustard_bottle', \n",
    "                        'orange', 'orange_juice', 'peach', 'pear', 'person', \n",
    "                        'plate', 'plum', 'potted_meat_can', 'pringles', 'rubiks_cube', 'soccer_ball', \n",
    "                        'spoon', 'strawberry', 'strawberry_gelatin_box', 'sugar_box', 'table', \n",
    "                        'tennis_ball', 'tomato_soup_can', 'tuna_fish_can']\n",
    "\n",
    "    train_dataset_add = select_classes(train_dataset_add, labels=relevant_classes)\n",
    "    valid_dataset_add = select_classes(valid_dataset_add, labels=relevant_classes)\n",
    "    \n",
    "    old_class_labels = ['orange_juice']\n",
    "    new_class_labels = ['tropical_juice']\n",
    "    train_dataset_add = merge_to_superclass(train_dataset_add, new_class_labels, old_class_labels)\n",
    "    valid_dataset_add = merge_to_superclass(valid_dataset_add, new_class_labels, old_class_labels)\n",
    "    \n",
    "    combined_train_dataset = concat_datasets([combined_train_dataset, train_dataset_add])\n",
    "    combined_valid_dataset = concat_datasets([combined_valid_dataset, valid_dataset_add])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad3565ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% |█████████████████| 132/132 [2.7s elapsed, 0s remaining, 54.1 samples/s]      \n",
      " 100% |███████████████████| 12/12 [250.5ms elapsed, 0s remaining, 48.2 samples/s]     \n",
      "Adding dataset difficult_objects...\n",
      "Found train dataset labels:\n",
      "['almdudler', 'apple', 'apple_juice', 'bowl', 'cereals', 'chair', 'coffee_can', 'coke', 'cracker_box', 'fork', 'iso_drink', 'knife', 'lemon', 'mug', 'peach', 'pear', 'plate', 'pringles', 'spoon', 'strawberry', 'strawberry_gelatin_box', 'sugar_box', 'table', 'tennis_ball', 'tomato_soup_can']\n",
      "Found valid dataset labels:\n",
      "['almdudler', 'apple_juice', 'bowl', 'cereals', 'coffee_can', 'coke', 'cracker_box', 'fork', 'iso_drink', 'knife', 'lemon', 'mug', 'mustard_bottle', 'peach', 'pear', 'spoon', 'strawberry', 'strawberry_gelatin_box', 'sugar_box', 'tomato_soup_can']\n"
     ]
    }
   ],
   "source": [
    "# this dataset contains some objects which we considered challenging after initial tests in Bonn\n",
    "\n",
    "USE_DIFFICULT_OBJECTS = True\n",
    "\n",
    "if USE_DIFFICULT_OBJECTS:\n",
    "    # add the older robotics hall dataset\n",
    "    name = \"difficult_objects\"\n",
    "\n",
    "    # The directory containing the dataset to import\n",
    "    data_dir = \"data/robocup_bordeaux_2023/robocup_data/coco_datasets/Bonn/difficult_objects_ais_coco\"\n",
    "    labels_path = \"data/robocup_bordeaux_2023/robocup_data/coco_datasets/Bonn/difficult_objects_ais_coco/annotations\"\n",
    "\n",
    "    train_dir = data_dir + '/train'\n",
    "    valid_dir = data_dir + '/val'\n",
    "    train_labels_path = labels_path + '/instances_Train.json'\n",
    "    valid_labels_path = labels_path + '/instances_Validation.json'\n",
    "\n",
    "    dataset_type = fo.types.COCODetectionDataset  # for example\n",
    "\n",
    "    train_dataset_add = fo.Dataset.from_dir(\n",
    "        dataset_type=dataset_type,\n",
    "        data_path=train_dir,\n",
    "        labels_path=train_labels_path,\n",
    "        name=name+'_train',\n",
    "    )\n",
    "\n",
    "    valid_dataset_add = fo.Dataset.from_dir(\n",
    "        dataset_type=dataset_type,\n",
    "        data_path=valid_dir,\n",
    "        labels_path=valid_labels_path,\n",
    "        name=name+'_valid',\n",
    "    )\n",
    "\n",
    "    print('Adding dataset ' + name + '...')\n",
    "    print('Found train dataset labels:')\n",
    "    print(train_dataset_add.distinct(\"segmentations.detections.label\"))\n",
    "    print('Found valid dataset labels:')\n",
    "    print(valid_dataset_add.distinct(\"segmentations.detections.label\"))\n",
    "\n",
    "    # most of the time we use all classes here and filter the total dataset at the end instead\n",
    "    relevant_classes = ['apple', 'bag', 'banana', 'baseball', 'bowl', 'cereals',\n",
    "                        'chair', 'chocolate_pudding_box', 'cleanser_bottle', 'coffee_can', 'cracker_box', 'dice', \n",
    "                        'dishwasher_tab', 'fork', 'knife', 'lemon', 'milk_bottle', 'mug', 'mustard_bottle', \n",
    "                        'orange', 'orange_juice', 'peach', 'pear', 'person', \n",
    "                        'plate', 'plum', 'potted_meat_can', 'pringles', 'rubiks_cube', 'soccer_ball', \n",
    "                        'spoon', 'strawberry', 'strawberry_gelatin_box', 'sugar_box', 'table', \n",
    "                        'tennis_ball', 'tomato_soup_can', 'tuna_fish_can']\n",
    "\n",
    "    train_dataset_add = select_classes(train_dataset_add, labels=relevant_classes)\n",
    "    valid_dataset_add = select_classes(valid_dataset_add, labels=relevant_classes)\n",
    "\n",
    "    # note that we freeze the validation dataset\n",
    "    combined_train_dataset = concat_datasets([combined_train_dataset, train_dataset_add])\n",
    "    combined_train_dataset = concat_datasets([combined_train_dataset, valid_dataset_add])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "865726bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% |█████████████████| 507/507 [10.5s elapsed, 0s remaining, 57.9 samples/s]      \n",
      " 100% |███████████████████| 30/30 [1.1s elapsed, 0s remaining, 28.5 samples/s]         \n",
      "Adding dataset hall_manyobject...\n",
      "Found train dataset labels:\n",
      "['almdudler', 'apple_juice', 'bag', 'bell_pepper', 'booster', 'bowl', 'brush', 'cereals', 'chair', 'chamomile_tea', 'chocolate_candy', 'coke', 'cookies', 'couch', 'cup', 'dishwasher_tab', 'fork', 'iso_drink', 'ketchup', 'kidney_bean_can', 'kleenex', 'knife', 'mustard', 'noodles_chicken_flavor', 'noodles_duck_flavor', 'orange_juice', 'person', 'plate', 'pringles', 'sparkling_water', 'sponge', 'spoon', 'table', 'tomato_can', 'tuc', 'windex', 'wipes']\n",
      "Found valid dataset labels:\n",
      "['almdudler', 'apple_juice', 'bag', 'bell_pepper', 'booster', 'bowl', 'brush', 'cereals', 'chair', 'chamomile_tea', 'chocolate_candy', 'coke', 'cookies', 'couch', 'cup', 'dishwasher_tab', 'fork', 'iso_drink', 'ketchup', 'kidney_bean_can', 'kleenex', 'knife', 'mustard', 'noodles_chicken_flavor', 'noodles_duck_flavor', 'orange_juice', 'person', 'plate', 'pringles', 'sparkling_water', 'sponge', 'spoon', 'table', 'tomato_can', 'tuc', 'windex', 'wipes']\n"
     ]
    }
   ],
   "source": [
    "# this is an older dataset, but its quite large and contains some useful objects\n",
    "# it does not contribute to the validation set\n",
    "\n",
    "USE_OLD_DATASET = True\n",
    "if USE_OLD_DATASET:\n",
    "    # add the older robotics hall dataset\n",
    "    name = \"hall_manyobject\"\n",
    "\n",
    "    # The directory containing the dataset to import\n",
    "    data_dir = \"data/robocup_bordeaux_2023/robocup_data/coco_datasets/Bonn/base_dataset_ais_coco\"\n",
    "    labels_path = \"data/robocup_bordeaux_2023/robocup_data/coco_datasets/Bonn/base_dataset_ais_coco/annotations\"\n",
    "\n",
    "    train_dir = data_dir + '/train'\n",
    "    valid_dir = data_dir + '/val'\n",
    "    train_labels_path = labels_path + '/instances_Train.json'\n",
    "    valid_labels_path = labels_path + '/instances_Validation.json'\n",
    "\n",
    "    # The type of the dataset being imported\n",
    "    dataset_type = fo.types.COCODetectionDataset  # for example\n",
    "\n",
    "    train_dataset_add = fo.Dataset.from_dir(\n",
    "        dataset_type=dataset_type,\n",
    "        data_path=train_dir,\n",
    "        labels_path=train_labels_path,\n",
    "        name=name+'_train',\n",
    "    )\n",
    "\n",
    "    valid_dataset_add = fo.Dataset.from_dir(\n",
    "        dataset_type=dataset_type,\n",
    "        data_path=valid_dir,\n",
    "        labels_path=valid_labels_path,\n",
    "        name=name+'_valid',\n",
    "    )\n",
    "\n",
    "    print('Adding dataset ' + name + '...')\n",
    "    print('Found train dataset labels:')\n",
    "    print(train_dataset_add.distinct(\"segmentations.detections.label\"))\n",
    "    print('Found valid dataset labels:')\n",
    "    print(valid_dataset_add.distinct(\"segmentations.detections.label\"))\n",
    "\n",
    "    # most of the time we use all classes here and filter the total dataset at the end instead\n",
    "    relevant_classes = ['bag', 'bowl', 'cereals', 'chair', 'cup', 'dishwasher_tab', 'fork', \n",
    "                        'knife', 'orange_juice', 'person', 'plate', 'pringles', 'spoon', 'table', \n",
    "                        'tomato_can', 'kidney_bean_can']\n",
    "\n",
    "    train_dataset_add = select_classes(train_dataset_add, labels=relevant_classes)\n",
    "    valid_dataset_add = select_classes(valid_dataset_add, labels=relevant_classes)\n",
    "    \n",
    "    # here tomato_can and kidney_bean_can are merged to tomato_soup_can\n",
    "    # the false-positive risk is low because tomato_soup was the only can relevant in Bordeaux\n",
    "    old_class_labels = ['orange_juice', 'cup', ['tomato_can', 'kidney_bean_can']]\n",
    "    new_class_labels = ['tropical_juice', 'mug', 'tomato_soup_can']\n",
    "    train_dataset_add = merge_to_superclass(train_dataset_add, new_class_labels, old_class_labels)\n",
    "    valid_dataset_add = merge_to_superclass(valid_dataset_add, new_class_labels, old_class_labels)\n",
    "\n",
    "    combined_train_dataset = concat_datasets([combined_train_dataset, train_dataset_add])\n",
    "    combined_train_dataset = concat_datasets([combined_train_dataset, valid_dataset_add])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "62c6e45b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% |███████████████████| 88/88 [2.8s elapsed, 0s remaining, 33.2 samples/s]        \n",
      " 100% |███████████████████| 10/10 [156.7ms elapsed, 0s remaining, 63.8 samples/s]     \n",
      "Adding dataset fruits...\n",
      "Training labels: ['apple', 'bag', 'banana', 'bell_pepper', 'chair', 'dishwasher', 'lemon', 'orange', 'orange_juice', 'peach', 'pear', 'person', 'plum', 'schweppes_lemon', 'strawberry', 'table']\n",
      "Validation labels: ['apple', 'banana', 'bell_pepper', 'chair', 'lemon', 'orange', 'orange_juice', 'pear', 'plum']\n",
      "\n",
      " 100% |███████████████████| 89/89 [1.3s elapsed, 0s remaining, 66.3 samples/s]         \n",
      " 100% |███████████████████| 11/11 [89.7ms elapsed, 0s remaining, 122.6 samples/s]     \n",
      "Adding dataset balls...\n",
      "Training labels: ['baseball', 'golf_ball', 'person', 'racquetball', 'soccer_ball', 'soft_ball', 'tennis_ball']\n",
      "Validation labels: ['baseball', 'golf_ball', 'soccer_ball', 'tennis_ball']\n",
      "\n",
      " 100% |███████████████████| 88/88 [1.2s elapsed, 0s remaining, 70.7 samples/s]          \n",
      " 100% |███████████████████| 10/10 [122.9ms elapsed, 0s remaining, 81.4 samples/s]     \n",
      "Adding dataset cereals...\n",
      "Training labels: ['cereals']\n",
      "Validation labels: ['cereals', 'strawberry']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# add some auxiliary datasets from image searches here\n",
    "# datasets are loaded, their labels merged and then filtered as defined below\n",
    "# they also do not contribute to validation\n",
    "\n",
    "aux_dataset_names = ['fruits', 'balls', 'cereals']\n",
    "merge_superclasses = [None, None, None]\n",
    "merge_labels = [None, None, None]\n",
    "use_classes = [['apple', 'lemon', 'orange', 'peach', 'pear', 'person', 'plum', 'strawberry'], \n",
    "               ['soccer_ball', 'baseball', 'tennis_ball', 'person'], \n",
    "               ['cereals', 'strawberry']]\n",
    "\n",
    "COMBINE = True\n",
    "if COMBINE:\n",
    "    ### load more datasets that are relevant\n",
    "    for i, dataset_name in enumerate(aux_dataset_names):\n",
    "        # The directory containing the dataset to import\n",
    "        data_dir_add = \"data/robocup_bordeaux_2023/robocup_data/coco_datasets/image_search/\" + dataset_name\n",
    "        labels_path_add = \"data/robocup_bordeaux_2023/robocup_data/coco_datasets/image_search/\" + dataset_name + \"/annotations\"\n",
    "\n",
    "        train_dir_add = data_dir_add + '/train'\n",
    "        valid_dir_add = data_dir_add + '/val'\n",
    "        train_labels_path_add = labels_path_add + '/instances_Train.json'\n",
    "        valid_labels_path_add = labels_path_add + '/instances_Validation.json'\n",
    "\n",
    "        train_dataset_add = fo.Dataset.from_dir(\n",
    "            dataset_type=dataset_type,\n",
    "            data_path=train_dir_add,\n",
    "            labels_path=train_labels_path_add,\n",
    "            name=dataset_name+'_train',\n",
    "        )\n",
    "\n",
    "        valid_dataset_add = fo.Dataset.from_dir(\n",
    "            dataset_type=dataset_type,\n",
    "            data_path=valid_dir_add,\n",
    "            labels_path=valid_labels_path_add,\n",
    "            name=dataset_name+'_valid',\n",
    "        )\n",
    "        \n",
    "        # summarize dataset being added\n",
    "        train_dataset_add_labels = train_dataset_add.distinct(\"segmentations.detections.label\")\n",
    "        valid_dataset_add_labels = valid_dataset_add.distinct(\"segmentations.detections.label\")\n",
    "        print('Adding dataset ' + dataset_name + '...')\n",
    "        print('Training labels:', train_dataset_add_labels)\n",
    "        print('Validation labels:', valid_dataset_add_labels)\n",
    "        print()\n",
    "        \n",
    "        # merge some classes as specified if necessary\n",
    "        train_dataset_add = merge_to_superclass(train_dataset_add, merge_superclasses[i], merge_labels[i])\n",
    "        valid_dataset_add = merge_to_superclass(valid_dataset_add, merge_superclasses[i], merge_labels[i])\n",
    "        # use only the classes we specified\n",
    "        train_dataset_add = select_classes(train_dataset_add, labels=use_classes[i])\n",
    "        valid_dataset_add = select_classes(valid_dataset_add, labels=use_classes[i])\n",
    "\n",
    "        # combine these with the previous dataset\n",
    "        combined_train_dataset = concat_datasets([combined_train_dataset, train_dataset_add])\n",
    "        combined_train_dataset = concat_datasets([combined_train_dataset, valid_dataset_add])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f6650bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in this cell we can further customize the dataset which we have constructed above\n",
    "# usually this means merging or filtering labels in a way which concerns all contributing datasets\n",
    "\n",
    "CUSTOMIZE = True\n",
    "if CUSTOMIZE:\n",
    "    # this renaming step ensures that we use the official class labels at RoboCup@Home23\n",
    "    old_class_labels = ['chocolate_pudding_box', 'cleanser_bottle', 'coffee_can', 'cracker_box', 'jacket_stand', 'mug', 'mustard_bottle', 'potted_meat_can', 'strawberry_gelatin_box', \n",
    "                        'sugar_box', 'tomato_soup_can', 'tuna_fish_can', 'milk_bottle']\n",
    "    new_class_labels = ['chocolate_jello', 'cleanser', 'coffee_grounds', 'cheezit', 'clothes_rack', 'cup', 'mustard', 'spam', 'strawberry_jello', \n",
    "                        'sugar', 'tomato_soup', 'tuna', 'milk']\n",
    "    train_dataset_processed = merge_to_superclass(combined_train_dataset, new_class_labels, old_class_labels)\n",
    "    valid_dataset_processed = merge_to_superclass(combined_valid_dataset, new_class_labels, old_class_labels)\n",
    "    \n",
    "    # finally constrain to classes relevant for task\n",
    "    task_specific_classes = ['apple', 'banana', 'baseball', 'cereals', 'chocolate_jello', 'cleanser', 'coffee_grounds', 'cola', \n",
    "                        'couch_table', 'cheezit', 'dice', 'fork', 'iced_tea', 'juice_pack', 'knife', 'lemon', 'milk', \n",
    "                        'peach', 'mustard', 'orange', 'orange_juice', 'pear', 'plum', 'spam', 'pringles', 'rubiks_cube', \n",
    "                        'soccer_ball', 'sponge', 'spoon', 'strawberry', 'strawberry_jello', 'sugar', 'tennis_ball', \n",
    "                        'tomato_soup', 'tropical_juice', 'tuna', 'shelf', 'shelf_door', 'red_wine']\n",
    "\n",
    "    train_dataset_processed = select_classes(train_dataset_processed, labels=task_specific_classes)\n",
    "    valid_dataset_processed = select_classes(valid_dataset_processed, labels=task_specific_classes)\n",
    "else:\n",
    "    train_dataset_processed = combined_train_dataset\n",
    "    valid_dataset_processed = combined_valid_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3afa8605",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label dictionary for complete dataset\n",
      "{'apple': 0, 'banana': 1, 'baseball': 2, 'cereals': 3, 'cheezit': 4, 'chocolate_jello': 5, 'cleanser': 6, 'coffee_grounds': 7, 'cola': 8, 'couch_table': 9, 'dice': 10, 'fork': 11, 'iced_tea': 12, 'juice_pack': 13, 'knife': 14, 'lemon': 15, 'milk': 16, 'mustard': 17, 'orange': 18, 'orange_juice': 19, 'peach': 20, 'pear': 21, 'plum': 22, 'pringles': 23, 'red_wine': 24, 'rubiks_cube': 25, 'shelf': 26, 'shelf_door': 27, 'soccer_ball': 28, 'spam': 29, 'sponge': 30, 'spoon': 31, 'strawberry': 32, 'strawberry_jello': 33, 'sugar': 34, 'tennis_ball': 35, 'tomato_soup': 36, 'tropical_juice': 37, 'tuna': 38}\n"
     ]
    }
   ],
   "source": [
    "# this cell bridges from fiftyone to detectron2 and allows sanity-checking the constructed dataset\n",
    "classes = train_dataset_processed.distinct(\"segmentations.detections.label\")\n",
    "labels_dict = {class_label: class_idx for class_idx, class_label in enumerate(classes)}\n",
    "print('Label dictionary for complete dataset')\n",
    "print(labels_dict)\n",
    "\n",
    "# dump labels for deployment\n",
    "os.makedirs('output', exist_ok=True)\n",
    "with open(\"output/classes.json\", \"w\") as fp:\n",
    "    json.dump(classes, fp)\n",
    "\n",
    "# bridge fiftyone to detectron2\n",
    "for dataset, tag in [(train_dataset_processed, \"train\"), (valid_dataset_processed, \"valid\")]:\n",
    "    view = dataset\n",
    "    if \"fiftyone_\" + tag in DatasetCatalog.list():\n",
    "        DatasetCatalog.remove(\"fiftyone_\" + tag)\n",
    "    DatasetCatalog.register(\"fiftyone_\" + tag, lambda view=view: get_fiftyone_dicts(view, labels_dict))\n",
    "    MetadataCatalog.get(\"fiftyone_\" + tag).set(thing_classes=classes, evaluator_type=\"coco\")\n",
    "metadata = MetadataCatalog.get(\"fiftyone_train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade7dd34",
   "metadata": {},
   "source": [
    "#### Detector Training\n",
    "\n",
    "Now that the task-specific dataset is ready, we can finetune a pretrained MaskDINO model on the selected data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9558f5dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/nogga/anaconda3/envs/athome23_detection/lib/python3.9/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3190.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "criterion.weight_dict  {'loss_ce': 4.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_bbox': 5.0, 'loss_giou': 2.0, 'loss_ce_interm': 4.0, 'loss_mask_interm': 5.0, 'loss_dice_interm': 5.0, 'loss_bbox_interm': 5.0, 'loss_giou_interm': 2.0, 'loss_ce_dn': 4.0, 'loss_mask_dn': 5.0, 'loss_dice_dn': 5.0, 'loss_bbox_dn': 5.0, 'loss_giou_dn': 2.0, 'loss_ce_interm_dn': 4.0, 'loss_mask_interm_dn': 5.0, 'loss_dice_interm_dn': 5.0, 'loss_bbox_interm_dn': 5.0, 'loss_giou_interm_dn': 2.0, 'loss_ce_0': 4.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_bbox_0': 5.0, 'loss_giou_0': 2.0, 'loss_ce_interm_0': 4.0, 'loss_mask_interm_0': 5.0, 'loss_dice_interm_0': 5.0, 'loss_bbox_interm_0': 5.0, 'loss_giou_interm_0': 2.0, 'loss_ce_dn_0': 4.0, 'loss_mask_dn_0': 5.0, 'loss_dice_dn_0': 5.0, 'loss_bbox_dn_0': 5.0, 'loss_giou_dn_0': 2.0, 'loss_ce_interm_dn_0': 4.0, 'loss_mask_interm_dn_0': 5.0, 'loss_dice_interm_dn_0': 5.0, 'loss_bbox_interm_dn_0': 5.0, 'loss_giou_interm_dn_0': 2.0, 'loss_ce_1': 4.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_bbox_1': 5.0, 'loss_giou_1': 2.0, 'loss_ce_interm_1': 4.0, 'loss_mask_interm_1': 5.0, 'loss_dice_interm_1': 5.0, 'loss_bbox_interm_1': 5.0, 'loss_giou_interm_1': 2.0, 'loss_ce_dn_1': 4.0, 'loss_mask_dn_1': 5.0, 'loss_dice_dn_1': 5.0, 'loss_bbox_dn_1': 5.0, 'loss_giou_dn_1': 2.0, 'loss_ce_interm_dn_1': 4.0, 'loss_mask_interm_dn_1': 5.0, 'loss_dice_interm_dn_1': 5.0, 'loss_bbox_interm_dn_1': 5.0, 'loss_giou_interm_dn_1': 2.0, 'loss_ce_2': 4.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_bbox_2': 5.0, 'loss_giou_2': 2.0, 'loss_ce_interm_2': 4.0, 'loss_mask_interm_2': 5.0, 'loss_dice_interm_2': 5.0, 'loss_bbox_interm_2': 5.0, 'loss_giou_interm_2': 2.0, 'loss_ce_dn_2': 4.0, 'loss_mask_dn_2': 5.0, 'loss_dice_dn_2': 5.0, 'loss_bbox_dn_2': 5.0, 'loss_giou_dn_2': 2.0, 'loss_ce_interm_dn_2': 4.0, 'loss_mask_interm_dn_2': 5.0, 'loss_dice_interm_dn_2': 5.0, 'loss_bbox_interm_dn_2': 5.0, 'loss_giou_interm_dn_2': 2.0, 'loss_ce_3': 4.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_bbox_3': 5.0, 'loss_giou_3': 2.0, 'loss_ce_interm_3': 4.0, 'loss_mask_interm_3': 5.0, 'loss_dice_interm_3': 5.0, 'loss_bbox_interm_3': 5.0, 'loss_giou_interm_3': 2.0, 'loss_ce_dn_3': 4.0, 'loss_mask_dn_3': 5.0, 'loss_dice_dn_3': 5.0, 'loss_bbox_dn_3': 5.0, 'loss_giou_dn_3': 2.0, 'loss_ce_interm_dn_3': 4.0, 'loss_mask_interm_dn_3': 5.0, 'loss_dice_interm_dn_3': 5.0, 'loss_bbox_interm_dn_3': 5.0, 'loss_giou_interm_dn_3': 2.0, 'loss_ce_4': 4.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_bbox_4': 5.0, 'loss_giou_4': 2.0, 'loss_ce_interm_4': 4.0, 'loss_mask_interm_4': 5.0, 'loss_dice_interm_4': 5.0, 'loss_bbox_interm_4': 5.0, 'loss_giou_interm_4': 2.0, 'loss_ce_dn_4': 4.0, 'loss_mask_dn_4': 5.0, 'loss_dice_dn_4': 5.0, 'loss_bbox_dn_4': 5.0, 'loss_giou_dn_4': 2.0, 'loss_ce_interm_dn_4': 4.0, 'loss_mask_interm_dn_4': 5.0, 'loss_dice_interm_dn_4': 5.0, 'loss_bbox_interm_dn_4': 5.0, 'loss_giou_interm_dn_4': 2.0, 'loss_ce_5': 4.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_bbox_5': 5.0, 'loss_giou_5': 2.0, 'loss_ce_interm_5': 4.0, 'loss_mask_interm_5': 5.0, 'loss_dice_interm_5': 5.0, 'loss_bbox_interm_5': 5.0, 'loss_giou_interm_5': 2.0, 'loss_ce_dn_5': 4.0, 'loss_mask_dn_5': 5.0, 'loss_dice_dn_5': 5.0, 'loss_bbox_dn_5': 5.0, 'loss_giou_dn_5': 2.0, 'loss_ce_interm_dn_5': 4.0, 'loss_mask_interm_dn_5': 5.0, 'loss_dice_interm_dn_5': 5.0, 'loss_bbox_interm_dn_5': 5.0, 'loss_giou_interm_dn_5': 2.0, 'loss_ce_6': 4.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_bbox_6': 5.0, 'loss_giou_6': 2.0, 'loss_ce_interm_6': 4.0, 'loss_mask_interm_6': 5.0, 'loss_dice_interm_6': 5.0, 'loss_bbox_interm_6': 5.0, 'loss_giou_interm_6': 2.0, 'loss_ce_dn_6': 4.0, 'loss_mask_dn_6': 5.0, 'loss_dice_dn_6': 5.0, 'loss_bbox_dn_6': 5.0, 'loss_giou_dn_6': 2.0, 'loss_ce_interm_dn_6': 4.0, 'loss_mask_interm_dn_6': 5.0, 'loss_dice_interm_dn_6': 5.0, 'loss_bbox_interm_dn_6': 5.0, 'loss_giou_interm_dn_6': 2.0, 'loss_ce_7': 4.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_bbox_7': 5.0, 'loss_giou_7': 2.0, 'loss_ce_interm_7': 4.0, 'loss_mask_interm_7': 5.0, 'loss_dice_interm_7': 5.0, 'loss_bbox_interm_7': 5.0, 'loss_giou_interm_7': 2.0, 'loss_ce_dn_7': 4.0, 'loss_mask_dn_7': 5.0, 'loss_dice_dn_7': 5.0, 'loss_bbox_dn_7': 5.0, 'loss_giou_dn_7': 2.0, 'loss_ce_interm_dn_7': 4.0, 'loss_mask_interm_dn_7': 5.0, 'loss_dice_interm_dn_7': 5.0, 'loss_bbox_interm_dn_7': 5.0, 'loss_giou_interm_dn_7': 2.0, 'loss_ce_8': 4.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0, 'loss_bbox_8': 5.0, 'loss_giou_8': 2.0, 'loss_ce_interm_8': 4.0, 'loss_mask_interm_8': 5.0, 'loss_dice_interm_8': 5.0, 'loss_bbox_interm_8': 5.0, 'loss_giou_interm_8': 2.0, 'loss_ce_dn_8': 4.0, 'loss_mask_dn_8': 5.0, 'loss_dice_dn_8': 5.0, 'loss_bbox_dn_8': 5.0, 'loss_giou_dn_8': 2.0, 'loss_ce_interm_dn_8': 4.0, 'loss_mask_interm_dn_8': 5.0, 'loss_dice_interm_dn_8': 5.0, 'loss_bbox_interm_dn_8': 5.0, 'loss_giou_interm_dn_8': 2.0}\n",
      "\u001b[32m[07/13 11:11:01 d2.engine.defaults]: \u001b[0mModel:\n",
      "MaskDINO(\n",
      "  (backbone): D2SwinTransformer(\n",
      "    (patch_embed): PatchEmbed(\n",
      "      (proj): Conv2d(3, 192, kernel_size=(4, 4), stride=(4, 4))\n",
      "      (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (pos_drop): Dropout(p=0.0, inplace=False)\n",
      "    (layers): ModuleList(\n",
      "      (0): BasicLayer(\n",
      "        (blocks): ModuleList(\n",
      "          (0): SwinTransformerBlock(\n",
      "            (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): Identity()\n",
      "            (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (1): SwinTransformerBlock(\n",
      "            (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.013)\n",
      "            (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (downsample): PatchMerging(\n",
      "          (reduction): Linear(in_features=768, out_features=384, bias=False)\n",
      "          (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicLayer(\n",
      "        (blocks): ModuleList(\n",
      "          (0): SwinTransformerBlock(\n",
      "            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.026)\n",
      "            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (1): SwinTransformerBlock(\n",
      "            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.039)\n",
      "            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (downsample): PatchMerging(\n",
      "          (reduction): Linear(in_features=1536, out_features=768, bias=False)\n",
      "          (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (2): BasicLayer(\n",
      "        (blocks): ModuleList(\n",
      "          (0): SwinTransformerBlock(\n",
      "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.052)\n",
      "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (1): SwinTransformerBlock(\n",
      "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.065)\n",
      "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (2): SwinTransformerBlock(\n",
      "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.078)\n",
      "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (3): SwinTransformerBlock(\n",
      "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.091)\n",
      "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (4): SwinTransformerBlock(\n",
      "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.104)\n",
      "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (5): SwinTransformerBlock(\n",
      "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.117)\n",
      "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (6): SwinTransformerBlock(\n",
      "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.130)\n",
      "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (7): SwinTransformerBlock(\n",
      "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.143)\n",
      "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (8): SwinTransformerBlock(\n",
      "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.157)\n",
      "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (9): SwinTransformerBlock(\n",
      "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.170)\n",
      "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (10): SwinTransformerBlock(\n",
      "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.183)\n",
      "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (11): SwinTransformerBlock(\n",
      "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.196)\n",
      "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (12): SwinTransformerBlock(\n",
      "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.209)\n",
      "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (13): SwinTransformerBlock(\n",
      "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.222)\n",
      "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (14): SwinTransformerBlock(\n",
      "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.235)\n",
      "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (15): SwinTransformerBlock(\n",
      "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.248)\n",
      "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (16): SwinTransformerBlock(\n",
      "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.261)\n",
      "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (17): SwinTransformerBlock(\n",
      "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.274)\n",
      "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (downsample): PatchMerging(\n",
      "          (reduction): Linear(in_features=3072, out_features=1536, bias=False)\n",
      "          (norm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (3): BasicLayer(\n",
      "        (blocks): ModuleList(\n",
      "          (0): SwinTransformerBlock(\n",
      "            (norm1): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              (qkv): Linear(in_features=1536, out_features=4608, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.287)\n",
      "            (norm2): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=1536, out_features=6144, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=6144, out_features=1536, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (1): SwinTransformerBlock(\n",
      "            (norm1): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              (qkv): Linear(in_features=1536, out_features=4608, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.300)\n",
      "            (norm2): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=1536, out_features=6144, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=6144, out_features=1536, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (norm0): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "    (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "    (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (norm3): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (sem_seg_head): MaskDINOHead(\n",
      "    (pixel_decoder): MaskDINOEncoder(\n",
      "      (input_proj): ModuleList(\n",
      "        (0): Sequential(\n",
      "          (0): Conv2d(192, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "        )\n",
      "        (1): Sequential(\n",
      "          (0): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "        )\n",
      "        (2): Sequential(\n",
      "          (0): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "        )\n",
      "        (3): Sequential(\n",
      "          (0): Conv2d(1536, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "        )\n",
      "        (4): Sequential(\n",
      "          (0): Conv2d(1536, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "        )\n",
      "      )\n",
      "      (transformer): MSDeformAttnTransformerEncoderOnly(\n",
      "        (encoder): MSDeformAttnTransformerEncoder(\n",
      "          (layers): ModuleList(\n",
      "            (0): MSDeformAttnTransformerEncoderLayer(\n",
      "              (self_attn): MSDeformAttn(\n",
      "                (sampling_offsets): Linear(in_features=256, out_features=320, bias=True)\n",
      "                (attention_weights): Linear(in_features=256, out_features=160, bias=True)\n",
      "                (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "                (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "              )\n",
      "              (dropout1): Dropout(p=0.0, inplace=False)\n",
      "              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "              (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "              (dropout2): Dropout(p=0.0, inplace=False)\n",
      "              (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "              (dropout3): Dropout(p=0.0, inplace=False)\n",
      "              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (1): MSDeformAttnTransformerEncoderLayer(\n",
      "              (self_attn): MSDeformAttn(\n",
      "                (sampling_offsets): Linear(in_features=256, out_features=320, bias=True)\n",
      "                (attention_weights): Linear(in_features=256, out_features=160, bias=True)\n",
      "                (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "                (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "              )\n",
      "              (dropout1): Dropout(p=0.0, inplace=False)\n",
      "              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "              (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "              (dropout2): Dropout(p=0.0, inplace=False)\n",
      "              (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "              (dropout3): Dropout(p=0.0, inplace=False)\n",
      "              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (2): MSDeformAttnTransformerEncoderLayer(\n",
      "              (self_attn): MSDeformAttn(\n",
      "                (sampling_offsets): Linear(in_features=256, out_features=320, bias=True)\n",
      "                (attention_weights): Linear(in_features=256, out_features=160, bias=True)\n",
      "                (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "                (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "              )\n",
      "              (dropout1): Dropout(p=0.0, inplace=False)\n",
      "              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "              (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "              (dropout2): Dropout(p=0.0, inplace=False)\n",
      "              (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "              (dropout3): Dropout(p=0.0, inplace=False)\n",
      "              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (3): MSDeformAttnTransformerEncoderLayer(\n",
      "              (self_attn): MSDeformAttn(\n",
      "                (sampling_offsets): Linear(in_features=256, out_features=320, bias=True)\n",
      "                (attention_weights): Linear(in_features=256, out_features=160, bias=True)\n",
      "                (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "                (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "              )\n",
      "              (dropout1): Dropout(p=0.0, inplace=False)\n",
      "              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "              (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "              (dropout2): Dropout(p=0.0, inplace=False)\n",
      "              (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "              (dropout3): Dropout(p=0.0, inplace=False)\n",
      "              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (4): MSDeformAttnTransformerEncoderLayer(\n",
      "              (self_attn): MSDeformAttn(\n",
      "                (sampling_offsets): Linear(in_features=256, out_features=320, bias=True)\n",
      "                (attention_weights): Linear(in_features=256, out_features=160, bias=True)\n",
      "                (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "                (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "              )\n",
      "              (dropout1): Dropout(p=0.0, inplace=False)\n",
      "              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "              (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "              (dropout2): Dropout(p=0.0, inplace=False)\n",
      "              (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "              (dropout3): Dropout(p=0.0, inplace=False)\n",
      "              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (5): MSDeformAttnTransformerEncoderLayer(\n",
      "              (self_attn): MSDeformAttn(\n",
      "                (sampling_offsets): Linear(in_features=256, out_features=320, bias=True)\n",
      "                (attention_weights): Linear(in_features=256, out_features=160, bias=True)\n",
      "                (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "                (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "              )\n",
      "              (dropout1): Dropout(p=0.0, inplace=False)\n",
      "              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "              (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "              (dropout2): Dropout(p=0.0, inplace=False)\n",
      "              (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "              (dropout3): Dropout(p=0.0, inplace=False)\n",
      "              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (pe_layer): Positional encoding PositionEmbeddingSine\n",
      "          num_pos_feats: 128\n",
      "          temperature: 10000\n",
      "          normalize: True\n",
      "          scale: 6.283185307179586\n",
      "      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (adapter_1): Conv2d(\n",
      "        192, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "      )\n",
      "      (layer_1): Conv2d(\n",
      "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "      )\n",
      "    )\n",
      "    (predictor): MaskDINODecoder(\n",
      "      (enc_output): Linear(in_features=256, out_features=256, bias=True)\n",
      "      (enc_output_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (input_proj): ModuleList(\n",
      "        (0): Sequential()\n",
      "        (1): Sequential()\n",
      "        (2): Sequential()\n",
      "        (3): Sequential()\n",
      "        (4): Sequential()\n",
      "      )\n",
      "      (class_embed): Linear(in_features=256, out_features=39, bias=True)\n",
      "      (label_enc): Embedding(39, 256)\n",
      "      (mask_embed): MLP(\n",
      "        (layers): ModuleList(\n",
      "          (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (1): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (2): Linear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (decoder): TransformerDecoder(\n",
      "        (layers): ModuleList(\n",
      "          (0): DeformableTransformerDecoderLayer(\n",
      "            (cross_attn): MSDeformAttn(\n",
      "              (sampling_offsets): Linear(in_features=256, out_features=320, bias=True)\n",
      "              (attention_weights): Linear(in_features=256, out_features=160, bias=True)\n",
      "              (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            )\n",
      "            (dropout1): Dropout(p=0.0, inplace=False)\n",
      "            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (self_attn): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "            )\n",
      "            (dropout2): Dropout(p=0.0, inplace=False)\n",
      "            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "            (dropout3): Dropout(p=0.0, inplace=False)\n",
      "            (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "            (dropout4): Dropout(p=0.0, inplace=False)\n",
      "            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (1): DeformableTransformerDecoderLayer(\n",
      "            (cross_attn): MSDeformAttn(\n",
      "              (sampling_offsets): Linear(in_features=256, out_features=320, bias=True)\n",
      "              (attention_weights): Linear(in_features=256, out_features=160, bias=True)\n",
      "              (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            )\n",
      "            (dropout1): Dropout(p=0.0, inplace=False)\n",
      "            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (self_attn): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "            )\n",
      "            (dropout2): Dropout(p=0.0, inplace=False)\n",
      "            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "            (dropout3): Dropout(p=0.0, inplace=False)\n",
      "            (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "            (dropout4): Dropout(p=0.0, inplace=False)\n",
      "            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (2): DeformableTransformerDecoderLayer(\n",
      "            (cross_attn): MSDeformAttn(\n",
      "              (sampling_offsets): Linear(in_features=256, out_features=320, bias=True)\n",
      "              (attention_weights): Linear(in_features=256, out_features=160, bias=True)\n",
      "              (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            )\n",
      "            (dropout1): Dropout(p=0.0, inplace=False)\n",
      "            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (self_attn): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "            )\n",
      "            (dropout2): Dropout(p=0.0, inplace=False)\n",
      "            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "            (dropout3): Dropout(p=0.0, inplace=False)\n",
      "            (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "            (dropout4): Dropout(p=0.0, inplace=False)\n",
      "            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (3): DeformableTransformerDecoderLayer(\n",
      "            (cross_attn): MSDeformAttn(\n",
      "              (sampling_offsets): Linear(in_features=256, out_features=320, bias=True)\n",
      "              (attention_weights): Linear(in_features=256, out_features=160, bias=True)\n",
      "              (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            )\n",
      "            (dropout1): Dropout(p=0.0, inplace=False)\n",
      "            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (self_attn): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "            )\n",
      "            (dropout2): Dropout(p=0.0, inplace=False)\n",
      "            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "            (dropout3): Dropout(p=0.0, inplace=False)\n",
      "            (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "            (dropout4): Dropout(p=0.0, inplace=False)\n",
      "            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (4): DeformableTransformerDecoderLayer(\n",
      "            (cross_attn): MSDeformAttn(\n",
      "              (sampling_offsets): Linear(in_features=256, out_features=320, bias=True)\n",
      "              (attention_weights): Linear(in_features=256, out_features=160, bias=True)\n",
      "              (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            )\n",
      "            (dropout1): Dropout(p=0.0, inplace=False)\n",
      "            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (self_attn): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "            )\n",
      "            (dropout2): Dropout(p=0.0, inplace=False)\n",
      "            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "            (dropout3): Dropout(p=0.0, inplace=False)\n",
      "            (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "            (dropout4): Dropout(p=0.0, inplace=False)\n",
      "            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (5): DeformableTransformerDecoderLayer(\n",
      "            (cross_attn): MSDeformAttn(\n",
      "              (sampling_offsets): Linear(in_features=256, out_features=320, bias=True)\n",
      "              (attention_weights): Linear(in_features=256, out_features=160, bias=True)\n",
      "              (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            )\n",
      "            (dropout1): Dropout(p=0.0, inplace=False)\n",
      "            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (self_attn): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "            )\n",
      "            (dropout2): Dropout(p=0.0, inplace=False)\n",
      "            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "            (dropout3): Dropout(p=0.0, inplace=False)\n",
      "            (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "            (dropout4): Dropout(p=0.0, inplace=False)\n",
      "            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (6): DeformableTransformerDecoderLayer(\n",
      "            (cross_attn): MSDeformAttn(\n",
      "              (sampling_offsets): Linear(in_features=256, out_features=320, bias=True)\n",
      "              (attention_weights): Linear(in_features=256, out_features=160, bias=True)\n",
      "              (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            )\n",
      "            (dropout1): Dropout(p=0.0, inplace=False)\n",
      "            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (self_attn): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "            )\n",
      "            (dropout2): Dropout(p=0.0, inplace=False)\n",
      "            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "            (dropout3): Dropout(p=0.0, inplace=False)\n",
      "            (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "            (dropout4): Dropout(p=0.0, inplace=False)\n",
      "            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (7): DeformableTransformerDecoderLayer(\n",
      "            (cross_attn): MSDeformAttn(\n",
      "              (sampling_offsets): Linear(in_features=256, out_features=320, bias=True)\n",
      "              (attention_weights): Linear(in_features=256, out_features=160, bias=True)\n",
      "              (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            )\n",
      "            (dropout1): Dropout(p=0.0, inplace=False)\n",
      "            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (self_attn): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "            )\n",
      "            (dropout2): Dropout(p=0.0, inplace=False)\n",
      "            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "            (dropout3): Dropout(p=0.0, inplace=False)\n",
      "            (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "            (dropout4): Dropout(p=0.0, inplace=False)\n",
      "            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (8): DeformableTransformerDecoderLayer(\n",
      "            (cross_attn): MSDeformAttn(\n",
      "              (sampling_offsets): Linear(in_features=256, out_features=320, bias=True)\n",
      "              (attention_weights): Linear(in_features=256, out_features=160, bias=True)\n",
      "              (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            )\n",
      "            (dropout1): Dropout(p=0.0, inplace=False)\n",
      "            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (self_attn): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "            )\n",
      "            (dropout2): Dropout(p=0.0, inplace=False)\n",
      "            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "            (dropout3): Dropout(p=0.0, inplace=False)\n",
      "            (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "            (dropout4): Dropout(p=0.0, inplace=False)\n",
      "            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "        (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (ref_point_head): MLP(\n",
      "          (layers): ModuleList(\n",
      "            (0): Linear(in_features=512, out_features=256, bias=True)\n",
      "            (1): Linear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (bbox_embed): ModuleList(\n",
      "          (0): MLP(\n",
      "            (layers): ModuleList(\n",
      "              (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (1): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (2): Linear(in_features=256, out_features=4, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (1): MLP(\n",
      "            (layers): ModuleList(\n",
      "              (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (1): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (2): Linear(in_features=256, out_features=4, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (2): MLP(\n",
      "            (layers): ModuleList(\n",
      "              (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (1): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (2): Linear(in_features=256, out_features=4, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (3): MLP(\n",
      "            (layers): ModuleList(\n",
      "              (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (1): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (2): Linear(in_features=256, out_features=4, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (4): MLP(\n",
      "            (layers): ModuleList(\n",
      "              (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (1): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (2): Linear(in_features=256, out_features=4, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (5): MLP(\n",
      "            (layers): ModuleList(\n",
      "              (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (1): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (2): Linear(in_features=256, out_features=4, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (6): MLP(\n",
      "            (layers): ModuleList(\n",
      "              (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (1): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (2): Linear(in_features=256, out_features=4, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (7): MLP(\n",
      "            (layers): ModuleList(\n",
      "              (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (1): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (2): Linear(in_features=256, out_features=4, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (8): MLP(\n",
      "            (layers): ModuleList(\n",
      "              (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (1): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (2): Linear(in_features=256, out_features=4, bias=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (_bbox_embed): MLP(\n",
      "        (layers): ModuleList(\n",
      "          (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (1): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (2): Linear(in_features=256, out_features=4, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (bbox_embed): ModuleList(\n",
      "        (0): MLP(\n",
      "          (layers): ModuleList(\n",
      "            (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (1): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (2): Linear(in_features=256, out_features=4, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (1): MLP(\n",
      "          (layers): ModuleList(\n",
      "            (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (1): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (2): Linear(in_features=256, out_features=4, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (2): MLP(\n",
      "          (layers): ModuleList(\n",
      "            (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (1): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (2): Linear(in_features=256, out_features=4, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (3): MLP(\n",
      "          (layers): ModuleList(\n",
      "            (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (1): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (2): Linear(in_features=256, out_features=4, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (4): MLP(\n",
      "          (layers): ModuleList(\n",
      "            (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (1): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (2): Linear(in_features=256, out_features=4, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (5): MLP(\n",
      "          (layers): ModuleList(\n",
      "            (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (1): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (2): Linear(in_features=256, out_features=4, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (6): MLP(\n",
      "          (layers): ModuleList(\n",
      "            (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (1): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (2): Linear(in_features=256, out_features=4, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (7): MLP(\n",
      "          (layers): ModuleList(\n",
      "            (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (1): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (2): Linear(in_features=256, out_features=4, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (8): MLP(\n",
      "          (layers): ModuleList(\n",
      "            (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (1): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (2): Linear(in_features=256, out_features=4, bias=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (criterion): Criterion SetCriterion\n",
      "      matcher: Matcher HungarianMatcher\n",
      "          cost_class: 4.0\n",
      "          cost_mask: 5.0\n",
      "          cost_dice: 5.0\n",
      "      losses: ['labels', 'masks', 'boxes']\n",
      "      weight_dict: {'loss_ce': 4.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_bbox': 5.0, 'loss_giou': 2.0, 'loss_ce_interm': 4.0, 'loss_mask_interm': 5.0, 'loss_dice_interm': 5.0, 'loss_bbox_interm': 5.0, 'loss_giou_interm': 2.0, 'loss_ce_dn': 4.0, 'loss_mask_dn': 5.0, 'loss_dice_dn': 5.0, 'loss_bbox_dn': 5.0, 'loss_giou_dn': 2.0, 'loss_ce_interm_dn': 4.0, 'loss_mask_interm_dn': 5.0, 'loss_dice_interm_dn': 5.0, 'loss_bbox_interm_dn': 5.0, 'loss_giou_interm_dn': 2.0, 'loss_ce_0': 4.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_bbox_0': 5.0, 'loss_giou_0': 2.0, 'loss_ce_interm_0': 4.0, 'loss_mask_interm_0': 5.0, 'loss_dice_interm_0': 5.0, 'loss_bbox_interm_0': 5.0, 'loss_giou_interm_0': 2.0, 'loss_ce_dn_0': 4.0, 'loss_mask_dn_0': 5.0, 'loss_dice_dn_0': 5.0, 'loss_bbox_dn_0': 5.0, 'loss_giou_dn_0': 2.0, 'loss_ce_interm_dn_0': 4.0, 'loss_mask_interm_dn_0': 5.0, 'loss_dice_interm_dn_0': 5.0, 'loss_bbox_interm_dn_0': 5.0, 'loss_giou_interm_dn_0': 2.0, 'loss_ce_1': 4.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_bbox_1': 5.0, 'loss_giou_1': 2.0, 'loss_ce_interm_1': 4.0, 'loss_mask_interm_1': 5.0, 'loss_dice_interm_1': 5.0, 'loss_bbox_interm_1': 5.0, 'loss_giou_interm_1': 2.0, 'loss_ce_dn_1': 4.0, 'loss_mask_dn_1': 5.0, 'loss_dice_dn_1': 5.0, 'loss_bbox_dn_1': 5.0, 'loss_giou_dn_1': 2.0, 'loss_ce_interm_dn_1': 4.0, 'loss_mask_interm_dn_1': 5.0, 'loss_dice_interm_dn_1': 5.0, 'loss_bbox_interm_dn_1': 5.0, 'loss_giou_interm_dn_1': 2.0, 'loss_ce_2': 4.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_bbox_2': 5.0, 'loss_giou_2': 2.0, 'loss_ce_interm_2': 4.0, 'loss_mask_interm_2': 5.0, 'loss_dice_interm_2': 5.0, 'loss_bbox_interm_2': 5.0, 'loss_giou_interm_2': 2.0, 'loss_ce_dn_2': 4.0, 'loss_mask_dn_2': 5.0, 'loss_dice_dn_2': 5.0, 'loss_bbox_dn_2': 5.0, 'loss_giou_dn_2': 2.0, 'loss_ce_interm_dn_2': 4.0, 'loss_mask_interm_dn_2': 5.0, 'loss_dice_interm_dn_2': 5.0, 'loss_bbox_interm_dn_2': 5.0, 'loss_giou_interm_dn_2': 2.0, 'loss_ce_3': 4.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_bbox_3': 5.0, 'loss_giou_3': 2.0, 'loss_ce_interm_3': 4.0, 'loss_mask_interm_3': 5.0, 'loss_dice_interm_3': 5.0, 'loss_bbox_interm_3': 5.0, 'loss_giou_interm_3': 2.0, 'loss_ce_dn_3': 4.0, 'loss_mask_dn_3': 5.0, 'loss_dice_dn_3': 5.0, 'loss_bbox_dn_3': 5.0, 'loss_giou_dn_3': 2.0, 'loss_ce_interm_dn_3': 4.0, 'loss_mask_interm_dn_3': 5.0, 'loss_dice_interm_dn_3': 5.0, 'loss_bbox_interm_dn_3': 5.0, 'loss_giou_interm_dn_3': 2.0, 'loss_ce_4': 4.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_bbox_4': 5.0, 'loss_giou_4': 2.0, 'loss_ce_interm_4': 4.0, 'loss_mask_interm_4': 5.0, 'loss_dice_interm_4': 5.0, 'loss_bbox_interm_4': 5.0, 'loss_giou_interm_4': 2.0, 'loss_ce_dn_4': 4.0, 'loss_mask_dn_4': 5.0, 'loss_dice_dn_4': 5.0, 'loss_bbox_dn_4': 5.0, 'loss_giou_dn_4': 2.0, 'loss_ce_interm_dn_4': 4.0, 'loss_mask_interm_dn_4': 5.0, 'loss_dice_interm_dn_4': 5.0, 'loss_bbox_interm_dn_4': 5.0, 'loss_giou_interm_dn_4': 2.0, 'loss_ce_5': 4.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_bbox_5': 5.0, 'loss_giou_5': 2.0, 'loss_ce_interm_5': 4.0, 'loss_mask_interm_5': 5.0, 'loss_dice_interm_5': 5.0, 'loss_bbox_interm_5': 5.0, 'loss_giou_interm_5': 2.0, 'loss_ce_dn_5': 4.0, 'loss_mask_dn_5': 5.0, 'loss_dice_dn_5': 5.0, 'loss_bbox_dn_5': 5.0, 'loss_giou_dn_5': 2.0, 'loss_ce_interm_dn_5': 4.0, 'loss_mask_interm_dn_5': 5.0, 'loss_dice_interm_dn_5': 5.0, 'loss_bbox_interm_dn_5': 5.0, 'loss_giou_interm_dn_5': 2.0, 'loss_ce_6': 4.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_bbox_6': 5.0, 'loss_giou_6': 2.0, 'loss_ce_interm_6': 4.0, 'loss_mask_interm_6': 5.0, 'loss_dice_interm_6': 5.0, 'loss_bbox_interm_6': 5.0, 'loss_giou_interm_6': 2.0, 'loss_ce_dn_6': 4.0, 'loss_mask_dn_6': 5.0, 'loss_dice_dn_6': 5.0, 'loss_bbox_dn_6': 5.0, 'loss_giou_dn_6': 2.0, 'loss_ce_interm_dn_6': 4.0, 'loss_mask_interm_dn_6': 5.0, 'loss_dice_interm_dn_6': 5.0, 'loss_bbox_interm_dn_6': 5.0, 'loss_giou_interm_dn_6': 2.0, 'loss_ce_7': 4.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_bbox_7': 5.0, 'loss_giou_7': 2.0, 'loss_ce_interm_7': 4.0, 'loss_mask_interm_7': 5.0, 'loss_dice_interm_7': 5.0, 'loss_bbox_interm_7': 5.0, 'loss_giou_interm_7': 2.0, 'loss_ce_dn_7': 4.0, 'loss_mask_dn_7': 5.0, 'loss_dice_dn_7': 5.0, 'loss_bbox_dn_7': 5.0, 'loss_giou_dn_7': 2.0, 'loss_ce_interm_dn_7': 4.0, 'loss_mask_interm_dn_7': 5.0, 'loss_dice_interm_dn_7': 5.0, 'loss_bbox_interm_dn_7': 5.0, 'loss_giou_interm_dn_7': 2.0, 'loss_ce_8': 4.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0, 'loss_bbox_8': 5.0, 'loss_giou_8': 2.0, 'loss_ce_interm_8': 4.0, 'loss_mask_interm_8': 5.0, 'loss_dice_interm_8': 5.0, 'loss_bbox_interm_8': 5.0, 'loss_giou_interm_8': 2.0, 'loss_ce_dn_8': 4.0, 'loss_mask_dn_8': 5.0, 'loss_dice_dn_8': 5.0, 'loss_bbox_dn_8': 5.0, 'loss_giou_dn_8': 2.0, 'loss_ce_interm_dn_8': 4.0, 'loss_mask_interm_dn_8': 5.0, 'loss_dice_interm_dn_8': 5.0, 'loss_bbox_interm_dn_8': 5.0, 'loss_giou_interm_dn_8': 2.0}\n",
      "      num_classes: 39\n",
      "      eos_coef: 0.1\n",
      "      num_points: 12544\n",
      "      oversample_ratio: 3.0\n",
      "      importance_sample_ratio: 0.75\n",
      ")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[07/13 11:11:29 d2.data.build]: \u001b[0mRemoved 1 images with no usable annotations. 1512 images left.\n",
      "\u001b[32m[07/13 11:11:29 d2.data.build]: \u001b[0mDistribution of instances among all 39 categories:\n",
      "\u001b[36m|   category    | #instances   |   category    | #instances   |   category    | #instances   |\n",
      "|:-------------:|:-------------|:-------------:|:-------------|:-------------:|:-------------|\n",
      "|     apple     | 417          |    banana     | 139          |   baseball    | 234          |\n",
      "|    cereals    | 1071         |    cheezit    | 258          | chocolate_j.. | 194          |\n",
      "|   cleanser    | 178          | coffee_grou.. | 311          |     cola      | 144          |\n",
      "|  couch_table  | 147          |     dice      | 73           |     fork      | 397          |\n",
      "|   iced_tea    | 146          |  juice_pack   | 149          |     knife     | 390          |\n",
      "|     lemon     | 336          |     milk      | 241          |    mustard    | 162          |\n",
      "|    orange     | 277          | orange_juice  | 112          |     peach     | 314          |\n",
      "|     pear      | 190          |     plum      | 199          |   pringles    | 785          |\n",
      "|   red_wine    | 68           |  rubiks_cube  | 219          |     shelf     | 325          |\n",
      "|  shelf_door   | 76           |  soccer_ball  | 248          |     spam      | 231          |\n",
      "|    sponge     | 149          |     spoon     | 372          |  strawberry   | 391          |\n",
      "| strawberry_.. | 285          |     sugar     | 341          |  tennis_ball  | 242          |\n",
      "|  tomato_soup  | 814          | tropical_ju.. | 339          |     tuna      | 159          |\n",
      "|               |              |               |              |               |              |\n",
      "|     total     | 11123        |               |              |               |              |\u001b[0m\n",
      "\u001b[32m[07/13 11:11:29 d2.data.build]: \u001b[0mUsing training sampler TrainingSampler\n",
      "\u001b[32m[07/13 11:11:29 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001b[32m[07/13 11:11:29 d2.data.common]: \u001b[0mSerializing 1512 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[07/13 11:11:29 d2.data.common]: \u001b[0mSerialized dataset takes 3.10 MiB\n",
      "\u001b[32m[07/13 11:11:29 d2.checkpoint.detection_checkpoint]: \u001b[0m[DetectionCheckpointer] Loading from data/robocup_bordeaux_2023/robocup_data/trained_models/coco_pretrained_maskdino/swin/maskdino_swinl_50ep_300q_hid2048_3sd1_instance_maskenhanced_mask52.3ap_box59.0ap.pth ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skip loading parameter 'sem_seg_head.predictor.class_embed.weight' to the model due to incompatible shapes: (80, 256) in the checkpoint but (39, 256) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'sem_seg_head.predictor.class_embed.bias' to the model due to incompatible shapes: (80,) in the checkpoint but (39,) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'sem_seg_head.predictor.label_enc.weight' to the model due to incompatible shapes: (80, 256) in the checkpoint but (39, 256) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'criterion.empty_weight' to the model due to incompatible shapes: (81,) in the checkpoint but (40,) in the model! You might want to double check if this is expected.\n",
      "Some model parameters or buffers are not found in the checkpoint:\n",
      "\u001b[34mcriterion.empty_weight\u001b[0m\n",
      "\u001b[34msem_seg_head.predictor.class_embed.{bias, weight}\u001b[0m\n",
      "\u001b[34msem_seg_head.predictor.label_enc.weight\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[07/13 11:11:29 d2.engine.train_loop]: \u001b[0mStarting training from iteration 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/nogga/anaconda3/envs/athome23_detection/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[07/13 11:11:48 d2.utils.events]: \u001b[0m eta: 0:29:42  iter: 19  total_loss: 5923  loss_ce: 459  loss_mask: 0.8046  loss_dice: 2.597  loss_bbox: 1.214  loss_giou: 1.742  loss_ce_dn: 48.84  loss_mask_dn: 0.1484  loss_dice_dn: 0.7376  loss_bbox_dn: 0.1079  loss_giou_dn: 0.3437  loss_ce_0: 477.8  loss_mask_0: 1.341  loss_dice_0: 4.244  loss_bbox_0: 1.326  loss_giou_0: 2.248  loss_ce_dn_0: 43.81  loss_mask_dn_0: 1.165  loss_dice_dn_0: 4.725  loss_bbox_dn_0: 0.3508  loss_giou_dn_0: 0.8503  loss_ce_1: 543.7  loss_mask_1: 1.121  loss_dice_1: 3.792  loss_bbox_1: 1.436  loss_giou_1: 2.098  loss_ce_dn_1: 53.86  loss_mask_dn_1: 0.4731  loss_dice_dn_1: 1.597  loss_bbox_dn_1: 0.2359  loss_giou_dn_1: 0.6011  loss_ce_2: 518.3  loss_mask_2: 0.9685  loss_dice_2: 3.575  loss_bbox_2: 1.403  loss_giou_2: 2.067  loss_ce_dn_2: 51.5  loss_mask_dn_2: 0.2135  loss_dice_dn_2: 0.8561  loss_bbox_dn_2: 0.1862  loss_giou_dn_2: 0.4675  loss_ce_3: 495.6  loss_mask_3: 0.9404  loss_dice_3: 3.415  loss_bbox_3: 1.403  loss_giou_3: 2.047  loss_ce_dn_3: 50.79  loss_mask_dn_3: 0.1426  loss_dice_dn_3: 0.7665  loss_bbox_dn_3: 0.156  loss_giou_dn_3: 0.3991  loss_ce_4: 464.3  loss_mask_4: 0.8406  loss_dice_4: 3.16  loss_bbox_4: 1.385  loss_giou_4: 1.92  loss_ce_dn_4: 50.14  loss_mask_dn_4: 0.1267  loss_dice_dn_4: 0.7387  loss_bbox_dn_4: 0.1258  loss_giou_dn_4: 0.3621  loss_ce_5: 460.8  loss_mask_5: 0.9016  loss_dice_5: 3.009  loss_bbox_5: 1.304  loss_giou_5: 1.794  loss_ce_dn_5: 50.17  loss_mask_dn_5: 0.1378  loss_dice_dn_5: 0.7196  loss_bbox_dn_5: 0.1151  loss_giou_dn_5: 0.353  loss_ce_6: 464.9  loss_mask_6: 0.8293  loss_dice_6: 2.853  loss_bbox_6: 1.231  loss_giou_6: 1.843  loss_ce_dn_6: 49.66  loss_mask_dn_6: 0.1592  loss_dice_dn_6: 0.74  loss_bbox_dn_6: 0.109  loss_giou_dn_6: 0.3475  loss_ce_7: 474.6  loss_mask_7: 0.8066  loss_dice_7: 2.714  loss_bbox_7: 1.228  loss_giou_7: 1.828  loss_ce_dn_7: 49.89  loss_mask_dn_7: 0.1474  loss_dice_dn_7: 0.7246  loss_bbox_dn_7: 0.11  loss_giou_dn_7: 0.3478  loss_ce_8: 468.4  loss_mask_8: 0.7124  loss_dice_8: 2.71  loss_bbox_8: 1.223  loss_giou_8: 1.767  loss_ce_dn_8: 49.72  loss_mask_dn_8: 0.1467  loss_dice_dn_8: 0.7303  loss_bbox_dn_8: 0.1076  loss_giou_dn_8: 0.3447  loss_ce_interm: 477.8  loss_mask_interm: 1.497  loss_dice_interm: 4.242  loss_bbox_interm: 1.943  loss_giou_interm: 2.263    time: 0.7698  last_time: 0.7772  data_time: 0.0359  last_data_time: 0.0080   lr: 0.0001  max_mem: 20284M\n",
      "\u001b[32m[07/13 11:12:04 d2.utils.events]: \u001b[0m eta: 0:30:23  iter: 39  total_loss: 1294  loss_ce: 17.66  loss_mask: 0.8741  loss_dice: 4.673  loss_bbox: 3.591  loss_giou: 3.07  loss_ce_dn: 3.271  loss_mask_dn: 0.4891  loss_dice_dn: 2.486  loss_bbox_dn: 0.2717  loss_giou_dn: 0.7429  loss_ce_0: 272.2  loss_mask_0: 0.9519  loss_dice_0: 4.656  loss_bbox_0: 3.506  loss_giou_0: 3.072  loss_ce_dn_0: 35.97  loss_mask_dn_0: 1.347  loss_dice_dn_0: 4.774  loss_bbox_dn_0: 0.3904  loss_giou_dn_0: 0.8569  loss_ce_1: 143.8  loss_mask_1: 0.906  loss_dice_1: 4.569  loss_bbox_1: 3.569  loss_giou_1: 3.052  loss_ce_dn_1: 10.07  loss_mask_dn_1: 0.4848  loss_dice_dn_1: 3.196  loss_bbox_dn_1: 0.3158  loss_giou_dn_1: 0.764  loss_ce_2: 61.28  loss_mask_2: 1.075  loss_dice_2: 4.61  loss_bbox_2: 3.561  loss_giou_2: 3.057  loss_ce_dn_2: 5.474  loss_mask_dn_2: 0.5144  loss_dice_dn_2: 2.436  loss_bbox_dn_2: 0.2565  loss_giou_dn_2: 0.7225  loss_ce_3: 41.47  loss_mask_3: 0.9669  loss_dice_3: 4.62  loss_bbox_3: 3.558  loss_giou_3: 3.066  loss_ce_dn_3: 4.811  loss_mask_dn_3: 0.4521  loss_dice_dn_3: 2.332  loss_bbox_dn_3: 0.2816  loss_giou_dn_3: 0.7523  loss_ce_4: 30.13  loss_mask_4: 1.008  loss_dice_4: 4.582  loss_bbox_4: 3.555  loss_giou_4: 3.059  loss_ce_dn_4: 4.188  loss_mask_dn_4: 0.4404  loss_dice_dn_4: 2.084  loss_bbox_dn_4: 0.2734  loss_giou_dn_4: 0.7215  loss_ce_5: 25.23  loss_mask_5: 0.9041  loss_dice_5: 4.607  loss_bbox_5: 3.539  loss_giou_5: 3.067  loss_ce_dn_5: 3.713  loss_mask_dn_5: 0.5129  loss_dice_dn_5: 2.291  loss_bbox_dn_5: 0.2758  loss_giou_dn_5: 0.7287  loss_ce_6: 21.65  loss_mask_6: 0.8204  loss_dice_6: 4.627  loss_bbox_6: 3.559  loss_giou_6: 3.064  loss_ce_dn_6: 3.61  loss_mask_dn_6: 0.5178  loss_dice_dn_6: 2.328  loss_bbox_dn_6: 0.2763  loss_giou_dn_6: 0.7362  loss_ce_7: 19.75  loss_mask_7: 0.8563  loss_dice_7: 4.641  loss_bbox_7: 3.551  loss_giou_7: 3.073  loss_ce_dn_7: 3.357  loss_mask_dn_7: 0.5012  loss_dice_dn_7: 2.27  loss_bbox_dn_7: 0.2735  loss_giou_dn_7: 0.7246  loss_ce_8: 17.94  loss_mask_8: 0.8605  loss_dice_8: 4.599  loss_bbox_8: 3.589  loss_giou_8: 3.062  loss_ce_dn_8: 3.339  loss_mask_dn_8: 0.4927  loss_dice_dn_8: 2.411  loss_bbox_dn_8: 0.2695  loss_giou_dn_8: 0.7411  loss_ce_interm: 272.2  loss_mask_interm: 1.237  loss_dice_interm: 4.667  loss_bbox_interm: 13.08  loss_giou_interm: 2.569    time: 0.7788  last_time: 0.8500  data_time: 0.0259  last_data_time: 0.0759   lr: 0.0001  max_mem: 20328M\n",
      "\u001b[32m[07/13 11:12:19 d2.utils.events]: \u001b[0m eta: 0:30:11  iter: 59  total_loss: 433.2  loss_ce: 4.975  loss_mask: 0.6787  loss_dice: 4.249  loss_bbox: 2.678  loss_giou: 2.784  loss_ce_dn: 2.053  loss_mask_dn: 0.1344  loss_dice_dn: 1.25  loss_bbox_dn: 0.2046  loss_giou_dn: 0.622  loss_ce_0: 88.47  loss_mask_0: 0.6475  loss_dice_0: 4.341  loss_bbox_0: 2.745  loss_giou_0: 2.752  loss_ce_dn_0: 31.03  loss_mask_dn_0: 0.6069  loss_dice_dn_0: 4.737  loss_bbox_dn_0: 0.2859  loss_giou_dn_0: 0.8494  loss_ce_1: 11.18  loss_mask_1: 0.7552  loss_dice_1: 4.367  loss_bbox_1: 2.701  loss_giou_1: 2.745  loss_ce_dn_1: 2.281  loss_mask_dn_1: 0.1832  loss_dice_dn_1: 2.07  loss_bbox_dn_1: 0.2316  loss_giou_dn_1: 0.6999  loss_ce_2: 6.581  loss_mask_2: 0.7965  loss_dice_2: 4.31  loss_bbox_2: 2.738  loss_giou_2: 2.752  loss_ce_dn_2: 2.11  loss_mask_dn_2: 0.1452  loss_dice_dn_2: 1.3  loss_bbox_dn_2: 0.2152  loss_giou_dn_2: 0.6792  loss_ce_3: 5.792  loss_mask_3: 0.7466  loss_dice_3: 4.296  loss_bbox_3: 2.695  loss_giou_3: 2.771  loss_ce_dn_3: 2.078  loss_mask_dn_3: 0.1182  loss_dice_dn_3: 1.268  loss_bbox_dn_3: 0.2083  loss_giou_dn_3: 0.6716  loss_ce_4: 5.492  loss_mask_4: 0.7679  loss_dice_4: 4.291  loss_bbox_4: 2.714  loss_giou_4: 2.773  loss_ce_dn_4: 2.092  loss_mask_dn_4: 0.1207  loss_dice_dn_4: 1.185  loss_bbox_dn_4: 0.1999  loss_giou_dn_4: 0.6415  loss_ce_5: 5.252  loss_mask_5: 0.8064  loss_dice_5: 4.259  loss_bbox_5: 2.71  loss_giou_5: 2.772  loss_ce_dn_5: 2.077  loss_mask_dn_5: 0.1296  loss_dice_dn_5: 1.159  loss_bbox_dn_5: 0.1981  loss_giou_dn_5: 0.6322  loss_ce_6: 5.05  loss_mask_6: 0.8117  loss_dice_6: 4.257  loss_bbox_6: 2.7  loss_giou_6: 2.773  loss_ce_dn_6: 2.066  loss_mask_dn_6: 0.1258  loss_dice_dn_6: 1.142  loss_bbox_dn_6: 0.1969  loss_giou_dn_6: 0.6231  loss_ce_7: 5.281  loss_mask_7: 0.7496  loss_dice_7: 4.266  loss_bbox_7: 2.697  loss_giou_7: 2.78  loss_ce_dn_7: 2.061  loss_mask_dn_7: 0.1188  loss_dice_dn_7: 1.165  loss_bbox_dn_7: 0.2006  loss_giou_dn_7: 0.6278  loss_ce_8: 5.028  loss_mask_8: 0.7268  loss_dice_8: 4.279  loss_bbox_8: 2.683  loss_giou_8: 2.779  loss_ce_dn_8: 2.054  loss_mask_dn_8: 0.1208  loss_dice_dn_8: 1.179  loss_bbox_dn_8: 0.2018  loss_giou_dn_8: 0.6237  loss_ce_interm: 88.47  loss_mask_interm: 0.6978  loss_dice_interm: 4.462  loss_bbox_interm: 12.25  loss_giou_interm: 2.608    time: 0.7780  last_time: 0.7727  data_time: 0.0125  last_data_time: 0.0094   lr: 0.0001  max_mem: 20544M\n",
      "\u001b[32m[07/13 11:12:35 d2.utils.events]: \u001b[0m eta: 0:29:54  iter: 79  total_loss: 264.8  loss_ce: 3.307  loss_mask: 0.6386  loss_dice: 3.847  loss_bbox: 1.671  loss_giou: 2.484  loss_ce_dn: 2.035  loss_mask_dn: 0.1119  loss_dice_dn: 0.9739  loss_bbox_dn: 0.1216  loss_giou_dn: 0.4523  loss_ce_0: 21.55  loss_mask_0: 0.4982  loss_dice_0: 3.939  loss_bbox_0: 1.98  loss_giou_0: 2.545  loss_ce_dn_0: 29.03  loss_mask_dn_0: 0.704  loss_dice_dn_0: 4.644  loss_bbox_dn_0: 0.2802  loss_giou_dn_0: 0.8479  loss_ce_1: 4.049  loss_mask_1: 0.7049  loss_dice_1: 3.932  loss_bbox_1: 1.932  loss_giou_1: 2.5  loss_ce_dn_1: 2.067  loss_mask_dn_1: 0.1487  loss_dice_dn_1: 1.359  loss_bbox_dn_1: 0.1718  loss_giou_dn_1: 0.5831  loss_ce_2: 3.598  loss_mask_2: 0.7406  loss_dice_2: 3.924  loss_bbox_2: 1.89  loss_giou_2: 2.502  loss_ce_dn_2: 1.995  loss_mask_dn_2: 0.1032  loss_dice_dn_2: 0.9596  loss_bbox_dn_2: 0.1394  loss_giou_dn_2: 0.5015  loss_ce_3: 3.499  loss_mask_3: 0.6272  loss_dice_3: 3.906  loss_bbox_3: 1.834  loss_giou_3: 2.458  loss_ce_dn_3: 1.997  loss_mask_dn_3: 0.09288  loss_dice_dn_3: 0.9226  loss_bbox_dn_3: 0.1338  loss_giou_dn_3: 0.4851  loss_ce_4: 3.467  loss_mask_4: 0.6924  loss_dice_4: 3.91  loss_bbox_4: 1.788  loss_giou_4: 2.494  loss_ce_dn_4: 2.019  loss_mask_dn_4: 0.09361  loss_dice_dn_4: 0.8873  loss_bbox_dn_4: 0.1271  loss_giou_dn_4: 0.4611  loss_ce_5: 3.504  loss_mask_5: 0.6566  loss_dice_5: 3.893  loss_bbox_5: 1.735  loss_giou_5: 2.49  loss_ce_dn_5: 1.991  loss_mask_dn_5: 0.08752  loss_dice_dn_5: 0.9176  loss_bbox_dn_5: 0.1241  loss_giou_dn_5: 0.4531  loss_ce_6: 3.39  loss_mask_6: 0.6691  loss_dice_6: 3.863  loss_bbox_6: 1.703  loss_giou_6: 2.472  loss_ce_dn_6: 2.014  loss_mask_dn_6: 0.09649  loss_dice_dn_6: 0.9291  loss_bbox_dn_6: 0.1196  loss_giou_dn_6: 0.4469  loss_ce_7: 3.497  loss_mask_7: 0.6907  loss_dice_7: 3.848  loss_bbox_7: 1.692  loss_giou_7: 2.466  loss_ce_dn_7: 1.996  loss_mask_dn_7: 0.09277  loss_dice_dn_7: 0.9248  loss_bbox_dn_7: 0.1227  loss_giou_dn_7: 0.448  loss_ce_8: 3.349  loss_mask_8: 0.6231  loss_dice_8: 3.855  loss_bbox_8: 1.699  loss_giou_8: 2.486  loss_ce_dn_8: 2.016  loss_mask_dn_8: 0.09995  loss_dice_dn_8: 0.9476  loss_bbox_dn_8: 0.1189  loss_giou_dn_8: 0.4478  loss_ce_interm: 21.53  loss_mask_interm: 0.6058  loss_dice_interm: 3.986  loss_bbox_interm: 8.158  loss_giou_interm: 2.378    time: 0.7797  last_time: 0.8875  data_time: 0.0159  last_data_time: 0.0076   lr: 0.0001  max_mem: 20544M\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[07/13 11:12:51 d2.utils.events]: \u001b[0m eta: 0:29:38  iter: 99  total_loss: 160.4  loss_ce: 2.858  loss_mask: 0.7085  loss_dice: 2.444  loss_bbox: 0.9981  loss_giou: 1.353  loss_ce_dn: 1.996  loss_mask_dn: 0.1536  loss_dice_dn: 0.6132  loss_bbox_dn: 0.1126  loss_giou_dn: 0.2771  loss_ce_0: 6.744  loss_mask_0: 0.8277  loss_dice_0: 2.637  loss_bbox_0: 1.173  loss_giou_0: 1.436  loss_ce_dn_0: 18.64  loss_mask_dn_0: 0.9692  loss_dice_dn_0: 4.659  loss_bbox_dn_0: 0.3746  loss_giou_dn_0: 0.8576  loss_ce_1: 3.084  loss_mask_1: 0.9013  loss_dice_1: 2.576  loss_bbox_1: 1.106  loss_giou_1: 1.46  loss_ce_dn_1: 1.928  loss_mask_dn_1: 0.1552  loss_dice_dn_1: 0.6929  loss_bbox_dn_1: 0.1802  loss_giou_dn_1: 0.4719  loss_ce_2: 2.957  loss_mask_2: 0.8823  loss_dice_2: 2.45  loss_bbox_2: 1.096  loss_giou_2: 1.364  loss_ce_dn_2: 1.919  loss_mask_dn_2: 0.1298  loss_dice_dn_2: 0.5793  loss_bbox_dn_2: 0.1338  loss_giou_dn_2: 0.3368  loss_ce_3: 2.951  loss_mask_3: 0.7922  loss_dice_3: 2.427  loss_bbox_3: 1.094  loss_giou_3: 1.364  loss_ce_dn_3: 1.917  loss_mask_dn_3: 0.1468  loss_dice_dn_3: 0.6239  loss_bbox_dn_3: 0.125  loss_giou_dn_3: 0.32  loss_ce_4: 2.944  loss_mask_4: 0.8097  loss_dice_4: 2.424  loss_bbox_4: 1.077  loss_giou_4: 1.367  loss_ce_dn_4: 1.914  loss_mask_dn_4: 0.1459  loss_dice_dn_4: 0.5706  loss_bbox_dn_4: 0.1166  loss_giou_dn_4: 0.2859  loss_ce_5: 2.951  loss_mask_5: 0.8001  loss_dice_5: 2.383  loss_bbox_5: 1.064  loss_giou_5: 1.378  loss_ce_dn_5: 1.891  loss_mask_dn_5: 0.1452  loss_dice_dn_5: 0.589  loss_bbox_dn_5: 0.1145  loss_giou_dn_5: 0.2856  loss_ce_6: 2.872  loss_mask_6: 0.8192  loss_dice_6: 2.4  loss_bbox_6: 1.05  loss_giou_6: 1.322  loss_ce_dn_6: 1.919  loss_mask_dn_6: 0.1411  loss_dice_dn_6: 0.5892  loss_bbox_dn_6: 0.1129  loss_giou_dn_6: 0.2786  loss_ce_7: 2.922  loss_mask_7: 0.777  loss_dice_7: 2.4  loss_bbox_7: 1.036  loss_giou_7: 1.331  loss_ce_dn_7: 1.934  loss_mask_dn_7: 0.1455  loss_dice_dn_7: 0.5821  loss_bbox_dn_7: 0.1134  loss_giou_dn_7: 0.2804  loss_ce_8: 2.894  loss_mask_8: 0.7101  loss_dice_8: 2.457  loss_bbox_8: 1.003  loss_giou_8: 1.31  loss_ce_dn_8: 1.966  loss_mask_dn_8: 0.1427  loss_dice_dn_8: 0.5985  loss_bbox_dn_8: 0.1105  loss_giou_dn_8: 0.2739  loss_ce_interm: 6.787  loss_mask_interm: 0.9011  loss_dice_interm: 2.68  loss_bbox_interm: 1.639  loss_giou_interm: 1.708    time: 0.7793  last_time: 0.7709  data_time: 0.0089  last_data_time: 0.0064   lr: 0.0001  max_mem: 20819M\n",
      "\u001b[32m[07/13 11:13:07 d2.utils.events]: \u001b[0m eta: 0:29:30  iter: 119  total_loss: 101.5  loss_ce: 2.327  loss_mask: 0.1756  loss_dice: 1.037  loss_bbox: 0.2323  loss_giou: 0.5724  loss_ce_dn: 1.622  loss_mask_dn: 0.1049  loss_dice_dn: 0.5189  loss_bbox_dn: 0.06855  loss_giou_dn: 0.2412  loss_ce_0: 3.421  loss_mask_0: 0.1987  loss_dice_0: 1.166  loss_bbox_0: 0.3296  loss_giou_0: 0.6797  loss_ce_dn_0: 16.23  loss_mask_dn_0: 0.8238  loss_dice_dn_0: 4.501  loss_bbox_dn_0: 0.3641  loss_giou_dn_0: 0.8512  loss_ce_1: 2.652  loss_mask_1: 0.2243  loss_dice_1: 1.237  loss_bbox_1: 0.2953  loss_giou_1: 0.641  loss_ce_dn_1: 1.685  loss_mask_dn_1: 0.1195  loss_dice_dn_1: 0.6868  loss_bbox_dn_1: 0.1561  loss_giou_dn_1: 0.4607  loss_ce_2: 2.564  loss_mask_2: 0.1766  loss_dice_2: 1.174  loss_bbox_2: 0.2539  loss_giou_2: 0.6183  loss_ce_dn_2: 1.571  loss_mask_dn_2: 0.108  loss_dice_dn_2: 0.5456  loss_bbox_dn_2: 0.09948  loss_giou_dn_2: 0.3201  loss_ce_3: 2.513  loss_mask_3: 0.1625  loss_dice_3: 1.09  loss_bbox_3: 0.251  loss_giou_3: 0.6223  loss_ce_dn_3: 1.566  loss_mask_dn_3: 0.1116  loss_dice_dn_3: 0.5288  loss_bbox_dn_3: 0.08559  loss_giou_dn_3: 0.2865  loss_ce_4: 2.453  loss_mask_4: 0.165  loss_dice_4: 1.083  loss_bbox_4: 0.2288  loss_giou_4: 0.5887  loss_ce_dn_4: 1.58  loss_mask_dn_4: 0.1076  loss_dice_dn_4: 0.5149  loss_bbox_dn_4: 0.07971  loss_giou_dn_4: 0.2615  loss_ce_5: 2.379  loss_mask_5: 0.1531  loss_dice_5: 1.072  loss_bbox_5: 0.2431  loss_giou_5: 0.5909  loss_ce_dn_5: 1.568  loss_mask_dn_5: 0.1099  loss_dice_dn_5: 0.5148  loss_bbox_dn_5: 0.07839  loss_giou_dn_5: 0.2476  loss_ce_6: 2.331  loss_mask_6: 0.1529  loss_dice_6: 0.9606  loss_bbox_6: 0.2304  loss_giou_6: 0.5742  loss_ce_dn_6: 1.606  loss_mask_dn_6: 0.1074  loss_dice_dn_6: 0.5231  loss_bbox_dn_6: 0.07194  loss_giou_dn_6: 0.241  loss_ce_7: 2.337  loss_mask_7: 0.1479  loss_dice_7: 1.014  loss_bbox_7: 0.2362  loss_giou_7: 0.5824  loss_ce_dn_7: 1.588  loss_mask_dn_7: 0.1114  loss_dice_dn_7: 0.5213  loss_bbox_dn_7: 0.07196  loss_giou_dn_7: 0.24  loss_ce_8: 2.338  loss_mask_8: 0.1614  loss_dice_8: 1.026  loss_bbox_8: 0.2317  loss_giou_8: 0.5591  loss_ce_dn_8: 1.602  loss_mask_dn_8: 0.1069  loss_dice_dn_8: 0.5172  loss_bbox_dn_8: 0.06858  loss_giou_dn_8: 0.2412  loss_ce_interm: 3.453  loss_mask_interm: 0.2251  loss_dice_interm: 1.157  loss_bbox_interm: 0.4623  loss_giou_interm: 0.8241    time: 0.7816  last_time: 0.8474  data_time: 0.0165  last_data_time: 0.0739   lr: 0.0001  max_mem: 20819M\n",
      "\u001b[32m[07/13 11:13:23 d2.utils.events]: \u001b[0m eta: 0:29:19  iter: 139  total_loss: 78.29  loss_ce: 1.928  loss_mask: 0.06735  loss_dice: 0.7244  loss_bbox: 0.1604  loss_giou: 0.4628  loss_ce_dn: 1.353  loss_mask_dn: 0.0512  loss_dice_dn: 0.5305  loss_bbox_dn: 0.04295  loss_giou_dn: 0.2177  loss_ce_0: 2.626  loss_mask_0: 0.06547  loss_dice_0: 0.7362  loss_bbox_0: 0.1568  loss_giou_0: 0.5007  loss_ce_dn_0: 13.17  loss_mask_dn_0: 0.6043  loss_dice_dn_0: 4.506  loss_bbox_dn_0: 0.3234  loss_giou_dn_0: 0.8542  loss_ce_1: 2.32  loss_mask_1: 0.06248  loss_dice_1: 0.8008  loss_bbox_1: 0.1598  loss_giou_1: 0.4736  loss_ce_dn_1: 1.438  loss_mask_dn_1: 0.06176  loss_dice_dn_1: 0.6575  loss_bbox_dn_1: 0.1206  loss_giou_dn_1: 0.4259  loss_ce_2: 2.231  loss_mask_2: 0.05441  loss_dice_2: 0.7764  loss_bbox_2: 0.1531  loss_giou_2: 0.4442  loss_ce_dn_2: 1.338  loss_mask_dn_2: 0.0506  loss_dice_dn_2: 0.5196  loss_bbox_dn_2: 0.06994  loss_giou_dn_2: 0.2897  loss_ce_3: 2.067  loss_mask_3: 0.0616  loss_dice_3: 0.7373  loss_bbox_3: 0.1534  loss_giou_3: 0.4342  loss_ce_dn_3: 1.302  loss_mask_dn_3: 0.05019  loss_dice_dn_3: 0.5243  loss_bbox_dn_3: 0.05654  loss_giou_dn_3: 0.2402  loss_ce_4: 2.046  loss_mask_4: 0.06437  loss_dice_4: 0.7174  loss_bbox_4: 0.164  loss_giou_4: 0.4401  loss_ce_dn_4: 1.308  loss_mask_dn_4: 0.05194  loss_dice_dn_4: 0.5231  loss_bbox_dn_4: 0.05026  loss_giou_dn_4: 0.224  loss_ce_5: 1.964  loss_mask_5: 0.06384  loss_dice_5: 0.7123  loss_bbox_5: 0.1621  loss_giou_5: 0.4399  loss_ce_dn_5: 1.284  loss_mask_dn_5: 0.05004  loss_dice_dn_5: 0.5277  loss_bbox_dn_5: 0.04882  loss_giou_dn_5: 0.2185  loss_ce_6: 1.921  loss_mask_6: 0.06139  loss_dice_6: 0.732  loss_bbox_6: 0.1622  loss_giou_6: 0.4535  loss_ce_dn_6: 1.313  loss_mask_dn_6: 0.04981  loss_dice_dn_6: 0.5278  loss_bbox_dn_6: 0.04399  loss_giou_dn_6: 0.2159  loss_ce_7: 1.958  loss_mask_7: 0.06821  loss_dice_7: 0.7121  loss_bbox_7: 0.1412  loss_giou_7: 0.4429  loss_ce_dn_7: 1.327  loss_mask_dn_7: 0.05366  loss_dice_dn_7: 0.5148  loss_bbox_dn_7: 0.04388  loss_giou_dn_7: 0.2171  loss_ce_8: 1.922  loss_mask_8: 0.06368  loss_dice_8: 0.6735  loss_bbox_8: 0.1586  loss_giou_8: 0.4267  loss_ce_dn_8: 1.342  loss_mask_dn_8: 0.05159  loss_dice_dn_8: 0.5197  loss_bbox_dn_8: 0.04292  loss_giou_dn_8: 0.2155  loss_ce_interm: 2.661  loss_mask_interm: 0.06344  loss_dice_interm: 0.7408  loss_bbox_interm: 0.2072  loss_giou_interm: 0.6116    time: 0.7843  last_time: 0.7655  data_time: 0.0160  last_data_time: 0.0038   lr: 0.0001  max_mem: 20819M\n",
      "\u001b[32m[07/13 11:13:39 d2.utils.events]: \u001b[0m eta: 0:29:08  iter: 159  total_loss: 72.65  loss_ce: 1.688  loss_mask: 0.07192  loss_dice: 0.6758  loss_bbox: 0.135  loss_giou: 0.3656  loss_ce_dn: 1.185  loss_mask_dn: 0.05126  loss_dice_dn: 0.412  loss_bbox_dn: 0.04822  loss_giou_dn: 0.1939  loss_ce_0: 2.306  loss_mask_0: 0.07656  loss_dice_0: 0.6858  loss_bbox_0: 0.1609  loss_giou_0: 0.4363  loss_ce_dn_0: 10.58  loss_mask_dn_0: 0.6944  loss_dice_dn_0: 4.414  loss_bbox_dn_0: 0.3165  loss_giou_dn_0: 0.8534  loss_ce_1: 2.079  loss_mask_1: 0.1022  loss_dice_1: 0.6418  loss_bbox_1: 0.1457  loss_giou_1: 0.4019  loss_ce_dn_1: 1.312  loss_mask_dn_1: 0.07643  loss_dice_dn_1: 0.5504  loss_bbox_dn_1: 0.1151  loss_giou_dn_1: 0.3949  loss_ce_2: 1.927  loss_mask_2: 0.0887  loss_dice_2: 0.6701  loss_bbox_2: 0.1442  loss_giou_2: 0.3905  loss_ce_dn_2: 1.159  loss_mask_dn_2: 0.06089  loss_dice_dn_2: 0.4639  loss_bbox_dn_2: 0.06917  loss_giou_dn_2: 0.2598  loss_ce_3: 1.888  loss_mask_3: 0.08072  loss_dice_3: 0.661  loss_bbox_3: 0.1173  loss_giou_3: 0.3787  loss_ce_dn_3: 1.133  loss_mask_dn_3: 0.05909  loss_dice_dn_3: 0.4492  loss_bbox_dn_3: 0.05537  loss_giou_dn_3: 0.217  loss_ce_4: 1.799  loss_mask_4: 0.07691  loss_dice_4: 0.6715  loss_bbox_4: 0.1335  loss_giou_4: 0.3908  loss_ce_dn_4: 1.14  loss_mask_dn_4: 0.05541  loss_dice_dn_4: 0.4248  loss_bbox_dn_4: 0.0498  loss_giou_dn_4: 0.1968  loss_ce_5: 1.734  loss_mask_5: 0.08147  loss_dice_5: 0.6646  loss_bbox_5: 0.1364  loss_giou_5: 0.3978  loss_ce_dn_5: 1.114  loss_mask_dn_5: 0.05413  loss_dice_dn_5: 0.4146  loss_bbox_dn_5: 0.04865  loss_giou_dn_5: 0.1922  loss_ce_6: 1.721  loss_mask_6: 0.06968  loss_dice_6: 0.6473  loss_bbox_6: 0.135  loss_giou_6: 0.3887  loss_ce_dn_6: 1.119  loss_mask_dn_6: 0.05301  loss_dice_dn_6: 0.4242  loss_bbox_dn_6: 0.04755  loss_giou_dn_6: 0.1884  loss_ce_7: 1.721  loss_mask_7: 0.06881  loss_dice_7: 0.648  loss_bbox_7: 0.12  loss_giou_7: 0.4032  loss_ce_dn_7: 1.15  loss_mask_dn_7: 0.05295  loss_dice_dn_7: 0.4214  loss_bbox_dn_7: 0.04757  loss_giou_dn_7: 0.1905  loss_ce_8: 1.738  loss_mask_8: 0.06418  loss_dice_8: 0.671  loss_bbox_8: 0.125  loss_giou_8: 0.3916  loss_ce_dn_8: 1.171  loss_mask_dn_8: 0.05198  loss_dice_dn_8: 0.4048  loss_bbox_dn_8: 0.04772  loss_giou_dn_8: 0.1907  loss_ce_interm: 2.307  loss_mask_interm: 0.08673  loss_dice_interm: 0.7021  loss_bbox_interm: 0.2224  loss_giou_interm: 0.5202    time: 0.7852  last_time: 0.7808  data_time: 0.0155  last_data_time: 0.0047   lr: 0.0001  max_mem: 20819M\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[07/13 11:13:55 d2.utils.events]: \u001b[0m eta: 0:28:53  iter: 179  total_loss: 73.81  loss_ce: 1.733  loss_mask: 0.07292  loss_dice: 0.8007  loss_bbox: 0.09462  loss_giou: 0.467  loss_ce_dn: 1.116  loss_mask_dn: 0.06015  loss_dice_dn: 0.5522  loss_bbox_dn: 0.04301  loss_giou_dn: 0.257  loss_ce_0: 2.222  loss_mask_0: 0.06588  loss_dice_0: 0.8566  loss_bbox_0: 0.1268  loss_giou_0: 0.5627  loss_ce_dn_0: 10.71  loss_mask_dn_0: 0.4619  loss_dice_dn_0: 4.206  loss_bbox_dn_0: 0.2525  loss_giou_dn_0: 0.8613  loss_ce_1: 2.042  loss_mask_1: 0.06736  loss_dice_1: 0.8862  loss_bbox_1: 0.1003  loss_giou_1: 0.4732  loss_ce_dn_1: 1.226  loss_mask_dn_1: 0.07449  loss_dice_dn_1: 0.6616  loss_bbox_dn_1: 0.09939  loss_giou_dn_1: 0.4611  loss_ce_2: 1.933  loss_mask_2: 0.0684  loss_dice_2: 0.8655  loss_bbox_2: 0.09733  loss_giou_2: 0.4676  loss_ce_dn_2: 1.054  loss_mask_dn_2: 0.07496  loss_dice_dn_2: 0.594  loss_bbox_dn_2: 0.06094  loss_giou_dn_2: 0.322  loss_ce_3: 1.851  loss_mask_3: 0.08415  loss_dice_3: 0.8879  loss_bbox_3: 0.09058  loss_giou_3: 0.4979  loss_ce_dn_3: 0.9933  loss_mask_dn_3: 0.069  loss_dice_dn_3: 0.5668  loss_bbox_dn_3: 0.05451  loss_giou_dn_3: 0.2794  loss_ce_4: 1.811  loss_mask_4: 0.07744  loss_dice_4: 0.8859  loss_bbox_4: 0.09179  loss_giou_4: 0.4603  loss_ce_dn_4: 1.022  loss_mask_dn_4: 0.06286  loss_dice_dn_4: 0.5562  loss_bbox_dn_4: 0.04885  loss_giou_dn_4: 0.2668  loss_ce_5: 1.743  loss_mask_5: 0.06394  loss_dice_5: 0.8687  loss_bbox_5: 0.1066  loss_giou_5: 0.4634  loss_ce_dn_5: 1.034  loss_mask_dn_5: 0.05882  loss_dice_dn_5: 0.5325  loss_bbox_dn_5: 0.04644  loss_giou_dn_5: 0.2657  loss_ce_6: 1.76  loss_mask_6: 0.07298  loss_dice_6: 0.8143  loss_bbox_6: 0.08512  loss_giou_6: 0.4691  loss_ce_dn_6: 1.069  loss_mask_dn_6: 0.0591  loss_dice_dn_6: 0.5473  loss_bbox_dn_6: 0.04415  loss_giou_dn_6: 0.2607  loss_ce_7: 1.744  loss_mask_7: 0.0824  loss_dice_7: 0.7913  loss_bbox_7: 0.1075  loss_giou_7: 0.4582  loss_ce_dn_7: 1.077  loss_mask_dn_7: 0.05722  loss_dice_dn_7: 0.545  loss_bbox_dn_7: 0.04357  loss_giou_dn_7: 0.2614  loss_ce_8: 1.711  loss_mask_8: 0.07744  loss_dice_8: 0.8342  loss_bbox_8: 0.1092  loss_giou_8: 0.4579  loss_ce_dn_8: 1.109  loss_mask_dn_8: 0.05897  loss_dice_dn_8: 0.5438  loss_bbox_dn_8: 0.04301  loss_giou_dn_8: 0.2563  loss_ce_interm: 2.24  loss_mask_interm: 0.06886  loss_dice_interm: 0.8646  loss_bbox_interm: 0.1812  loss_giou_interm: 0.602    time: 0.7866  last_time: 0.7752  data_time: 0.0193  last_data_time: 0.0104   lr: 0.0001  max_mem: 20819M\n",
      "\u001b[32m[07/13 11:14:10 d2.utils.events]: \u001b[0m eta: 0:28:38  iter: 199  total_loss: 63.34  loss_ce: 1.559  loss_mask: 0.0721  loss_dice: 0.6248  loss_bbox: 0.08231  loss_giou: 0.3396  loss_ce_dn: 0.8669  loss_mask_dn: 0.04903  loss_dice_dn: 0.529  loss_bbox_dn: 0.04797  loss_giou_dn: 0.2177  loss_ce_0: 2.115  loss_mask_0: 0.07739  loss_dice_0: 0.6393  loss_bbox_0: 0.1249  loss_giou_0: 0.4272  loss_ce_dn_0: 8.991  loss_mask_dn_0: 0.4977  loss_dice_dn_0: 4.126  loss_bbox_dn_0: 0.2656  loss_giou_dn_0: 0.8583  loss_ce_1: 1.903  loss_mask_1: 0.07127  loss_dice_1: 0.6169  loss_bbox_1: 0.1112  loss_giou_1: 0.3849  loss_ce_dn_1: 1.009  loss_mask_dn_1: 0.06298  loss_dice_dn_1: 0.6028  loss_bbox_dn_1: 0.1119  loss_giou_dn_1: 0.4282  loss_ce_2: 1.738  loss_mask_2: 0.07288  loss_dice_2: 0.633  loss_bbox_2: 0.1123  loss_giou_2: 0.3631  loss_ce_dn_2: 0.8254  loss_mask_dn_2: 0.05183  loss_dice_dn_2: 0.5469  loss_bbox_dn_2: 0.07084  loss_giou_dn_2: 0.2979  loss_ce_3: 1.702  loss_mask_3: 0.06941  loss_dice_3: 0.6116  loss_bbox_3: 0.1089  loss_giou_3: 0.3371  loss_ce_dn_3: 0.7894  loss_mask_dn_3: 0.04682  loss_dice_dn_3: 0.5314  loss_bbox_dn_3: 0.05723  loss_giou_dn_3: 0.2412  loss_ce_4: 1.664  loss_mask_4: 0.06768  loss_dice_4: 0.6198  loss_bbox_4: 0.096  loss_giou_4: 0.3138  loss_ce_dn_4: 0.776  loss_mask_dn_4: 0.04638  loss_dice_dn_4: 0.5109  loss_bbox_dn_4: 0.0515  loss_giou_dn_4: 0.2208  loss_ce_5: 1.584  loss_mask_5: 0.07758  loss_dice_5: 0.6246  loss_bbox_5: 0.09678  loss_giou_5: 0.339  loss_ce_dn_5: 0.7979  loss_mask_dn_5: 0.04756  loss_dice_dn_5: 0.5213  loss_bbox_dn_5: 0.04928  loss_giou_dn_5: 0.2165  loss_ce_6: 1.587  loss_mask_6: 0.07081  loss_dice_6: 0.6698  loss_bbox_6: 0.08278  loss_giou_6: 0.3295  loss_ce_dn_6: 0.8081  loss_mask_dn_6: 0.05343  loss_dice_dn_6: 0.5245  loss_bbox_dn_6: 0.04845  loss_giou_dn_6: 0.2157  loss_ce_7: 1.581  loss_mask_7: 0.07863  loss_dice_7: 0.6438  loss_bbox_7: 0.08784  loss_giou_7: 0.3356  loss_ce_dn_7: 0.8312  loss_mask_dn_7: 0.05063  loss_dice_dn_7: 0.5215  loss_bbox_dn_7: 0.04838  loss_giou_dn_7: 0.2136  loss_ce_8: 1.572  loss_mask_8: 0.06751  loss_dice_8: 0.5494  loss_bbox_8: 0.08542  loss_giou_8: 0.3277  loss_ce_dn_8: 0.8655  loss_mask_dn_8: 0.04667  loss_dice_dn_8: 0.5155  loss_bbox_dn_8: 0.04791  loss_giou_dn_8: 0.2145  loss_ce_interm: 2.126  loss_mask_interm: 0.08098  loss_dice_interm: 0.5605  loss_bbox_interm: 0.1362  loss_giou_interm: 0.5068    time: 0.7871  last_time: 0.7866  data_time: 0.0162  last_data_time: 0.0071   lr: 0.0001  max_mem: 20819M\n",
      "\u001b[32m[07/13 11:14:26 d2.utils.events]: \u001b[0m eta: 0:28:23  iter: 219  total_loss: 56.12  loss_ce: 1.434  loss_mask: 0.07899  loss_dice: 0.4971  loss_bbox: 0.09  loss_giou: 0.309  loss_ce_dn: 0.7625  loss_mask_dn: 0.08587  loss_dice_dn: 0.3789  loss_bbox_dn: 0.04868  loss_giou_dn: 0.1786  loss_ce_0: 1.838  loss_mask_0: 0.06947  loss_dice_0: 0.5236  loss_bbox_0: 0.09429  loss_giou_0: 0.3599  loss_ce_dn_0: 8.438  loss_mask_dn_0: 0.6972  loss_dice_dn_0: 4.362  loss_bbox_dn_0: 0.3507  loss_giou_dn_0: 0.8554  loss_ce_1: 1.728  loss_mask_1: 0.08237  loss_dice_1: 0.5362  loss_bbox_1: 0.09497  loss_giou_1: 0.3184  loss_ce_dn_1: 0.9002  loss_mask_dn_1: 0.1032  loss_dice_dn_1: 0.5054  loss_bbox_dn_1: 0.1263  loss_giou_dn_1: 0.4043  loss_ce_2: 1.594  loss_mask_2: 0.08027  loss_dice_2: 0.531  loss_bbox_2: 0.09105  loss_giou_2: 0.2969  loss_ce_dn_2: 0.7304  loss_mask_dn_2: 0.08597  loss_dice_dn_2: 0.4208  loss_bbox_dn_2: 0.07737  loss_giou_dn_2: 0.2549  loss_ce_3: 1.525  loss_mask_3: 0.09019  loss_dice_3: 0.499  loss_bbox_3: 0.08815  loss_giou_3: 0.2886  loss_ce_dn_3: 0.674  loss_mask_dn_3: 0.08426  loss_dice_dn_3: 0.4185  loss_bbox_dn_3: 0.06262  loss_giou_dn_3: 0.2229  loss_ce_4: 1.485  loss_mask_4: 0.0829  loss_dice_4: 0.5327  loss_bbox_4: 0.08845  loss_giou_4: 0.2881  loss_ce_dn_4: 0.6552  loss_mask_dn_4: 0.08672  loss_dice_dn_4: 0.409  loss_bbox_dn_4: 0.05367  loss_giou_dn_4: 0.1978  loss_ce_5: 1.445  loss_mask_5: 0.07586  loss_dice_5: 0.5011  loss_bbox_5: 0.08852  loss_giou_5: 0.3073  loss_ce_dn_5: 0.6692  loss_mask_dn_5: 0.08206  loss_dice_dn_5: 0.394  loss_bbox_dn_5: 0.0534  loss_giou_dn_5: 0.1871  loss_ce_6: 1.444  loss_mask_6: 0.07818  loss_dice_6: 0.5284  loss_bbox_6: 0.08599  loss_giou_6: 0.2868  loss_ce_dn_6: 0.692  loss_mask_dn_6: 0.0805  loss_dice_dn_6: 0.3901  loss_bbox_dn_6: 0.05105  loss_giou_dn_6: 0.1805  loss_ce_7: 1.425  loss_mask_7: 0.08096  loss_dice_7: 0.5192  loss_bbox_7: 0.08139  loss_giou_7: 0.3065  loss_ce_dn_7: 0.7172  loss_mask_dn_7: 0.08425  loss_dice_dn_7: 0.3886  loss_bbox_dn_7: 0.05066  loss_giou_dn_7: 0.1802  loss_ce_8: 1.425  loss_mask_8: 0.07861  loss_dice_8: 0.4905  loss_bbox_8: 0.08205  loss_giou_8: 0.3045  loss_ce_dn_8: 0.742  loss_mask_dn_8: 0.08388  loss_dice_dn_8: 0.3915  loss_bbox_dn_8: 0.04926  loss_giou_dn_8: 0.1767  loss_ce_interm: 1.847  loss_mask_interm: 0.07747  loss_dice_interm: 0.534  loss_bbox_interm: 0.15  loss_giou_interm: 0.4145    time: 0.7877  last_time: 0.7721  data_time: 0.0169  last_data_time: 0.0078   lr: 0.0001  max_mem: 20819M\n",
      "\u001b[32m[07/13 11:14:42 d2.utils.events]: \u001b[0m eta: 0:28:08  iter: 239  total_loss: 57.36  loss_ce: 1.385  loss_mask: 0.04944  loss_dice: 0.5509  loss_bbox: 0.0757  loss_giou: 0.3249  loss_ce_dn: 0.6266  loss_mask_dn: 0.04244  loss_dice_dn: 0.4787  loss_bbox_dn: 0.04182  loss_giou_dn: 0.2193  loss_ce_0: 1.856  loss_mask_0: 0.04174  loss_dice_0: 0.5768  loss_bbox_0: 0.09398  loss_giou_0: 0.3944  loss_ce_dn_0: 7.333  loss_mask_dn_0: 0.6956  loss_dice_dn_0: 4.006  loss_bbox_dn_0: 0.321  loss_giou_dn_0: 0.8602  loss_ce_1: 1.712  loss_mask_1: 0.05983  loss_dice_1: 0.5969  loss_bbox_1: 0.08272  loss_giou_1: 0.3704  loss_ce_dn_1: 0.8072  loss_mask_dn_1: 0.05038  loss_dice_dn_1: 0.6194  loss_bbox_dn_1: 0.11  loss_giou_dn_1: 0.423  loss_ce_2: 1.538  loss_mask_2: 0.05599  loss_dice_2: 0.5464  loss_bbox_2: 0.0825  loss_giou_2: 0.332  loss_ce_dn_2: 0.6106  loss_mask_dn_2: 0.04647  loss_dice_dn_2: 0.53  loss_bbox_dn_2: 0.06948  loss_giou_dn_2: 0.2971  loss_ce_3: 1.466  loss_mask_3: 0.04885  loss_dice_3: 0.6637  loss_bbox_3: 0.07996  loss_giou_3: 0.3568  loss_ce_dn_3: 0.5693  loss_mask_dn_3: 0.04342  loss_dice_dn_3: 0.498  loss_bbox_dn_3: 0.05072  loss_giou_dn_3: 0.2567  loss_ce_4: 1.422  loss_mask_4: 0.04974  loss_dice_4: 0.5385  loss_bbox_4: 0.07815  loss_giou_4: 0.3278  loss_ce_dn_4: 0.5535  loss_mask_dn_4: 0.04311  loss_dice_dn_4: 0.4731  loss_bbox_dn_4: 0.04709  loss_giou_dn_4: 0.2355  loss_ce_5: 1.479  loss_mask_5: 0.04913  loss_dice_5: 0.5487  loss_bbox_5: 0.0763  loss_giou_5: 0.3486  loss_ce_dn_5: 0.579  loss_mask_dn_5: 0.043  loss_dice_dn_5: 0.4851  loss_bbox_dn_5: 0.04403  loss_giou_dn_5: 0.229  loss_ce_6: 1.415  loss_mask_6: 0.04523  loss_dice_6: 0.5037  loss_bbox_6: 0.07468  loss_giou_6: 0.3247  loss_ce_dn_6: 0.5817  loss_mask_dn_6: 0.04345  loss_dice_dn_6: 0.4973  loss_bbox_dn_6: 0.04349  loss_giou_dn_6: 0.2248  loss_ce_7: 1.394  loss_mask_7: 0.05188  loss_dice_7: 0.5271  loss_bbox_7: 0.07697  loss_giou_7: 0.3197  loss_ce_dn_7: 0.6091  loss_mask_dn_7: 0.04195  loss_dice_dn_7: 0.4972  loss_bbox_dn_7: 0.04338  loss_giou_dn_7: 0.2242  loss_ce_8: 1.375  loss_mask_8: 0.05205  loss_dice_8: 0.5722  loss_bbox_8: 0.07552  loss_giou_8: 0.3266  loss_ce_dn_8: 0.6172  loss_mask_dn_8: 0.04191  loss_dice_dn_8: 0.4797  loss_bbox_dn_8: 0.04195  loss_giou_dn_8: 0.2199  loss_ce_interm: 1.842  loss_mask_interm: 0.05301  loss_dice_interm: 0.5679  loss_bbox_interm: 0.1091  loss_giou_interm: 0.4583    time: 0.7875  last_time: 0.7864  data_time: 0.0095  last_data_time: 0.0084   lr: 0.0001  max_mem: 20819M\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[07/13 11:14:58 d2.utils.events]: \u001b[0m eta: 0:27:53  iter: 259  total_loss: 49.14  loss_ce: 1.281  loss_mask: 0.06543  loss_dice: 0.4279  loss_bbox: 0.05621  loss_giou: 0.2599  loss_ce_dn: 0.5548  loss_mask_dn: 0.06901  loss_dice_dn: 0.3887  loss_bbox_dn: 0.04337  loss_giou_dn: 0.1726  loss_ce_0: 1.716  loss_mask_0: 0.04925  loss_dice_0: 0.4742  loss_bbox_0: 0.07607  loss_giou_0: 0.3293  loss_ce_dn_0: 6.912  loss_mask_dn_0: 0.8401  loss_dice_dn_0: 3.934  loss_bbox_dn_0: 0.3978  loss_giou_dn_0: 0.8518  loss_ce_1: 1.541  loss_mask_1: 0.06205  loss_dice_1: 0.4755  loss_bbox_1: 0.07538  loss_giou_1: 0.2857  loss_ce_dn_1: 0.7024  loss_mask_dn_1: 0.06716  loss_dice_dn_1: 0.5033  loss_bbox_dn_1: 0.1179  loss_giou_dn_1: 0.3867  loss_ce_2: 1.403  loss_mask_2: 0.06893  loss_dice_2: 0.4684  loss_bbox_2: 0.07074  loss_giou_2: 0.2714  loss_ce_dn_2: 0.5331  loss_mask_dn_2: 0.06546  loss_dice_dn_2: 0.4201  loss_bbox_dn_2: 0.06541  loss_giou_dn_2: 0.2443  loss_ce_3: 1.381  loss_mask_3: 0.07931  loss_dice_3: 0.455  loss_bbox_3: 0.06155  loss_giou_3: 0.2826  loss_ce_dn_3: 0.4969  loss_mask_dn_3: 0.06017  loss_dice_dn_3: 0.4013  loss_bbox_dn_3: 0.05315  loss_giou_dn_3: 0.1984  loss_ce_4: 1.337  loss_mask_4: 0.05378  loss_dice_4: 0.4815  loss_bbox_4: 0.05958  loss_giou_4: 0.2818  loss_ce_dn_4: 0.4925  loss_mask_dn_4: 0.06285  loss_dice_dn_4: 0.3945  loss_bbox_dn_4: 0.04571  loss_giou_dn_4: 0.1851  loss_ce_5: 1.315  loss_mask_5: 0.05776  loss_dice_5: 0.4796  loss_bbox_5: 0.06191  loss_giou_5: 0.2691  loss_ce_dn_5: 0.492  loss_mask_dn_5: 0.06293  loss_dice_dn_5: 0.394  loss_bbox_dn_5: 0.04413  loss_giou_dn_5: 0.18  loss_ce_6: 1.295  loss_mask_6: 0.06953  loss_dice_6: 0.4556  loss_bbox_6: 0.05729  loss_giou_6: 0.2773  loss_ce_dn_6: 0.5077  loss_mask_dn_6: 0.06579  loss_dice_dn_6: 0.3915  loss_bbox_dn_6: 0.04292  loss_giou_dn_6: 0.1739  loss_ce_7: 1.301  loss_mask_7: 0.06504  loss_dice_7: 0.4594  loss_bbox_7: 0.0575  loss_giou_7: 0.2606  loss_ce_dn_7: 0.5281  loss_mask_dn_7: 0.06817  loss_dice_dn_7: 0.3874  loss_bbox_dn_7: 0.04301  loss_giou_dn_7: 0.1763  loss_ce_8: 1.322  loss_mask_8: 0.04849  loss_dice_8: 0.4277  loss_bbox_8: 0.05563  loss_giou_8: 0.2626  loss_ce_dn_8: 0.5452  loss_mask_dn_8: 0.06703  loss_dice_dn_8: 0.3844  loss_bbox_dn_8: 0.04291  loss_giou_dn_8: 0.1708  loss_ce_interm: 1.723  loss_mask_interm: 0.05447  loss_dice_interm: 0.4515  loss_bbox_interm: 0.1017  loss_giou_interm: 0.3443    time: 0.7874  last_time: 0.7969  data_time: 0.0125  last_data_time: 0.0136   lr: 0.0001  max_mem: 20819M\n",
      "\u001b[32m[07/13 11:15:14 d2.utils.events]: \u001b[0m eta: 0:27:38  iter: 279  total_loss: 46.18  loss_ce: 1.099  loss_mask: 0.05735  loss_dice: 0.443  loss_bbox: 0.05654  loss_giou: 0.2477  loss_ce_dn: 0.424  loss_mask_dn: 0.04871  loss_dice_dn: 0.3691  loss_bbox_dn: 0.04974  loss_giou_dn: 0.1808  loss_ce_0: 1.651  loss_mask_0: 0.0486  loss_dice_0: 0.4498  loss_bbox_0: 0.06559  loss_giou_0: 0.2998  loss_ce_dn_0: 6.385  loss_mask_dn_0: 0.6689  loss_dice_dn_0: 3.976  loss_bbox_dn_0: 0.3502  loss_giou_dn_0: 0.855  loss_ce_1: 1.495  loss_mask_1: 0.05907  loss_dice_1: 0.4327  loss_bbox_1: 0.06058  loss_giou_1: 0.2541  loss_ce_dn_1: 0.65  loss_mask_dn_1: 0.06206  loss_dice_dn_1: 0.4114  loss_bbox_dn_1: 0.1146  loss_giou_dn_1: 0.3745  loss_ce_2: 1.343  loss_mask_2: 0.05604  loss_dice_2: 0.4278  loss_bbox_2: 0.0614  loss_giou_2: 0.2275  loss_ce_dn_2: 0.4726  loss_mask_dn_2: 0.05384  loss_dice_dn_2: 0.3704  loss_bbox_dn_2: 0.07257  loss_giou_dn_2: 0.2471  loss_ce_3: 1.284  loss_mask_3: 0.05559  loss_dice_3: 0.4441  loss_bbox_3: 0.06979  loss_giou_3: 0.2424  loss_ce_dn_3: 0.4265  loss_mask_dn_3: 0.0585  loss_dice_dn_3: 0.3726  loss_bbox_dn_3: 0.05826  loss_giou_dn_3: 0.2101  loss_ce_4: 1.189  loss_mask_4: 0.05606  loss_dice_4: 0.4314  loss_bbox_4: 0.06217  loss_giou_4: 0.2409  loss_ce_dn_4: 0.392  loss_mask_dn_4: 0.05436  loss_dice_dn_4: 0.3602  loss_bbox_dn_4: 0.05473  loss_giou_dn_4: 0.1945  loss_ce_5: 1.139  loss_mask_5: 0.05935  loss_dice_5: 0.4164  loss_bbox_5: 0.05779  loss_giou_5: 0.2296  loss_ce_dn_5: 0.3791  loss_mask_dn_5: 0.05421  loss_dice_dn_5: 0.3603  loss_bbox_dn_5: 0.05193  loss_giou_dn_5: 0.1889  loss_ce_6: 1.128  loss_mask_6: 0.06134  loss_dice_6: 0.4378  loss_bbox_6: 0.05725  loss_giou_6: 0.2469  loss_ce_dn_6: 0.3916  loss_mask_dn_6: 0.05225  loss_dice_dn_6: 0.3682  loss_bbox_dn_6: 0.05077  loss_giou_dn_6: 0.1828  loss_ce_7: 1.129  loss_mask_7: 0.0607  loss_dice_7: 0.4264  loss_bbox_7: 0.05624  loss_giou_7: 0.241  loss_ce_dn_7: 0.3987  loss_mask_dn_7: 0.04999  loss_dice_dn_7: 0.3575  loss_bbox_dn_7: 0.0505  loss_giou_dn_7: 0.1879  loss_ce_8: 1.125  loss_mask_8: 0.05965  loss_dice_8: 0.423  loss_bbox_8: 0.05859  loss_giou_8: 0.2398  loss_ce_dn_8: 0.4155  loss_mask_dn_8: 0.04912  loss_dice_dn_8: 0.3596  loss_bbox_dn_8: 0.04922  loss_giou_dn_8: 0.1803  loss_ce_interm: 1.652  loss_mask_interm: 0.05221  loss_dice_interm: 0.417  loss_bbox_interm: 0.1129  loss_giou_interm: 0.3529    time: 0.7877  last_time: 0.8238  data_time: 0.0153  last_data_time: 0.0535   lr: 0.0001  max_mem: 20819M\n",
      "\u001b[32m[07/13 11:15:30 d2.utils.events]: \u001b[0m eta: 0:27:22  iter: 299  total_loss: 50.08  loss_ce: 1.287  loss_mask: 0.04202  loss_dice: 0.551  loss_bbox: 0.05403  loss_giou: 0.2733  loss_ce_dn: 0.4418  loss_mask_dn: 0.04286  loss_dice_dn: 0.423  loss_bbox_dn: 0.03528  loss_giou_dn: 0.1957  loss_ce_0: 1.747  loss_mask_0: 0.05843  loss_dice_0: 0.5887  loss_bbox_0: 0.0806  loss_giou_0: 0.4485  loss_ce_dn_0: 5.801  loss_mask_dn_0: 0.463  loss_dice_dn_0: 3.888  loss_bbox_dn_0: 0.2768  loss_giou_dn_0: 0.8551  loss_ce_1: 1.548  loss_mask_1: 0.05195  loss_dice_1: 0.6171  loss_bbox_1: 0.06185  loss_giou_1: 0.3473  loss_ce_dn_1: 0.5553  loss_mask_dn_1: 0.06711  loss_dice_dn_1: 0.4942  loss_bbox_dn_1: 0.08999  loss_giou_dn_1: 0.3739  loss_ce_2: 1.448  loss_mask_2: 0.04508  loss_dice_2: 0.5412  loss_bbox_2: 0.0672  loss_giou_2: 0.3351  loss_ce_dn_2: 0.4525  loss_mask_dn_2: 0.04847  loss_dice_dn_2: 0.4554  loss_bbox_dn_2: 0.05475  loss_giou_dn_2: 0.2629  loss_ce_3: 1.349  loss_mask_3: 0.0441  loss_dice_3: 0.5494  loss_bbox_3: 0.06217  loss_giou_3: 0.3064  loss_ce_dn_3: 0.428  loss_mask_dn_3: 0.04865  loss_dice_dn_3: 0.4486  loss_bbox_dn_3: 0.04436  loss_giou_dn_3: 0.2235  loss_ce_4: 1.353  loss_mask_4: 0.04743  loss_dice_4: 0.5695  loss_bbox_4: 0.05992  loss_giou_4: 0.3026  loss_ce_dn_4: 0.4134  loss_mask_dn_4: 0.04598  loss_dice_dn_4: 0.428  loss_bbox_dn_4: 0.03934  loss_giou_dn_4: 0.2078  loss_ce_5: 1.34  loss_mask_5: 0.04467  loss_dice_5: 0.5683  loss_bbox_5: 0.06027  loss_giou_5: 0.2877  loss_ce_dn_5: 0.4137  loss_mask_dn_5: 0.04502  loss_dice_dn_5: 0.4222  loss_bbox_dn_5: 0.03827  loss_giou_dn_5: 0.2005  loss_ce_6: 1.329  loss_mask_6: 0.04489  loss_dice_6: 0.528  loss_bbox_6: 0.0547  loss_giou_6: 0.2837  loss_ce_dn_6: 0.4099  loss_mask_dn_6: 0.04356  loss_dice_dn_6: 0.4526  loss_bbox_dn_6: 0.0372  loss_giou_dn_6: 0.1958  loss_ce_7: 1.296  loss_mask_7: 0.04118  loss_dice_7: 0.5151  loss_bbox_7: 0.05704  loss_giou_7: 0.2813  loss_ce_dn_7: 0.4186  loss_mask_dn_7: 0.04404  loss_dice_dn_7: 0.434  loss_bbox_dn_7: 0.03652  loss_giou_dn_7: 0.196  loss_ce_8: 1.29  loss_mask_8: 0.04408  loss_dice_8: 0.5479  loss_bbox_8: 0.05567  loss_giou_8: 0.2763  loss_ce_dn_8: 0.4309  loss_mask_dn_8: 0.04211  loss_dice_dn_8: 0.4298  loss_bbox_dn_8: 0.03553  loss_giou_dn_8: 0.1948  loss_ce_interm: 1.747  loss_mask_interm: 0.05897  loss_dice_interm: 0.5461  loss_bbox_interm: 0.09376  loss_giou_interm: 0.4016    time: 0.7878  last_time: 0.7851  data_time: 0.0167  last_data_time: 0.0080   lr: 0.0001  max_mem: 20819M\n",
      "\u001b[32m[07/13 11:15:46 d2.utils.events]: \u001b[0m eta: 0:27:07  iter: 319  total_loss: 46.59  loss_ce: 1.094  loss_mask: 0.06272  loss_dice: 0.5034  loss_bbox: 0.05923  loss_giou: 0.2522  loss_ce_dn: 0.3745  loss_mask_dn: 0.05105  loss_dice_dn: 0.4052  loss_bbox_dn: 0.04066  loss_giou_dn: 0.1887  loss_ce_0: 1.558  loss_mask_0: 0.05436  loss_dice_0: 0.5283  loss_bbox_0: 0.07706  loss_giou_0: 0.3632  loss_ce_dn_0: 5.152  loss_mask_dn_0: 0.6209  loss_dice_dn_0: 3.728  loss_bbox_dn_0: 0.3538  loss_giou_dn_0: 0.8531  loss_ce_1: 1.422  loss_mask_1: 0.06289  loss_dice_1: 0.5312  loss_bbox_1: 0.07266  loss_giou_1: 0.3185  loss_ce_dn_1: 0.5586  loss_mask_dn_1: 0.06616  loss_dice_dn_1: 0.4799  loss_bbox_dn_1: 0.1076  loss_giou_dn_1: 0.3506  loss_ce_2: 1.285  loss_mask_2: 0.05557  loss_dice_2: 0.4834  loss_bbox_2: 0.07123  loss_giou_2: 0.2886  loss_ce_dn_2: 0.4269  loss_mask_dn_2: 0.0526  loss_dice_dn_2: 0.4193  loss_bbox_dn_2: 0.06424  loss_giou_dn_2: 0.2468  loss_ce_3: 1.254  loss_mask_3: 0.0558  loss_dice_3: 0.4829  loss_bbox_3: 0.06627  loss_giou_3: 0.2554  loss_ce_dn_3: 0.3743  loss_mask_dn_3: 0.05345  loss_dice_dn_3: 0.4207  loss_bbox_dn_3: 0.05214  loss_giou_dn_3: 0.2125  loss_ce_4: 1.19  loss_mask_4: 0.05877  loss_dice_4: 0.4936  loss_bbox_4: 0.0645  loss_giou_4: 0.2526  loss_ce_dn_4: 0.3701  loss_mask_dn_4: 0.04908  loss_dice_dn_4: 0.3903  loss_bbox_dn_4: 0.0462  loss_giou_dn_4: 0.201  loss_ce_5: 1.189  loss_mask_5: 0.061  loss_dice_5: 0.4814  loss_bbox_5: 0.06611  loss_giou_5: 0.2496  loss_ce_dn_5: 0.3693  loss_mask_dn_5: 0.04752  loss_dice_dn_5: 0.4078  loss_bbox_dn_5: 0.04431  loss_giou_dn_5: 0.1915  loss_ce_6: 1.145  loss_mask_6: 0.06096  loss_dice_6: 0.4794  loss_bbox_6: 0.06393  loss_giou_6: 0.2492  loss_ce_dn_6: 0.3767  loss_mask_dn_6: 0.04928  loss_dice_dn_6: 0.4036  loss_bbox_dn_6: 0.04179  loss_giou_dn_6: 0.1878  loss_ce_7: 1.105  loss_mask_7: 0.06887  loss_dice_7: 0.4747  loss_bbox_7: 0.06108  loss_giou_7: 0.2516  loss_ce_dn_7: 0.3713  loss_mask_dn_7: 0.04856  loss_dice_dn_7: 0.4104  loss_bbox_dn_7: 0.04189  loss_giou_dn_7: 0.188  loss_ce_8: 1.125  loss_mask_8: 0.07197  loss_dice_8: 0.4808  loss_bbox_8: 0.06458  loss_giou_8: 0.2533  loss_ce_dn_8: 0.3741  loss_mask_dn_8: 0.05031  loss_dice_dn_8: 0.4092  loss_bbox_dn_8: 0.04101  loss_giou_dn_8: 0.1877  loss_ce_interm: 1.494  loss_mask_interm: 0.05629  loss_dice_interm: 0.4747  loss_bbox_interm: 0.09718  loss_giou_interm: 0.3475    time: 0.7882  last_time: 0.7825  data_time: 0.0162  last_data_time: 0.0093   lr: 0.0001  max_mem: 20850M\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[07/13 11:16:01 d2.utils.events]: \u001b[0m eta: 0:26:51  iter: 339  total_loss: 44.23  loss_ce: 1.058  loss_mask: 0.09724  loss_dice: 0.4453  loss_bbox: 0.05235  loss_giou: 0.2674  loss_ce_dn: 0.3426  loss_mask_dn: 0.08161  loss_dice_dn: 0.4239  loss_bbox_dn: 0.03368  loss_giou_dn: 0.1937  loss_ce_0: 1.489  loss_mask_0: 0.08411  loss_dice_0: 0.5234  loss_bbox_0: 0.0773  loss_giou_0: 0.3539  loss_ce_dn_0: 5.028  loss_mask_dn_0: 0.5879  loss_dice_dn_0: 3.698  loss_bbox_dn_0: 0.3401  loss_giou_dn_0: 0.8492  loss_ce_1: 1.339  loss_mask_1: 0.09238  loss_dice_1: 0.5232  loss_bbox_1: 0.07617  loss_giou_1: 0.3631  loss_ce_dn_1: 0.5727  loss_mask_dn_1: 0.07726  loss_dice_dn_1: 0.5188  loss_bbox_dn_1: 0.09621  loss_giou_dn_1: 0.346  loss_ce_2: 1.199  loss_mask_2: 0.07733  loss_dice_2: 0.5738  loss_bbox_2: 0.06465  loss_giou_2: 0.2818  loss_ce_dn_2: 0.4297  loss_mask_dn_2: 0.07443  loss_dice_dn_2: 0.4463  loss_bbox_dn_2: 0.05535  loss_giou_dn_2: 0.2499  loss_ce_3: 1.108  loss_mask_3: 0.08038  loss_dice_3: 0.541  loss_bbox_3: 0.06175  loss_giou_3: 0.2917  loss_ce_dn_3: 0.3575  loss_mask_dn_3: 0.07064  loss_dice_dn_3: 0.4155  loss_bbox_dn_3: 0.04335  loss_giou_dn_3: 0.213  loss_ce_4: 1.114  loss_mask_4: 0.07037  loss_dice_4: 0.4809  loss_bbox_4: 0.05475  loss_giou_4: 0.2822  loss_ce_dn_4: 0.3265  loss_mask_dn_4: 0.07227  loss_dice_dn_4: 0.3998  loss_bbox_dn_4: 0.03916  loss_giou_dn_4: 0.2044  loss_ce_5: 1.076  loss_mask_5: 0.07053  loss_dice_5: 0.4922  loss_bbox_5: 0.05367  loss_giou_5: 0.263  loss_ce_dn_5: 0.3326  loss_mask_dn_5: 0.06851  loss_dice_dn_5: 0.408  loss_bbox_dn_5: 0.03754  loss_giou_dn_5: 0.2023  loss_ce_6: 1.074  loss_mask_6: 0.08326  loss_dice_6: 0.5122  loss_bbox_6: 0.05006  loss_giou_6: 0.3039  loss_ce_dn_6: 0.3277  loss_mask_dn_6: 0.07423  loss_dice_dn_6: 0.4176  loss_bbox_dn_6: 0.03441  loss_giou_dn_6: 0.1948  loss_ce_7: 1.089  loss_mask_7: 0.08151  loss_dice_7: 0.4912  loss_bbox_7: 0.04869  loss_giou_7: 0.2682  loss_ce_dn_7: 0.3368  loss_mask_dn_7: 0.07174  loss_dice_dn_7: 0.411  loss_bbox_dn_7: 0.03481  loss_giou_dn_7: 0.1955  loss_ce_8: 1.082  loss_mask_8: 0.08278  loss_dice_8: 0.477  loss_bbox_8: 0.04876  loss_giou_8: 0.2654  loss_ce_dn_8: 0.3343  loss_mask_dn_8: 0.07446  loss_dice_dn_8: 0.4302  loss_bbox_dn_8: 0.03332  loss_giou_dn_8: 0.1928  loss_ce_interm: 1.497  loss_mask_interm: 0.08271  loss_dice_interm: 0.4631  loss_bbox_interm: 0.09058  loss_giou_interm: 0.3438    time: 0.7882  last_time: 0.7807  data_time: 0.0178  last_data_time: 0.0081   lr: 0.0001  max_mem: 20850M\n",
      "\u001b[32m[07/13 11:16:17 d2.utils.events]: \u001b[0m eta: 0:26:35  iter: 359  total_loss: 45.15  loss_ce: 1.02  loss_mask: 0.09794  loss_dice: 0.5005  loss_bbox: 0.05648  loss_giou: 0.2634  loss_ce_dn: 0.3541  loss_mask_dn: 0.09533  loss_dice_dn: 0.395  loss_bbox_dn: 0.04371  loss_giou_dn: 0.1897  loss_ce_0: 1.509  loss_mask_0: 0.1034  loss_dice_0: 0.5333  loss_bbox_0: 0.0704  loss_giou_0: 0.3752  loss_ce_dn_0: 5.122  loss_mask_dn_0: 0.6128  loss_dice_dn_0: 3.7  loss_bbox_dn_0: 0.3638  loss_giou_dn_0: 0.8557  loss_ce_1: 1.345  loss_mask_1: 0.109  loss_dice_1: 0.5842  loss_bbox_1: 0.0582  loss_giou_1: 0.3036  loss_ce_dn_1: 0.5246  loss_mask_dn_1: 0.09951  loss_dice_dn_1: 0.4934  loss_bbox_dn_1: 0.11  loss_giou_dn_1: 0.3637  loss_ce_2: 1.156  loss_mask_2: 0.09518  loss_dice_2: 0.5175  loss_bbox_2: 0.05778  loss_giou_2: 0.2588  loss_ce_dn_2: 0.3905  loss_mask_dn_2: 0.09882  loss_dice_dn_2: 0.4384  loss_bbox_dn_2: 0.06517  loss_giou_dn_2: 0.2559  loss_ce_3: 1.104  loss_mask_3: 0.09889  loss_dice_3: 0.5355  loss_bbox_3: 0.05697  loss_giou_3: 0.2882  loss_ce_dn_3: 0.3533  loss_mask_dn_3: 0.09603  loss_dice_dn_3: 0.4051  loss_bbox_dn_3: 0.05481  loss_giou_dn_3: 0.2144  loss_ce_4: 1.045  loss_mask_4: 0.09423  loss_dice_4: 0.546  loss_bbox_4: 0.05444  loss_giou_4: 0.2613  loss_ce_dn_4: 0.3297  loss_mask_dn_4: 0.1035  loss_dice_dn_4: 0.4143  loss_bbox_dn_4: 0.04893  loss_giou_dn_4: 0.1958  loss_ce_5: 1.067  loss_mask_5: 0.1003  loss_dice_5: 0.5212  loss_bbox_5: 0.05881  loss_giou_5: 0.2605  loss_ce_dn_5: 0.326  loss_mask_dn_5: 0.09656  loss_dice_dn_5: 0.4011  loss_bbox_dn_5: 0.04879  loss_giou_dn_5: 0.1966  loss_ce_6: 1.015  loss_mask_6: 0.09883  loss_dice_6: 0.4851  loss_bbox_6: 0.05619  loss_giou_6: 0.2594  loss_ce_dn_6: 0.3288  loss_mask_dn_6: 0.1006  loss_dice_dn_6: 0.3934  loss_bbox_dn_6: 0.04435  loss_giou_dn_6: 0.1873  loss_ce_7: 1.015  loss_mask_7: 0.09516  loss_dice_7: 0.5454  loss_bbox_7: 0.05497  loss_giou_7: 0.2624  loss_ce_dn_7: 0.3406  loss_mask_dn_7: 0.09721  loss_dice_dn_7: 0.3891  loss_bbox_dn_7: 0.04408  loss_giou_dn_7: 0.1881  loss_ce_8: 1.028  loss_mask_8: 0.09857  loss_dice_8: 0.4797  loss_bbox_8: 0.05573  loss_giou_8: 0.2641  loss_ce_dn_8: 0.351  loss_mask_dn_8: 0.09756  loss_dice_dn_8: 0.4026  loss_bbox_dn_8: 0.04361  loss_giou_dn_8: 0.1883  loss_ce_interm: 1.48  loss_mask_interm: 0.1054  loss_dice_interm: 0.5072  loss_bbox_interm: 0.1092  loss_giou_interm: 0.3791    time: 0.7881  last_time: 0.8369  data_time: 0.0150  last_data_time: 0.0700   lr: 0.0001  max_mem: 20850M\n",
      "\u001b[32m[07/13 11:16:33 d2.utils.events]: \u001b[0m eta: 0:26:19  iter: 379  total_loss: 48.56  loss_ce: 1.073  loss_mask: 0.04894  loss_dice: 0.598  loss_bbox: 0.07237  loss_giou: 0.3421  loss_ce_dn: 0.3321  loss_mask_dn: 0.04627  loss_dice_dn: 0.5013  loss_bbox_dn: 0.04202  loss_giou_dn: 0.2158  loss_ce_0: 1.594  loss_mask_0: 0.06245  loss_dice_0: 0.6877  loss_bbox_0: 0.08597  loss_giou_0: 0.4532  loss_ce_dn_0: 4.503  loss_mask_dn_0: 0.5698  loss_dice_dn_0: 3.672  loss_bbox_dn_0: 0.3048  loss_giou_dn_0: 0.8485  loss_ce_1: 1.415  loss_mask_1: 0.04918  loss_dice_1: 0.6799  loss_bbox_1: 0.07768  loss_giou_1: 0.4421  loss_ce_dn_1: 0.5109  loss_mask_dn_1: 0.05876  loss_dice_dn_1: 0.6183  loss_bbox_dn_1: 0.09027  loss_giou_dn_1: 0.3748  loss_ce_2: 1.277  loss_mask_2: 0.05022  loss_dice_2: 0.6256  loss_bbox_2: 0.08372  loss_giou_2: 0.3779  loss_ce_dn_2: 0.3853  loss_mask_dn_2: 0.04948  loss_dice_dn_2: 0.5249  loss_bbox_dn_2: 0.06206  loss_giou_dn_2: 0.268  loss_ce_3: 1.177  loss_mask_3: 0.05235  loss_dice_3: 0.6276  loss_bbox_3: 0.08006  loss_giou_3: 0.3719  loss_ce_dn_3: 0.3512  loss_mask_dn_3: 0.04515  loss_dice_dn_3: 0.5061  loss_bbox_dn_3: 0.0521  loss_giou_dn_3: 0.2384  loss_ce_4: 1.145  loss_mask_4: 0.05305  loss_dice_4: 0.6154  loss_bbox_4: 0.07903  loss_giou_4: 0.3532  loss_ce_dn_4: 0.3329  loss_mask_dn_4: 0.04495  loss_dice_dn_4: 0.5137  loss_bbox_dn_4: 0.04515  loss_giou_dn_4: 0.2289  loss_ce_5: 1.097  loss_mask_5: 0.04996  loss_dice_5: 0.61  loss_bbox_5: 0.08038  loss_giou_5: 0.345  loss_ce_dn_5: 0.3213  loss_mask_dn_5: 0.04378  loss_dice_dn_5: 0.493  loss_bbox_dn_5: 0.04549  loss_giou_dn_5: 0.2239  loss_ce_6: 1.09  loss_mask_6: 0.04678  loss_dice_6: 0.6171  loss_bbox_6: 0.07715  loss_giou_6: 0.3457  loss_ce_dn_6: 0.3171  loss_mask_dn_6: 0.04505  loss_dice_dn_6: 0.525  loss_bbox_dn_6: 0.04266  loss_giou_dn_6: 0.221  loss_ce_7: 1.096  loss_mask_7: 0.04895  loss_dice_7: 0.6184  loss_bbox_7: 0.07659  loss_giou_7: 0.3518  loss_ce_dn_7: 0.3223  loss_mask_dn_7: 0.04603  loss_dice_dn_7: 0.5194  loss_bbox_dn_7: 0.04277  loss_giou_dn_7: 0.2202  loss_ce_8: 1.073  loss_mask_8: 0.0483  loss_dice_8: 0.603  loss_bbox_8: 0.0721  loss_giou_8: 0.3433  loss_ce_dn_8: 0.3256  loss_mask_dn_8: 0.04656  loss_dice_dn_8: 0.4884  loss_bbox_dn_8: 0.04185  loss_giou_dn_8: 0.2153  loss_ce_interm: 1.601  loss_mask_interm: 0.05131  loss_dice_interm: 0.578  loss_bbox_interm: 0.1115  loss_giou_interm: 0.451    time: 0.7878  last_time: 0.7599  data_time: 0.0106  last_data_time: 0.0065   lr: 0.0001  max_mem: 20850M\n",
      "\u001b[32m[07/13 11:16:49 d2.utils.events]: \u001b[0m eta: 0:26:04  iter: 399  total_loss: 42.63  loss_ce: 0.9135  loss_mask: 0.07658  loss_dice: 0.4415  loss_bbox: 0.04947  loss_giou: 0.2392  loss_ce_dn: 0.3309  loss_mask_dn: 0.07409  loss_dice_dn: 0.4017  loss_bbox_dn: 0.04004  loss_giou_dn: 0.1703  loss_ce_0: 1.399  loss_mask_0: 0.08822  loss_dice_0: 0.5082  loss_bbox_0: 0.07832  loss_giou_0: 0.3753  loss_ce_dn_0: 3.984  loss_mask_dn_0: 0.6723  loss_dice_dn_0: 3.478  loss_bbox_dn_0: 0.3495  loss_giou_dn_0: 0.8505  loss_ce_1: 1.277  loss_mask_1: 0.07988  loss_dice_1: 0.4786  loss_bbox_1: 0.07722  loss_giou_1: 0.2788  loss_ce_dn_1: 0.4474  loss_mask_dn_1: 0.06684  loss_dice_dn_1: 0.4702  loss_bbox_dn_1: 0.1124  loss_giou_dn_1: 0.336  loss_ce_2: 1.126  loss_mask_2: 0.07704  loss_dice_2: 0.4845  loss_bbox_2: 0.05276  loss_giou_2: 0.2597  loss_ce_dn_2: 0.3444  loss_mask_dn_2: 0.06556  loss_dice_dn_2: 0.4268  loss_bbox_dn_2: 0.0633  loss_giou_dn_2: 0.2405  loss_ce_3: 1.031  loss_mask_3: 0.07904  loss_dice_3: 0.446  loss_bbox_3: 0.05054  loss_giou_3: 0.2605  loss_ce_dn_3: 0.324  loss_mask_dn_3: 0.07998  loss_dice_dn_3: 0.4011  loss_bbox_dn_3: 0.0527  loss_giou_dn_3: 0.1998  loss_ce_4: 0.9907  loss_mask_4: 0.08729  loss_dice_4: 0.4584  loss_bbox_4: 0.05142  loss_giou_4: 0.2469  loss_ce_dn_4: 0.309  loss_mask_dn_4: 0.07787  loss_dice_dn_4: 0.3972  loss_bbox_dn_4: 0.04472  loss_giou_dn_4: 0.1829  loss_ce_5: 0.9831  loss_mask_5: 0.08314  loss_dice_5: 0.4331  loss_bbox_5: 0.04958  loss_giou_5: 0.2439  loss_ce_dn_5: 0.3209  loss_mask_dn_5: 0.07552  loss_dice_dn_5: 0.4032  loss_bbox_dn_5: 0.04073  loss_giou_dn_5: 0.1766  loss_ce_6: 0.9708  loss_mask_6: 0.0969  loss_dice_6: 0.4264  loss_bbox_6: 0.04898  loss_giou_6: 0.2407  loss_ce_dn_6: 0.312  loss_mask_dn_6: 0.07377  loss_dice_dn_6: 0.4089  loss_bbox_dn_6: 0.04058  loss_giou_dn_6: 0.1733  loss_ce_7: 0.9556  loss_mask_7: 0.08289  loss_dice_7: 0.4372  loss_bbox_7: 0.04976  loss_giou_7: 0.2428  loss_ce_dn_7: 0.3128  loss_mask_dn_7: 0.07625  loss_dice_dn_7: 0.3956  loss_bbox_dn_7: 0.04082  loss_giou_dn_7: 0.1733  loss_ce_8: 0.982  loss_mask_8: 0.07863  loss_dice_8: 0.4377  loss_bbox_8: 0.04684  loss_giou_8: 0.2509  loss_ce_dn_8: 0.324  loss_mask_dn_8: 0.07552  loss_dice_dn_8: 0.4121  loss_bbox_dn_8: 0.04015  loss_giou_dn_8: 0.1701  loss_ce_interm: 1.419  loss_mask_interm: 0.07755  loss_dice_interm: 0.488  loss_bbox_interm: 0.1091  loss_giou_interm: 0.3625    time: 0.7882  last_time: 0.7819  data_time: 0.0204  last_data_time: 0.0066   lr: 0.0001  max_mem: 20850M\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[07/13 11:17:05 d2.utils.events]: \u001b[0m eta: 0:25:48  iter: 419  total_loss: 46.21  loss_ce: 0.9875  loss_mask: 0.1043  loss_dice: 0.5378  loss_bbox: 0.07691  loss_giou: 0.2902  loss_ce_dn: 0.3439  loss_mask_dn: 0.06852  loss_dice_dn: 0.4614  loss_bbox_dn: 0.04132  loss_giou_dn: 0.2038  loss_ce_0: 1.449  loss_mask_0: 0.1081  loss_dice_0: 0.5587  loss_bbox_0: 0.07942  loss_giou_0: 0.4424  loss_ce_dn_0: 4.052  loss_mask_dn_0: 0.7075  loss_dice_dn_0: 3.527  loss_bbox_dn_0: 0.3572  loss_giou_dn_0: 0.8544  loss_ce_1: 1.246  loss_mask_1: 0.1286  loss_dice_1: 0.5662  loss_bbox_1: 0.09006  loss_giou_1: 0.3572  loss_ce_dn_1: 0.4763  loss_mask_dn_1: 0.09796  loss_dice_dn_1: 0.5626  loss_bbox_dn_1: 0.1064  loss_giou_dn_1: 0.3497  loss_ce_2: 1.141  loss_mask_2: 0.1036  loss_dice_2: 0.642  loss_bbox_2: 0.07989  loss_giou_2: 0.331  loss_ce_dn_2: 0.3646  loss_mask_dn_2: 0.08519  loss_dice_dn_2: 0.4922  loss_bbox_dn_2: 0.06694  loss_giou_dn_2: 0.2581  loss_ce_3: 1.065  loss_mask_3: 0.09728  loss_dice_3: 0.5771  loss_bbox_3: 0.07993  loss_giou_3: 0.3184  loss_ce_dn_3: 0.324  loss_mask_dn_3: 0.08272  loss_dice_dn_3: 0.4817  loss_bbox_dn_3: 0.05371  loss_giou_dn_3: 0.2257  loss_ce_4: 1.053  loss_mask_4: 0.1159  loss_dice_4: 0.503  loss_bbox_4: 0.0758  loss_giou_4: 0.31  loss_ce_dn_4: 0.3215  loss_mask_dn_4: 0.08109  loss_dice_dn_4: 0.4923  loss_bbox_dn_4: 0.04812  loss_giou_dn_4: 0.2194  loss_ce_5: 1.013  loss_mask_5: 0.1212  loss_dice_5: 0.6103  loss_bbox_5: 0.0756  loss_giou_5: 0.3148  loss_ce_dn_5: 0.3213  loss_mask_dn_5: 0.082  loss_dice_dn_5: 0.49  loss_bbox_dn_5: 0.04609  loss_giou_dn_5: 0.2135  loss_ce_6: 1.016  loss_mask_6: 0.1016  loss_dice_6: 0.5293  loss_bbox_6: 0.07319  loss_giou_6: 0.3005  loss_ce_dn_6: 0.3149  loss_mask_dn_6: 0.0827  loss_dice_dn_6: 0.4835  loss_bbox_dn_6: 0.04257  loss_giou_dn_6: 0.2068  loss_ce_7: 0.9959  loss_mask_7: 0.09567  loss_dice_7: 0.5231  loss_bbox_7: 0.06031  loss_giou_7: 0.3022  loss_ce_dn_7: 0.3221  loss_mask_dn_7: 0.06756  loss_dice_dn_7: 0.4907  loss_bbox_dn_7: 0.04261  loss_giou_dn_7: 0.2073  loss_ce_8: 1.008  loss_mask_8: 0.09369  loss_dice_8: 0.602  loss_bbox_8: 0.07495  loss_giou_8: 0.2884  loss_ce_dn_8: 0.3321  loss_mask_dn_8: 0.06411  loss_dice_dn_8: 0.4745  loss_bbox_dn_8: 0.04124  loss_giou_dn_8: 0.2036  loss_ce_interm: 1.423  loss_mask_interm: 0.1198  loss_dice_interm: 0.5474  loss_bbox_interm: 0.1359  loss_giou_interm: 0.4296    time: 0.7884  last_time: 0.8000  data_time: 0.0185  last_data_time: 0.0081   lr: 0.0001  max_mem: 20850M\n",
      "\u001b[32m[07/13 11:17:20 d2.utils.events]: \u001b[0m eta: 0:25:33  iter: 439  total_loss: 43.57  loss_ce: 0.9272  loss_mask: 0.06475  loss_dice: 0.494  loss_bbox: 0.06067  loss_giou: 0.2813  loss_ce_dn: 0.3292  loss_mask_dn: 0.06939  loss_dice_dn: 0.4485  loss_bbox_dn: 0.03963  loss_giou_dn: 0.2169  loss_ce_0: 1.532  loss_mask_0: 0.07039  loss_dice_0: 0.5729  loss_bbox_0: 0.08575  loss_giou_0: 0.4456  loss_ce_dn_0: 3.706  loss_mask_dn_0: 0.6895  loss_dice_dn_0: 3.547  loss_bbox_dn_0: 0.3664  loss_giou_dn_0: 0.8537  loss_ce_1: 1.325  loss_mask_1: 0.07336  loss_dice_1: 0.608  loss_bbox_1: 0.08191  loss_giou_1: 0.3674  loss_ce_dn_1: 0.4884  loss_mask_dn_1: 0.06473  loss_dice_dn_1: 0.5432  loss_bbox_dn_1: 0.09918  loss_giou_dn_1: 0.3606  loss_ce_2: 1.143  loss_mask_2: 0.06819  loss_dice_2: 0.6011  loss_bbox_2: 0.06985  loss_giou_2: 0.3274  loss_ce_dn_2: 0.3814  loss_mask_dn_2: 0.06171  loss_dice_dn_2: 0.5014  loss_bbox_dn_2: 0.05878  loss_giou_dn_2: 0.2677  loss_ce_3: 1.029  loss_mask_3: 0.06944  loss_dice_3: 0.5585  loss_bbox_3: 0.06308  loss_giou_3: 0.3397  loss_ce_dn_3: 0.3433  loss_mask_dn_3: 0.06156  loss_dice_dn_3: 0.4686  loss_bbox_dn_3: 0.04854  loss_giou_dn_3: 0.2363  loss_ce_4: 1.024  loss_mask_4: 0.07539  loss_dice_4: 0.5535  loss_bbox_4: 0.07053  loss_giou_4: 0.3293  loss_ce_dn_4: 0.3293  loss_mask_dn_4: 0.06504  loss_dice_dn_4: 0.4503  loss_bbox_dn_4: 0.04199  loss_giou_dn_4: 0.2172  loss_ce_5: 1.006  loss_mask_5: 0.06761  loss_dice_5: 0.5434  loss_bbox_5: 0.05855  loss_giou_5: 0.2877  loss_ce_dn_5: 0.3304  loss_mask_dn_5: 0.06893  loss_dice_dn_5: 0.4825  loss_bbox_dn_5: 0.04164  loss_giou_dn_5: 0.2189  loss_ce_6: 0.9466  loss_mask_6: 0.06841  loss_dice_6: 0.5382  loss_bbox_6: 0.06469  loss_giou_6: 0.3056  loss_ce_dn_6: 0.33  loss_mask_dn_6: 0.0686  loss_dice_dn_6: 0.4436  loss_bbox_dn_6: 0.04052  loss_giou_dn_6: 0.2154  loss_ce_7: 0.9489  loss_mask_7: 0.06514  loss_dice_7: 0.5144  loss_bbox_7: 0.06492  loss_giou_7: 0.2832  loss_ce_dn_7: 0.3277  loss_mask_dn_7: 0.06949  loss_dice_dn_7: 0.4449  loss_bbox_dn_7: 0.04141  loss_giou_dn_7: 0.2173  loss_ce_8: 0.945  loss_mask_8: 0.05953  loss_dice_8: 0.4934  loss_bbox_8: 0.05535  loss_giou_8: 0.2815  loss_ce_dn_8: 0.3262  loss_mask_dn_8: 0.07006  loss_dice_dn_8: 0.4654  loss_bbox_dn_8: 0.03962  loss_giou_dn_8: 0.216  loss_ce_interm: 1.524  loss_mask_interm: 0.07387  loss_dice_interm: 0.571  loss_bbox_interm: 0.1121  loss_giou_interm: 0.4093    time: 0.7885  last_time: 0.7864  data_time: 0.0155  last_data_time: 0.0088   lr: 0.0001  max_mem: 20850M\n",
      "\u001b[32m[07/13 11:17:36 d2.utils.events]: \u001b[0m eta: 0:25:18  iter: 459  total_loss: 38.99  loss_ce: 0.8611  loss_mask: 0.05935  loss_dice: 0.4459  loss_bbox: 0.05099  loss_giou: 0.222  loss_ce_dn: 0.2594  loss_mask_dn: 0.04832  loss_dice_dn: 0.423  loss_bbox_dn: 0.03845  loss_giou_dn: 0.1791  loss_ce_0: 1.26  loss_mask_0: 0.06735  loss_dice_0: 0.4851  loss_bbox_0: 0.08327  loss_giou_0: 0.3765  loss_ce_dn_0: 3.512  loss_mask_dn_0: 0.5901  loss_dice_dn_0: 3.524  loss_bbox_dn_0: 0.3176  loss_giou_dn_0: 0.8496  loss_ce_1: 1.151  loss_mask_1: 0.06597  loss_dice_1: 0.515  loss_bbox_1: 0.07119  loss_giou_1: 0.2908  loss_ce_dn_1: 0.4327  loss_mask_dn_1: 0.04901  loss_dice_dn_1: 0.4913  loss_bbox_dn_1: 0.09284  loss_giou_dn_1: 0.3272  loss_ce_2: 1.015  loss_mask_2: 0.07037  loss_dice_2: 0.4851  loss_bbox_2: 0.0654  loss_giou_2: 0.2693  loss_ce_dn_2: 0.3228  loss_mask_dn_2: 0.04488  loss_dice_dn_2: 0.4455  loss_bbox_dn_2: 0.05521  loss_giou_dn_2: 0.2336  loss_ce_3: 0.9322  loss_mask_3: 0.07387  loss_dice_3: 0.4814  loss_bbox_3: 0.05934  loss_giou_3: 0.23  loss_ce_dn_3: 0.2739  loss_mask_dn_3: 0.04526  loss_dice_dn_3: 0.4276  loss_bbox_dn_3: 0.04674  loss_giou_dn_3: 0.2038  loss_ce_4: 0.8797  loss_mask_4: 0.06609  loss_dice_4: 0.5142  loss_bbox_4: 0.0653  loss_giou_4: 0.2546  loss_ce_dn_4: 0.2634  loss_mask_dn_4: 0.04681  loss_dice_dn_4: 0.4442  loss_bbox_dn_4: 0.0388  loss_giou_dn_4: 0.1862  loss_ce_5: 0.8756  loss_mask_5: 0.05705  loss_dice_5: 0.4627  loss_bbox_5: 0.05671  loss_giou_5: 0.2188  loss_ce_dn_5: 0.2556  loss_mask_dn_5: 0.04567  loss_dice_dn_5: 0.4085  loss_bbox_dn_5: 0.03745  loss_giou_dn_5: 0.1816  loss_ce_6: 0.8441  loss_mask_6: 0.06198  loss_dice_6: 0.4974  loss_bbox_6: 0.062  loss_giou_6: 0.2266  loss_ce_dn_6: 0.2607  loss_mask_dn_6: 0.047  loss_dice_dn_6: 0.4024  loss_bbox_dn_6: 0.03721  loss_giou_dn_6: 0.1797  loss_ce_7: 0.8269  loss_mask_7: 0.06007  loss_dice_7: 0.4507  loss_bbox_7: 0.06104  loss_giou_7: 0.2162  loss_ce_dn_7: 0.2584  loss_mask_dn_7: 0.04792  loss_dice_dn_7: 0.4122  loss_bbox_dn_7: 0.03725  loss_giou_dn_7: 0.1801  loss_ce_8: 0.8644  loss_mask_8: 0.05526  loss_dice_8: 0.4868  loss_bbox_8: 0.05389  loss_giou_8: 0.2383  loss_ce_dn_8: 0.2651  loss_mask_dn_8: 0.04766  loss_dice_dn_8: 0.4188  loss_bbox_dn_8: 0.03808  loss_giou_dn_8: 0.1782  loss_ce_interm: 1.26  loss_mask_interm: 0.06672  loss_dice_interm: 0.4701  loss_bbox_interm: 0.09804  loss_giou_interm: 0.3355    time: 0.7883  last_time: 0.7745  data_time: 0.0121  last_data_time: 0.0063   lr: 0.0001  max_mem: 20850M\n",
      "\u001b[32m[07/13 11:17:52 d2.utils.events]: \u001b[0m eta: 0:25:02  iter: 479  total_loss: 38.84  loss_ce: 0.8853  loss_mask: 0.04597  loss_dice: 0.4983  loss_bbox: 0.06288  loss_giou: 0.2621  loss_ce_dn: 0.2773  loss_mask_dn: 0.03594  loss_dice_dn: 0.4103  loss_bbox_dn: 0.03654  loss_giou_dn: 0.1946  loss_ce_0: 1.338  loss_mask_0: 0.03983  loss_dice_0: 0.4856  loss_bbox_0: 0.0729  loss_giou_0: 0.3659  loss_ce_dn_0: 3.29  loss_mask_dn_0: 0.4339  loss_dice_dn_0: 3.426  loss_bbox_dn_0: 0.2926  loss_giou_dn_0: 0.8537  loss_ce_1: 1.217  loss_mask_1: 0.04449  loss_dice_1: 0.4518  loss_bbox_1: 0.06223  loss_giou_1: 0.3071  loss_ce_dn_1: 0.4551  loss_mask_dn_1: 0.04048  loss_dice_dn_1: 0.481  loss_bbox_dn_1: 0.08369  loss_giou_dn_1: 0.3461  loss_ce_2: 1.018  loss_mask_2: 0.0464  loss_dice_2: 0.4869  loss_bbox_2: 0.06559  loss_giou_2: 0.2909  loss_ce_dn_2: 0.3426  loss_mask_dn_2: 0.03538  loss_dice_dn_2: 0.4313  loss_bbox_dn_2: 0.05215  loss_giou_dn_2: 0.248  loss_ce_3: 0.9631  loss_mask_3: 0.05144  loss_dice_3: 0.5198  loss_bbox_3: 0.06679  loss_giou_3: 0.2675  loss_ce_dn_3: 0.309  loss_mask_dn_3: 0.03413  loss_dice_dn_3: 0.4306  loss_bbox_dn_3: 0.04358  loss_giou_dn_3: 0.2167  loss_ce_4: 0.9653  loss_mask_4: 0.04588  loss_dice_4: 0.5007  loss_bbox_4: 0.06563  loss_giou_4: 0.2635  loss_ce_dn_4: 0.2797  loss_mask_dn_4: 0.03236  loss_dice_dn_4: 0.4108  loss_bbox_dn_4: 0.03914  loss_giou_dn_4: 0.2028  loss_ce_5: 0.9139  loss_mask_5: 0.04924  loss_dice_5: 0.5127  loss_bbox_5: 0.06422  loss_giou_5: 0.271  loss_ce_dn_5: 0.2703  loss_mask_dn_5: 0.03366  loss_dice_dn_5: 0.4122  loss_bbox_dn_5: 0.038  loss_giou_dn_5: 0.1971  loss_ce_6: 0.9032  loss_mask_6: 0.04851  loss_dice_6: 0.4631  loss_bbox_6: 0.06345  loss_giou_6: 0.2665  loss_ce_dn_6: 0.2716  loss_mask_dn_6: 0.03601  loss_dice_dn_6: 0.4168  loss_bbox_dn_6: 0.03665  loss_giou_dn_6: 0.1953  loss_ce_7: 0.9119  loss_mask_7: 0.04496  loss_dice_7: 0.4329  loss_bbox_7: 0.05848  loss_giou_7: 0.2571  loss_ce_dn_7: 0.2735  loss_mask_dn_7: 0.03589  loss_dice_dn_7: 0.4206  loss_bbox_dn_7: 0.03732  loss_giou_dn_7: 0.1951  loss_ce_8: 0.8948  loss_mask_8: 0.04677  loss_dice_8: 0.4394  loss_bbox_8: 0.06309  loss_giou_8: 0.2627  loss_ce_dn_8: 0.2776  loss_mask_dn_8: 0.03636  loss_dice_dn_8: 0.4222  loss_bbox_dn_8: 0.03599  loss_giou_dn_8: 0.1933  loss_ce_interm: 1.369  loss_mask_interm: 0.04067  loss_dice_interm: 0.4889  loss_bbox_interm: 0.09115  loss_giou_interm: 0.3748    time: 0.7883  last_time: 0.7766  data_time: 0.0128  last_data_time: 0.0078   lr: 0.0001  max_mem: 20850M\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[07/13 11:18:08 d2.utils.events]: \u001b[0m eta: 0:24:46  iter: 499  total_loss: 37.53  loss_ce: 0.9206  loss_mask: 0.04815  loss_dice: 0.4556  loss_bbox: 0.06666  loss_giou: 0.2554  loss_ce_dn: 0.2902  loss_mask_dn: 0.03436  loss_dice_dn: 0.4074  loss_bbox_dn: 0.03578  loss_giou_dn: 0.1757  loss_ce_0: 1.318  loss_mask_0: 0.05323  loss_dice_0: 0.5441  loss_bbox_0: 0.08112  loss_giou_0: 0.3897  loss_ce_dn_0: 3.383  loss_mask_dn_0: 0.4588  loss_dice_dn_0: 3.408  loss_bbox_dn_0: 0.3186  loss_giou_dn_0: 0.8562  loss_ce_1: 1.192  loss_mask_1: 0.05278  loss_dice_1: 0.5314  loss_bbox_1: 0.07257  loss_giou_1: 0.327  loss_ce_dn_1: 0.4459  loss_mask_dn_1: 0.03776  loss_dice_dn_1: 0.5093  loss_bbox_dn_1: 0.08937  loss_giou_dn_1: 0.3308  loss_ce_2: 1.058  loss_mask_2: 0.05586  loss_dice_2: 0.5006  loss_bbox_2: 0.06656  loss_giou_2: 0.2649  loss_ce_dn_2: 0.3503  loss_mask_dn_2: 0.03382  loss_dice_dn_2: 0.4339  loss_bbox_dn_2: 0.05614  loss_giou_dn_2: 0.2424  loss_ce_3: 1.038  loss_mask_3: 0.04757  loss_dice_3: 0.4497  loss_bbox_3: 0.07959  loss_giou_3: 0.2844  loss_ce_dn_3: 0.2986  loss_mask_dn_3: 0.03401  loss_dice_dn_3: 0.4335  loss_bbox_dn_3: 0.04606  loss_giou_dn_3: 0.2045  loss_ce_4: 0.9586  loss_mask_4: 0.046  loss_dice_4: 0.4713  loss_bbox_4: 0.07577  loss_giou_4: 0.2791  loss_ce_dn_4: 0.2828  loss_mask_dn_4: 0.03242  loss_dice_dn_4: 0.414  loss_bbox_dn_4: 0.03924  loss_giou_dn_4: 0.1884  loss_ce_5: 0.9563  loss_mask_5: 0.0507  loss_dice_5: 0.502  loss_bbox_5: 0.07368  loss_giou_5: 0.273  loss_ce_dn_5: 0.2618  loss_mask_dn_5: 0.03293  loss_dice_dn_5: 0.4032  loss_bbox_dn_5: 0.03789  loss_giou_dn_5: 0.1829  loss_ce_6: 0.9369  loss_mask_6: 0.05337  loss_dice_6: 0.4714  loss_bbox_6: 0.07153  loss_giou_6: 0.2718  loss_ce_dn_6: 0.2649  loss_mask_dn_6: 0.03366  loss_dice_dn_6: 0.4175  loss_bbox_dn_6: 0.03647  loss_giou_dn_6: 0.1773  loss_ce_7: 0.8996  loss_mask_7: 0.04478  loss_dice_7: 0.4259  loss_bbox_7: 0.06564  loss_giou_7: 0.2499  loss_ce_dn_7: 0.2791  loss_mask_dn_7: 0.03471  loss_dice_dn_7: 0.4067  loss_bbox_dn_7: 0.03588  loss_giou_dn_7: 0.1785  loss_ce_8: 0.8782  loss_mask_8: 0.04982  loss_dice_8: 0.4857  loss_bbox_8: 0.05968  loss_giou_8: 0.2533  loss_ce_dn_8: 0.2839  loss_mask_dn_8: 0.03335  loss_dice_dn_8: 0.4205  loss_bbox_dn_8: 0.03621  loss_giou_dn_8: 0.1749  loss_ce_interm: 1.305  loss_mask_interm: 0.05831  loss_dice_interm: 0.4924  loss_bbox_interm: 0.1166  loss_giou_interm: 0.4381    time: 0.7881  last_time: 0.7594  data_time: 0.0164  last_data_time: 0.0085   lr: 0.0001  max_mem: 20850M\n",
      "\u001b[32m[07/13 11:18:24 d2.utils.events]: \u001b[0m eta: 0:24:31  iter: 519  total_loss: 36.91  loss_ce: 0.8168  loss_mask: 0.09644  loss_dice: 0.3934  loss_bbox: 0.04115  loss_giou: 0.1875  loss_ce_dn: 0.2374  loss_mask_dn: 0.08005  loss_dice_dn: 0.3459  loss_bbox_dn: 0.03837  loss_giou_dn: 0.1648  loss_ce_0: 1.241  loss_mask_0: 0.09023  loss_dice_0: 0.4309  loss_bbox_0: 0.05605  loss_giou_0: 0.2844  loss_ce_dn_0: 3.158  loss_mask_dn_0: 0.7595  loss_dice_dn_0: 3.26  loss_bbox_dn_0: 0.3949  loss_giou_dn_0: 0.8489  loss_ce_1: 1.133  loss_mask_1: 0.1016  loss_dice_1: 0.4297  loss_bbox_1: 0.05229  loss_giou_1: 0.2218  loss_ce_dn_1: 0.3969  loss_mask_dn_1: 0.08977  loss_dice_dn_1: 0.3816  loss_bbox_dn_1: 0.1063  loss_giou_dn_1: 0.3069  loss_ce_2: 1.012  loss_mask_2: 0.09342  loss_dice_2: 0.4045  loss_bbox_2: 0.04372  loss_giou_2: 0.2221  loss_ce_dn_2: 0.3052  loss_mask_dn_2: 0.08399  loss_dice_dn_2: 0.3722  loss_bbox_dn_2: 0.06386  loss_giou_dn_2: 0.2175  loss_ce_3: 0.9359  loss_mask_3: 0.09995  loss_dice_3: 0.3881  loss_bbox_3: 0.04  loss_giou_3: 0.1943  loss_ce_dn_3: 0.2511  loss_mask_dn_3: 0.07894  loss_dice_dn_3: 0.3678  loss_bbox_dn_3: 0.05505  loss_giou_dn_3: 0.1851  loss_ce_4: 0.8735  loss_mask_4: 0.09892  loss_dice_4: 0.4111  loss_bbox_4: 0.03972  loss_giou_4: 0.1837  loss_ce_dn_4: 0.2355  loss_mask_dn_4: 0.08059  loss_dice_dn_4: 0.3525  loss_bbox_dn_4: 0.04794  loss_giou_dn_4: 0.1737  loss_ce_5: 0.8852  loss_mask_5: 0.09566  loss_dice_5: 0.3862  loss_bbox_5: 0.04715  loss_giou_5: 0.1984  loss_ce_dn_5: 0.2328  loss_mask_dn_5: 0.07857  loss_dice_dn_5: 0.3497  loss_bbox_dn_5: 0.04691  loss_giou_dn_5: 0.1657  loss_ce_6: 0.859  loss_mask_6: 0.09797  loss_dice_6: 0.4175  loss_bbox_6: 0.04264  loss_giou_6: 0.1917  loss_ce_dn_6: 0.2271  loss_mask_dn_6: 0.08089  loss_dice_dn_6: 0.356  loss_bbox_dn_6: 0.04333  loss_giou_dn_6: 0.1655  loss_ce_7: 0.8117  loss_mask_7: 0.09441  loss_dice_7: 0.4056  loss_bbox_7: 0.04177  loss_giou_7: 0.186  loss_ce_dn_7: 0.2324  loss_mask_dn_7: 0.07984  loss_dice_dn_7: 0.3503  loss_bbox_dn_7: 0.04053  loss_giou_dn_7: 0.1664  loss_ce_8: 0.8123  loss_mask_8: 0.0959  loss_dice_8: 0.3977  loss_bbox_8: 0.04321  loss_giou_8: 0.1844  loss_ce_dn_8: 0.2363  loss_mask_dn_8: 0.08034  loss_dice_dn_8: 0.3564  loss_bbox_dn_8: 0.03901  loss_giou_dn_8: 0.164  loss_ce_interm: 1.249  loss_mask_interm: 0.09104  loss_dice_interm: 0.4062  loss_bbox_interm: 0.1053  loss_giou_interm: 0.3425    time: 0.7883  last_time: 0.7966  data_time: 0.0163  last_data_time: 0.0098   lr: 0.0001  max_mem: 20850M\n",
      "\u001b[32m[07/13 11:18:40 d2.utils.events]: \u001b[0m eta: 0:24:15  iter: 539  total_loss: 43.69  loss_ce: 0.8766  loss_mask: 0.0756  loss_dice: 0.6291  loss_bbox: 0.07404  loss_giou: 0.3769  loss_ce_dn: 0.2579  loss_mask_dn: 0.05952  loss_dice_dn: 0.4119  loss_bbox_dn: 0.03461  loss_giou_dn: 0.1988  loss_ce_0: 1.206  loss_mask_0: 0.07903  loss_dice_0: 0.5158  loss_bbox_0: 0.1131  loss_giou_0: 0.4405  loss_ce_dn_0: 2.9  loss_mask_dn_0: 0.6753  loss_dice_dn_0: 3.39  loss_bbox_dn_0: 0.3558  loss_giou_dn_0: 0.854  loss_ce_1: 1.143  loss_mask_1: 0.08905  loss_dice_1: 0.6338  loss_bbox_1: 0.09326  loss_giou_1: 0.4048  loss_ce_dn_1: 0.3775  loss_mask_dn_1: 0.07242  loss_dice_dn_1: 0.4544  loss_bbox_dn_1: 0.0916  loss_giou_dn_1: 0.3288  loss_ce_2: 1.106  loss_mask_2: 0.09315  loss_dice_2: 0.584  loss_bbox_2: 0.1018  loss_giou_2: 0.4104  loss_ce_dn_2: 0.2782  loss_mask_dn_2: 0.06206  loss_dice_dn_2: 0.4261  loss_bbox_dn_2: 0.05873  loss_giou_dn_2: 0.2572  loss_ce_3: 0.9721  loss_mask_3: 0.08926  loss_dice_3: 0.6475  loss_bbox_3: 0.07636  loss_giou_3: 0.3955  loss_ce_dn_3: 0.236  loss_mask_dn_3: 0.05827  loss_dice_dn_3: 0.4223  loss_bbox_dn_3: 0.04571  loss_giou_dn_3: 0.2245  loss_ce_4: 0.9496  loss_mask_4: 0.07758  loss_dice_4: 0.5579  loss_bbox_4: 0.09605  loss_giou_4: 0.3886  loss_ce_dn_4: 0.2317  loss_mask_dn_4: 0.05725  loss_dice_dn_4: 0.4244  loss_bbox_dn_4: 0.03856  loss_giou_dn_4: 0.2127  loss_ce_5: 0.9127  loss_mask_5: 0.07365  loss_dice_5: 0.6451  loss_bbox_5: 0.09256  loss_giou_5: 0.3898  loss_ce_dn_5: 0.2202  loss_mask_dn_5: 0.05907  loss_dice_dn_5: 0.409  loss_bbox_dn_5: 0.0391  loss_giou_dn_5: 0.207  loss_ce_6: 0.9257  loss_mask_6: 0.07413  loss_dice_6: 0.6173  loss_bbox_6: 0.08144  loss_giou_6: 0.4071  loss_ce_dn_6: 0.243  loss_mask_dn_6: 0.05784  loss_dice_dn_6: 0.4147  loss_bbox_dn_6: 0.03647  loss_giou_dn_6: 0.2036  loss_ce_7: 0.8775  loss_mask_7: 0.07748  loss_dice_7: 0.5717  loss_bbox_7: 0.08115  loss_giou_7: 0.3833  loss_ce_dn_7: 0.2449  loss_mask_dn_7: 0.05765  loss_dice_dn_7: 0.3982  loss_bbox_dn_7: 0.03657  loss_giou_dn_7: 0.2032  loss_ce_8: 0.8758  loss_mask_8: 0.0768  loss_dice_8: 0.5941  loss_bbox_8: 0.08188  loss_giou_8: 0.3895  loss_ce_dn_8: 0.2432  loss_mask_dn_8: 0.05946  loss_dice_dn_8: 0.4  loss_bbox_dn_8: 0.03523  loss_giou_dn_8: 0.1988  loss_ce_interm: 1.257  loss_mask_interm: 0.07767  loss_dice_interm: 0.4529  loss_bbox_interm: 0.1021  loss_giou_interm: 0.42    time: 0.7884  last_time: 0.7870  data_time: 0.0154  last_data_time: 0.0082   lr: 0.0001  max_mem: 20850M\n",
      "\u001b[32m[07/13 11:18:55 d2.utils.events]: \u001b[0m eta: 0:24:00  iter: 559  total_loss: 35.25  loss_ce: 0.852  loss_mask: 0.04033  loss_dice: 0.389  loss_bbox: 0.0435  loss_giou: 0.2347  loss_ce_dn: 0.2227  loss_mask_dn: 0.03278  loss_dice_dn: 0.3814  loss_bbox_dn: 0.02889  loss_giou_dn: 0.1755  loss_ce_0: 1.294  loss_mask_0: 0.04759  loss_dice_0: 0.4782  loss_bbox_0: 0.06613  loss_giou_0: 0.386  loss_ce_dn_0: 3.023  loss_mask_dn_0: 0.5285  loss_dice_dn_0: 3.284  loss_bbox_dn_0: 0.3227  loss_giou_dn_0: 0.8455  loss_ce_1: 1.163  loss_mask_1: 0.04474  loss_dice_1: 0.4355  loss_bbox_1: 0.05817  loss_giou_1: 0.2999  loss_ce_dn_1: 0.3924  loss_mask_dn_1: 0.03128  loss_dice_dn_1: 0.4474  loss_bbox_dn_1: 0.07794  loss_giou_dn_1: 0.3223  loss_ce_2: 1.024  loss_mask_2: 0.0488  loss_dice_2: 0.4917  loss_bbox_2: 0.05424  loss_giou_2: 0.2787  loss_ce_dn_2: 0.2845  loss_mask_dn_2: 0.03  loss_dice_dn_2: 0.4309  loss_bbox_dn_2: 0.04389  loss_giou_dn_2: 0.2304  loss_ce_3: 0.9429  loss_mask_3: 0.03905  loss_dice_3: 0.4392  loss_bbox_3: 0.05121  loss_giou_3: 0.2615  loss_ce_dn_3: 0.2448  loss_mask_dn_3: 0.03062  loss_dice_dn_3: 0.4093  loss_bbox_dn_3: 0.03922  loss_giou_dn_3: 0.1968  loss_ce_4: 0.9029  loss_mask_4: 0.03867  loss_dice_4: 0.4507  loss_bbox_4: 0.04442  loss_giou_4: 0.2421  loss_ce_dn_4: 0.2372  loss_mask_dn_4: 0.02957  loss_dice_dn_4: 0.3871  loss_bbox_dn_4: 0.03617  loss_giou_dn_4: 0.1799  loss_ce_5: 0.8753  loss_mask_5: 0.04  loss_dice_5: 0.3961  loss_bbox_5: 0.04278  loss_giou_5: 0.2512  loss_ce_dn_5: 0.2255  loss_mask_dn_5: 0.03012  loss_dice_dn_5: 0.3993  loss_bbox_dn_5: 0.03205  loss_giou_dn_5: 0.1777  loss_ce_6: 0.8763  loss_mask_6: 0.03696  loss_dice_6: 0.3606  loss_bbox_6: 0.04456  loss_giou_6: 0.2399  loss_ce_dn_6: 0.2237  loss_mask_dn_6: 0.03115  loss_dice_dn_6: 0.3986  loss_bbox_dn_6: 0.0297  loss_giou_dn_6: 0.177  loss_ce_7: 0.869  loss_mask_7: 0.03248  loss_dice_7: 0.4309  loss_bbox_7: 0.04523  loss_giou_7: 0.2215  loss_ce_dn_7: 0.2233  loss_mask_dn_7: 0.03186  loss_dice_dn_7: 0.4014  loss_bbox_dn_7: 0.02967  loss_giou_dn_7: 0.1777  loss_ce_8: 0.8567  loss_mask_8: 0.03825  loss_dice_8: 0.3912  loss_bbox_8: 0.04359  loss_giou_8: 0.2246  loss_ce_dn_8: 0.2233  loss_mask_dn_8: 0.03206  loss_dice_dn_8: 0.3917  loss_bbox_dn_8: 0.0287  loss_giou_dn_8: 0.1754  loss_ce_interm: 1.245  loss_mask_interm: 0.04024  loss_dice_interm: 0.4585  loss_bbox_interm: 0.1005  loss_giou_interm: 0.3734    time: 0.7883  last_time: 0.7958  data_time: 0.0150  last_data_time: 0.0086   lr: 0.0001  max_mem: 20850M\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[07/13 11:19:11 d2.utils.events]: \u001b[0m eta: 0:23:44  iter: 579  total_loss: 35.33  loss_ce: 0.7799  loss_mask: 0.06  loss_dice: 0.42  loss_bbox: 0.05682  loss_giou: 0.1962  loss_ce_dn: 0.2085  loss_mask_dn: 0.05664  loss_dice_dn: 0.3675  loss_bbox_dn: 0.03924  loss_giou_dn: 0.1574  loss_ce_0: 1.219  loss_mask_0: 0.05713  loss_dice_0: 0.4559  loss_bbox_0: 0.07249  loss_giou_0: 0.2573  loss_ce_dn_0: 2.925  loss_mask_dn_0: 0.6121  loss_dice_dn_0: 3.197  loss_bbox_dn_0: 0.4056  loss_giou_dn_0: 0.8458  loss_ce_1: 1.048  loss_mask_1: 0.05988  loss_dice_1: 0.4676  loss_bbox_1: 0.06547  loss_giou_1: 0.224  loss_ce_dn_1: 0.4038  loss_mask_dn_1: 0.05947  loss_dice_dn_1: 0.5157  loss_bbox_dn_1: 0.1031  loss_giou_dn_1: 0.2985  loss_ce_2: 0.9823  loss_mask_2: 0.06132  loss_dice_2: 0.4663  loss_bbox_2: 0.06713  loss_giou_2: 0.2208  loss_ce_dn_2: 0.3115  loss_mask_dn_2: 0.06006  loss_dice_dn_2: 0.4395  loss_bbox_dn_2: 0.06329  loss_giou_dn_2: 0.2169  loss_ce_3: 0.9013  loss_mask_3: 0.05971  loss_dice_3: 0.4118  loss_bbox_3: 0.06614  loss_giou_3: 0.2088  loss_ce_dn_3: 0.2591  loss_mask_dn_3: 0.05878  loss_dice_dn_3: 0.3999  loss_bbox_dn_3: 0.0519  loss_giou_dn_3: 0.1796  loss_ce_4: 0.8358  loss_mask_4: 0.05838  loss_dice_4: 0.406  loss_bbox_4: 0.05806  loss_giou_4: 0.207  loss_ce_dn_4: 0.235  loss_mask_dn_4: 0.05726  loss_dice_dn_4: 0.3998  loss_bbox_dn_4: 0.04681  loss_giou_dn_4: 0.1708  loss_ce_5: 0.8187  loss_mask_5: 0.05842  loss_dice_5: 0.3812  loss_bbox_5: 0.06866  loss_giou_5: 0.1992  loss_ce_dn_5: 0.2233  loss_mask_dn_5: 0.05598  loss_dice_dn_5: 0.3815  loss_bbox_dn_5: 0.0443  loss_giou_dn_5: 0.166  loss_ce_6: 0.767  loss_mask_6: 0.06141  loss_dice_6: 0.3698  loss_bbox_6: 0.05903  loss_giou_6: 0.1927  loss_ce_dn_6: 0.2178  loss_mask_dn_6: 0.05641  loss_dice_dn_6: 0.3645  loss_bbox_dn_6: 0.04116  loss_giou_dn_6: 0.1595  loss_ce_7: 0.7795  loss_mask_7: 0.05967  loss_dice_7: 0.4722  loss_bbox_7: 0.06418  loss_giou_7: 0.1892  loss_ce_dn_7: 0.2075  loss_mask_dn_7: 0.0561  loss_dice_dn_7: 0.3618  loss_bbox_dn_7: 0.04094  loss_giou_dn_7: 0.1592  loss_ce_8: 0.778  loss_mask_8: 0.05919  loss_dice_8: 0.4142  loss_bbox_8: 0.064  loss_giou_8: 0.1895  loss_ce_dn_8: 0.2059  loss_mask_dn_8: 0.05771  loss_dice_dn_8: 0.3494  loss_bbox_dn_8: 0.03914  loss_giou_dn_8: 0.157  loss_ce_interm: 1.219  loss_mask_interm: 0.05197  loss_dice_interm: 0.4298  loss_bbox_interm: 0.1208  loss_giou_interm: 0.3323    time: 0.7885  last_time: 0.7714  data_time: 0.0118  last_data_time: 0.0088   lr: 0.0001  max_mem: 20850M\n",
      "\u001b[32m[07/13 11:19:27 d2.utils.events]: \u001b[0m eta: 0:23:29  iter: 599  total_loss: 37.24  loss_ce: 0.8805  loss_mask: 0.04713  loss_dice: 0.4222  loss_bbox: 0.05201  loss_giou: 0.1969  loss_ce_dn: 0.2353  loss_mask_dn: 0.04805  loss_dice_dn: 0.3969  loss_bbox_dn: 0.04289  loss_giou_dn: 0.1745  loss_ce_0: 1.257  loss_mask_0: 0.04641  loss_dice_0: 0.4148  loss_bbox_0: 0.06833  loss_giou_0: 0.3156  loss_ce_dn_0: 2.696  loss_mask_dn_0: 0.5067  loss_dice_dn_0: 3.4  loss_bbox_dn_0: 0.3182  loss_giou_dn_0: 0.8524  loss_ce_1: 1.169  loss_mask_1: 0.05845  loss_dice_1: 0.4635  loss_bbox_1: 0.05724  loss_giou_1: 0.2441  loss_ce_dn_1: 0.3749  loss_mask_dn_1: 0.05426  loss_dice_dn_1: 0.4476  loss_bbox_dn_1: 0.08715  loss_giou_dn_1: 0.328  loss_ce_2: 1.059  loss_mask_2: 0.05007  loss_dice_2: 0.4298  loss_bbox_2: 0.05441  loss_giou_2: 0.2284  loss_ce_dn_2: 0.296  loss_mask_dn_2: 0.04931  loss_dice_dn_2: 0.4502  loss_bbox_dn_2: 0.06371  loss_giou_dn_2: 0.2349  loss_ce_3: 0.9713  loss_mask_3: 0.0522  loss_dice_3: 0.4087  loss_bbox_3: 0.04937  loss_giou_3: 0.2098  loss_ce_dn_3: 0.2572  loss_mask_dn_3: 0.04738  loss_dice_dn_3: 0.4124  loss_bbox_dn_3: 0.05631  loss_giou_dn_3: 0.2097  loss_ce_4: 0.9347  loss_mask_4: 0.04951  loss_dice_4: 0.4219  loss_bbox_4: 0.04996  loss_giou_4: 0.2023  loss_ce_dn_4: 0.2423  loss_mask_dn_4: 0.04595  loss_dice_dn_4: 0.406  loss_bbox_dn_4: 0.05039  loss_giou_dn_4: 0.1931  loss_ce_5: 0.9072  loss_mask_5: 0.0474  loss_dice_5: 0.4037  loss_bbox_5: 0.04988  loss_giou_5: 0.1918  loss_ce_dn_5: 0.233  loss_mask_dn_5: 0.04645  loss_dice_dn_5: 0.397  loss_bbox_dn_5: 0.04941  loss_giou_dn_5: 0.1859  loss_ce_6: 0.8964  loss_mask_6: 0.05081  loss_dice_6: 0.429  loss_bbox_6: 0.05437  loss_giou_6: 0.1912  loss_ce_dn_6: 0.2303  loss_mask_dn_6: 0.04903  loss_dice_dn_6: 0.3957  loss_bbox_dn_6: 0.0483  loss_giou_dn_6: 0.1798  loss_ce_7: 0.8854  loss_mask_7: 0.04726  loss_dice_7: 0.4502  loss_bbox_7: 0.05257  loss_giou_7: 0.197  loss_ce_dn_7: 0.2288  loss_mask_dn_7: 0.04808  loss_dice_dn_7: 0.3928  loss_bbox_dn_7: 0.04803  loss_giou_dn_7: 0.1792  loss_ce_8: 0.8901  loss_mask_8: 0.04375  loss_dice_8: 0.4078  loss_bbox_8: 0.05329  loss_giou_8: 0.2159  loss_ce_dn_8: 0.2301  loss_mask_dn_8: 0.04633  loss_dice_dn_8: 0.3839  loss_bbox_dn_8: 0.04397  loss_giou_dn_8: 0.1739  loss_ce_interm: 1.284  loss_mask_interm: 0.04518  loss_dice_interm: 0.4528  loss_bbox_interm: 0.09362  loss_giou_interm: 0.3557    time: 0.7887  last_time: 0.7724  data_time: 0.0131  last_data_time: 0.0075   lr: 0.0001  max_mem: 20850M\n",
      "\u001b[32m[07/13 11:19:43 d2.utils.events]: \u001b[0m eta: 0:23:13  iter: 619  total_loss: 39.56  loss_ce: 0.7477  loss_mask: 0.04732  loss_dice: 0.4763  loss_bbox: 0.05551  loss_giou: 0.283  loss_ce_dn: 0.1981  loss_mask_dn: 0.04227  loss_dice_dn: 0.4516  loss_bbox_dn: 0.03892  loss_giou_dn: 0.1897  loss_ce_0: 1.185  loss_mask_0: 0.051  loss_dice_0: 0.5555  loss_bbox_0: 0.09356  loss_giou_0: 0.4205  loss_ce_dn_0: 2.635  loss_mask_dn_0: 0.646  loss_dice_dn_0: 3.326  loss_bbox_dn_0: 0.3532  loss_giou_dn_0: 0.8609  loss_ce_1: 1.033  loss_mask_1: 0.05193  loss_dice_1: 0.5751  loss_bbox_1: 0.08193  loss_giou_1: 0.3394  loss_ce_dn_1: 0.3561  loss_mask_dn_1: 0.05213  loss_dice_dn_1: 0.5428  loss_bbox_dn_1: 0.1052  loss_giou_dn_1: 0.3305  loss_ce_2: 0.9384  loss_mask_2: 0.04694  loss_dice_2: 0.5726  loss_bbox_2: 0.07305  loss_giou_2: 0.3156  loss_ce_dn_2: 0.2636  loss_mask_dn_2: 0.04506  loss_dice_dn_2: 0.4631  loss_bbox_dn_2: 0.07123  loss_giou_dn_2: 0.2505  loss_ce_3: 0.8942  loss_mask_3: 0.04904  loss_dice_3: 0.5441  loss_bbox_3: 0.06496  loss_giou_3: 0.2994  loss_ce_dn_3: 0.2484  loss_mask_dn_3: 0.04582  loss_dice_dn_3: 0.4588  loss_bbox_dn_3: 0.05092  loss_giou_dn_3: 0.2151  loss_ce_4: 0.8115  loss_mask_4: 0.0478  loss_dice_4: 0.5725  loss_bbox_4: 0.07199  loss_giou_4: 0.2935  loss_ce_dn_4: 0.2323  loss_mask_dn_4: 0.04161  loss_dice_dn_4: 0.4455  loss_bbox_dn_4: 0.04675  loss_giou_dn_4: 0.202  loss_ce_5: 0.7898  loss_mask_5: 0.04417  loss_dice_5: 0.5465  loss_bbox_5: 0.05501  loss_giou_5: 0.2897  loss_ce_dn_5: 0.2161  loss_mask_dn_5: 0.04382  loss_dice_dn_5: 0.4455  loss_bbox_dn_5: 0.04144  loss_giou_dn_5: 0.1961  loss_ce_6: 0.7754  loss_mask_6: 0.04676  loss_dice_6: 0.5281  loss_bbox_6: 0.06606  loss_giou_6: 0.295  loss_ce_dn_6: 0.2049  loss_mask_dn_6: 0.0434  loss_dice_dn_6: 0.4444  loss_bbox_dn_6: 0.03964  loss_giou_dn_6: 0.1908  loss_ce_7: 0.7715  loss_mask_7: 0.04606  loss_dice_7: 0.5048  loss_bbox_7: 0.05687  loss_giou_7: 0.2818  loss_ce_dn_7: 0.2011  loss_mask_dn_7: 0.04279  loss_dice_dn_7: 0.4542  loss_bbox_dn_7: 0.03928  loss_giou_dn_7: 0.1894  loss_ce_8: 0.7639  loss_mask_8: 0.04792  loss_dice_8: 0.4928  loss_bbox_8: 0.05645  loss_giou_8: 0.278  loss_ce_dn_8: 0.1965  loss_mask_dn_8: 0.04318  loss_dice_dn_8: 0.4407  loss_bbox_dn_8: 0.03899  loss_giou_dn_8: 0.1895  loss_ce_interm: 1.203  loss_mask_interm: 0.05407  loss_dice_interm: 0.5376  loss_bbox_interm: 0.1244  loss_giou_interm: 0.4077    time: 0.7888  last_time: 0.8557  data_time: 0.0202  last_data_time: 0.0746   lr: 0.0001  max_mem: 20850M\n",
      "\u001b[32m[07/13 11:19:59 d2.utils.events]: \u001b[0m eta: 0:22:57  iter: 639  total_loss: 35.85  loss_ce: 0.7203  loss_mask: 0.05989  loss_dice: 0.4613  loss_bbox: 0.05459  loss_giou: 0.2258  loss_ce_dn: 0.1975  loss_mask_dn: 0.04502  loss_dice_dn: 0.3979  loss_bbox_dn: 0.03863  loss_giou_dn: 0.1728  loss_ce_0: 1.192  loss_mask_0: 0.06993  loss_dice_0: 0.459  loss_bbox_0: 0.06851  loss_giou_0: 0.3639  loss_ce_dn_0: 2.593  loss_mask_dn_0: 0.4745  loss_dice_dn_0: 3.145  loss_bbox_dn_0: 0.3438  loss_giou_dn_0: 0.8557  loss_ce_1: 1.029  loss_mask_1: 0.0713  loss_dice_1: 0.5065  loss_bbox_1: 0.05185  loss_giou_1: 0.3006  loss_ce_dn_1: 0.3301  loss_mask_dn_1: 0.05235  loss_dice_dn_1: 0.4339  loss_bbox_dn_1: 0.09124  loss_giou_dn_1: 0.3105  loss_ce_2: 0.8808  loss_mask_2: 0.07079  loss_dice_2: 0.4638  loss_bbox_2: 0.04949  loss_giou_2: 0.2802  loss_ce_dn_2: 0.2457  loss_mask_dn_2: 0.05301  loss_dice_dn_2: 0.4112  loss_bbox_dn_2: 0.06048  loss_giou_dn_2: 0.2315  loss_ce_3: 0.7961  loss_mask_3: 0.06363  loss_dice_3: 0.4559  loss_bbox_3: 0.04756  loss_giou_3: 0.2562  loss_ce_dn_3: 0.2145  loss_mask_dn_3: 0.05295  loss_dice_dn_3: 0.4128  loss_bbox_dn_3: 0.04885  loss_giou_dn_3: 0.2005  loss_ce_4: 0.8138  loss_mask_4: 0.06706  loss_dice_4: 0.4437  loss_bbox_4: 0.04635  loss_giou_4: 0.2197  loss_ce_dn_4: 0.2064  loss_mask_dn_4: 0.04741  loss_dice_dn_4: 0.4014  loss_bbox_dn_4: 0.04142  loss_giou_dn_4: 0.1848  loss_ce_5: 0.7337  loss_mask_5: 0.07055  loss_dice_5: 0.4608  loss_bbox_5: 0.05595  loss_giou_5: 0.2255  loss_ce_dn_5: 0.1958  loss_mask_dn_5: 0.04502  loss_dice_dn_5: 0.3858  loss_bbox_dn_5: 0.03934  loss_giou_dn_5: 0.1781  loss_ce_6: 0.7089  loss_mask_6: 0.06849  loss_dice_6: 0.487  loss_bbox_6: 0.05529  loss_giou_6: 0.2197  loss_ce_dn_6: 0.19  loss_mask_dn_6: 0.04427  loss_dice_dn_6: 0.41  loss_bbox_dn_6: 0.03903  loss_giou_dn_6: 0.1733  loss_ce_7: 0.7191  loss_mask_7: 0.0661  loss_dice_7: 0.4676  loss_bbox_7: 0.05611  loss_giou_7: 0.2252  loss_ce_dn_7: 0.1933  loss_mask_dn_7: 0.04398  loss_dice_dn_7: 0.3917  loss_bbox_dn_7: 0.03919  loss_giou_dn_7: 0.1742  loss_ce_8: 0.7389  loss_mask_8: 0.05977  loss_dice_8: 0.4578  loss_bbox_8: 0.05714  loss_giou_8: 0.2268  loss_ce_dn_8: 0.195  loss_mask_dn_8: 0.04532  loss_dice_dn_8: 0.4073  loss_bbox_dn_8: 0.0383  loss_giou_dn_8: 0.1707  loss_ce_interm: 1.148  loss_mask_interm: 0.06871  loss_dice_interm: 0.5135  loss_bbox_interm: 0.08493  loss_giou_interm: 0.3801    time: 0.7886  last_time: 0.7739  data_time: 0.0137  last_data_time: 0.0076   lr: 0.0001  max_mem: 20850M\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[07/13 11:20:15 d2.utils.events]: \u001b[0m eta: 0:22:42  iter: 659  total_loss: 37.95  loss_ce: 0.819  loss_mask: 0.03757  loss_dice: 0.5824  loss_bbox: 0.04849  loss_giou: 0.3094  loss_ce_dn: 0.2299  loss_mask_dn: 0.03005  loss_dice_dn: 0.4728  loss_bbox_dn: 0.02664  loss_giou_dn: 0.2188  loss_ce_0: 1.294  loss_mask_0: 0.0317  loss_dice_0: 0.6211  loss_bbox_0: 0.06046  loss_giou_0: 0.4569  loss_ce_dn_0: 2.385  loss_mask_dn_0: 0.2801  loss_dice_dn_0: 3.093  loss_bbox_dn_0: 0.2417  loss_giou_dn_0: 0.8551  loss_ce_1: 1.179  loss_mask_1: 0.03465  loss_dice_1: 0.5623  loss_bbox_1: 0.04693  loss_giou_1: 0.3687  loss_ce_dn_1: 0.3636  loss_mask_dn_1: 0.03501  loss_dice_dn_1: 0.5277  loss_bbox_dn_1: 0.06488  loss_giou_dn_1: 0.3565  loss_ce_2: 0.9951  loss_mask_2: 0.03401  loss_dice_2: 0.5986  loss_bbox_2: 0.04428  loss_giou_2: 0.339  loss_ce_dn_2: 0.3005  loss_mask_dn_2: 0.0334  loss_dice_dn_2: 0.4793  loss_bbox_dn_2: 0.04421  loss_giou_dn_2: 0.263  loss_ce_3: 0.954  loss_mask_3: 0.03124  loss_dice_3: 0.532  loss_bbox_3: 0.04329  loss_giou_3: 0.3267  loss_ce_dn_3: 0.265  loss_mask_dn_3: 0.03183  loss_dice_dn_3: 0.4602  loss_bbox_dn_3: 0.03682  loss_giou_dn_3: 0.2379  loss_ce_4: 0.9102  loss_mask_4: 0.03417  loss_dice_4: 0.579  loss_bbox_4: 0.04891  loss_giou_4: 0.3147  loss_ce_dn_4: 0.2417  loss_mask_dn_4: 0.02979  loss_dice_dn_4: 0.4727  loss_bbox_dn_4: 0.03035  loss_giou_dn_4: 0.2282  loss_ce_5: 0.8512  loss_mask_5: 0.03472  loss_dice_5: 0.525  loss_bbox_5: 0.0475  loss_giou_5: 0.3309  loss_ce_dn_5: 0.2302  loss_mask_dn_5: 0.0301  loss_dice_dn_5: 0.4796  loss_bbox_dn_5: 0.02853  loss_giou_dn_5: 0.2231  loss_ce_6: 0.8453  loss_mask_6: 0.03525  loss_dice_6: 0.5242  loss_bbox_6: 0.04628  loss_giou_6: 0.2978  loss_ce_dn_6: 0.2239  loss_mask_dn_6: 0.03037  loss_dice_dn_6: 0.4825  loss_bbox_dn_6: 0.02743  loss_giou_dn_6: 0.2193  loss_ce_7: 0.8377  loss_mask_7: 0.0348  loss_dice_7: 0.5143  loss_bbox_7: 0.04772  loss_giou_7: 0.2949  loss_ce_dn_7: 0.2266  loss_mask_dn_7: 0.02932  loss_dice_dn_7: 0.4695  loss_bbox_dn_7: 0.02741  loss_giou_dn_7: 0.2204  loss_ce_8: 0.8107  loss_mask_8: 0.03521  loss_dice_8: 0.5114  loss_bbox_8: 0.04794  loss_giou_8: 0.2977  loss_ce_dn_8: 0.2253  loss_mask_dn_8: 0.02986  loss_dice_dn_8: 0.4614  loss_bbox_dn_8: 0.02653  loss_giou_dn_8: 0.2182  loss_ce_interm: 1.24  loss_mask_interm: 0.03201  loss_dice_interm: 0.6181  loss_bbox_interm: 0.09462  loss_giou_interm: 0.443    time: 0.7886  last_time: 0.7889  data_time: 0.0157  last_data_time: 0.0086   lr: 0.0001  max_mem: 20850M\n",
      "\u001b[32m[07/13 11:20:30 d2.utils.events]: \u001b[0m eta: 0:22:26  iter: 679  total_loss: 39.27  loss_ce: 0.8428  loss_mask: 0.0568  loss_dice: 0.4596  loss_bbox: 0.06506  loss_giou: 0.2788  loss_ce_dn: 0.22  loss_mask_dn: 0.05596  loss_dice_dn: 0.4244  loss_bbox_dn: 0.03584  loss_giou_dn: 0.2031  loss_ce_0: 1.262  loss_mask_0: 0.05609  loss_dice_0: 0.5222  loss_bbox_0: 0.07012  loss_giou_0: 0.369  loss_ce_dn_0: 2.277  loss_mask_dn_0: 0.4569  loss_dice_dn_0: 3.176  loss_bbox_dn_0: 0.3854  loss_giou_dn_0: 0.8612  loss_ce_1: 1.2  loss_mask_1: 0.05717  loss_dice_1: 0.5052  loss_bbox_1: 0.06735  loss_giou_1: 0.3015  loss_ce_dn_1: 0.3666  loss_mask_dn_1: 0.06402  loss_dice_dn_1: 0.4845  loss_bbox_dn_1: 0.09815  loss_giou_dn_1: 0.3322  loss_ce_2: 1.027  loss_mask_2: 0.06122  loss_dice_2: 0.4647  loss_bbox_2: 0.06654  loss_giou_2: 0.2645  loss_ce_dn_2: 0.283  loss_mask_dn_2: 0.05814  loss_dice_dn_2: 0.4487  loss_bbox_dn_2: 0.05942  loss_giou_dn_2: 0.2453  loss_ce_3: 0.9372  loss_mask_3: 0.05265  loss_dice_3: 0.4177  loss_bbox_3: 0.06399  loss_giou_3: 0.2876  loss_ce_dn_3: 0.2352  loss_mask_dn_3: 0.05411  loss_dice_dn_3: 0.4332  loss_bbox_dn_3: 0.04567  loss_giou_dn_3: 0.2199  loss_ce_4: 0.8859  loss_mask_4: 0.05641  loss_dice_4: 0.4955  loss_bbox_4: 0.06152  loss_giou_4: 0.2793  loss_ce_dn_4: 0.2285  loss_mask_dn_4: 0.05426  loss_dice_dn_4: 0.4033  loss_bbox_dn_4: 0.04274  loss_giou_dn_4: 0.2082  loss_ce_5: 0.8868  loss_mask_5: 0.05804  loss_dice_5: 0.4698  loss_bbox_5: 0.06264  loss_giou_5: 0.2625  loss_ce_dn_5: 0.214  loss_mask_dn_5: 0.05084  loss_dice_dn_5: 0.4199  loss_bbox_dn_5: 0.03911  loss_giou_dn_5: 0.2067  loss_ce_6: 0.8634  loss_mask_6: 0.05787  loss_dice_6: 0.4777  loss_bbox_6: 0.06375  loss_giou_6: 0.2767  loss_ce_dn_6: 0.2152  loss_mask_dn_6: 0.05455  loss_dice_dn_6: 0.4251  loss_bbox_dn_6: 0.03661  loss_giou_dn_6: 0.2031  loss_ce_7: 0.8496  loss_mask_7: 0.05828  loss_dice_7: 0.4754  loss_bbox_7: 0.06352  loss_giou_7: 0.2733  loss_ce_dn_7: 0.2141  loss_mask_dn_7: 0.05559  loss_dice_dn_7: 0.4287  loss_bbox_dn_7: 0.03658  loss_giou_dn_7: 0.2041  loss_ce_8: 0.8487  loss_mask_8: 0.05677  loss_dice_8: 0.479  loss_bbox_8: 0.06464  loss_giou_8: 0.2772  loss_ce_dn_8: 0.2159  loss_mask_dn_8: 0.05728  loss_dice_dn_8: 0.4366  loss_bbox_dn_8: 0.03581  loss_giou_dn_8: 0.2027  loss_ce_interm: 1.219  loss_mask_interm: 0.05036  loss_dice_interm: 0.4717  loss_bbox_interm: 0.1336  loss_giou_interm: 0.4014    time: 0.7887  last_time: 0.7834  data_time: 0.0187  last_data_time: 0.0090   lr: 0.0001  max_mem: 20850M\n",
      "\u001b[32m[07/13 11:20:46 d2.utils.events]: \u001b[0m eta: 0:22:11  iter: 699  total_loss: 33.16  loss_ce: 0.749  loss_mask: 0.04706  loss_dice: 0.494  loss_bbox: 0.05495  loss_giou: 0.244  loss_ce_dn: 0.1742  loss_mask_dn: 0.03624  loss_dice_dn: 0.4209  loss_bbox_dn: 0.03488  loss_giou_dn: 0.1742  loss_ce_0: 1.082  loss_mask_0: 0.0437  loss_dice_0: 0.4801  loss_bbox_0: 0.06739  loss_giou_0: 0.3619  loss_ce_dn_0: 2.215  loss_mask_dn_0: 0.389  loss_dice_dn_0: 3.074  loss_bbox_dn_0: 0.2809  loss_giou_dn_0: 0.8519  loss_ce_1: 0.9674  loss_mask_1: 0.04332  loss_dice_1: 0.4684  loss_bbox_1: 0.06613  loss_giou_1: 0.2845  loss_ce_dn_1: 0.3131  loss_mask_dn_1: 0.04497  loss_dice_dn_1: 0.4918  loss_bbox_dn_1: 0.0725  loss_giou_dn_1: 0.3103  loss_ce_2: 0.8795  loss_mask_2: 0.04211  loss_dice_2: 0.4312  loss_bbox_2: 0.05569  loss_giou_2: 0.2591  loss_ce_dn_2: 0.2359  loss_mask_dn_2: 0.04224  loss_dice_dn_2: 0.4371  loss_bbox_dn_2: 0.05071  loss_giou_dn_2: 0.2279  loss_ce_3: 0.8131  loss_mask_3: 0.04376  loss_dice_3: 0.4604  loss_bbox_3: 0.05651  loss_giou_3: 0.2546  loss_ce_dn_3: 0.2014  loss_mask_dn_3: 0.03747  loss_dice_dn_3: 0.4221  loss_bbox_dn_3: 0.04296  loss_giou_dn_3: 0.1954  loss_ce_4: 0.7894  loss_mask_4: 0.04399  loss_dice_4: 0.4279  loss_bbox_4: 0.05562  loss_giou_4: 0.2481  loss_ce_dn_4: 0.1836  loss_mask_dn_4: 0.03601  loss_dice_dn_4: 0.4368  loss_bbox_dn_4: 0.03856  loss_giou_dn_4: 0.1848  loss_ce_5: 0.7619  loss_mask_5: 0.04564  loss_dice_5: 0.4836  loss_bbox_5: 0.05612  loss_giou_5: 0.2447  loss_ce_dn_5: 0.1719  loss_mask_dn_5: 0.03659  loss_dice_dn_5: 0.4387  loss_bbox_dn_5: 0.03691  loss_giou_dn_5: 0.1827  loss_ce_6: 0.7498  loss_mask_6: 0.04556  loss_dice_6: 0.4709  loss_bbox_6: 0.053  loss_giou_6: 0.2444  loss_ce_dn_6: 0.1667  loss_mask_dn_6: 0.03748  loss_dice_dn_6: 0.4269  loss_bbox_dn_6: 0.0361  loss_giou_dn_6: 0.1745  loss_ce_7: 0.7409  loss_mask_7: 0.04605  loss_dice_7: 0.4539  loss_bbox_7: 0.0548  loss_giou_7: 0.2451  loss_ce_dn_7: 0.1685  loss_mask_dn_7: 0.03522  loss_dice_dn_7: 0.428  loss_bbox_dn_7: 0.03613  loss_giou_dn_7: 0.1757  loss_ce_8: 0.7448  loss_mask_8: 0.0474  loss_dice_8: 0.4254  loss_bbox_8: 0.05119  loss_giou_8: 0.2368  loss_ce_dn_8: 0.1717  loss_mask_dn_8: 0.03579  loss_dice_dn_8: 0.4156  loss_bbox_dn_8: 0.03469  loss_giou_dn_8: 0.1727  loss_ce_interm: 1.102  loss_mask_interm: 0.04364  loss_dice_interm: 0.468  loss_bbox_interm: 0.08437  loss_giou_interm: 0.3379    time: 0.7889  last_time: 0.8033  data_time: 0.0152  last_data_time: 0.0091   lr: 0.0001  max_mem: 20850M\n",
      "\u001b[32m[07/13 11:21:02 d2.utils.events]: \u001b[0m eta: 0:21:55  iter: 719  total_loss: 38.99  loss_ce: 0.8186  loss_mask: 0.03946  loss_dice: 0.5619  loss_bbox: 0.06999  loss_giou: 0.2901  loss_ce_dn: 0.1963  loss_mask_dn: 0.0383  loss_dice_dn: 0.4379  loss_bbox_dn: 0.03112  loss_giou_dn: 0.2009  loss_ce_0: 1.188  loss_mask_0: 0.04511  loss_dice_0: 0.582  loss_bbox_0: 0.07152  loss_giou_0: 0.3954  loss_ce_dn_0: 2.196  loss_mask_dn_0: 0.4224  loss_dice_dn_0: 3.293  loss_bbox_dn_0: 0.2555  loss_giou_dn_0: 0.8469  loss_ce_1: 1.074  loss_mask_1: 0.04734  loss_dice_1: 0.5513  loss_bbox_1: 0.06008  loss_giou_1: 0.3165  loss_ce_dn_1: 0.347  loss_mask_dn_1: 0.04241  loss_dice_dn_1: 0.534  loss_bbox_dn_1: 0.06922  loss_giou_dn_1: 0.3283  loss_ce_2: 0.9578  loss_mask_2: 0.04144  loss_dice_2: 0.581  loss_bbox_2: 0.0563  loss_giou_2: 0.3194  loss_ce_dn_2: 0.2708  loss_mask_dn_2: 0.03994  loss_dice_dn_2: 0.4414  loss_bbox_dn_2: 0.04527  loss_giou_dn_2: 0.2484  loss_ce_3: 0.8703  loss_mask_3: 0.04172  loss_dice_3: 0.5773  loss_bbox_3: 0.05379  loss_giou_3: 0.3123  loss_ce_dn_3: 0.2237  loss_mask_dn_3: 0.03892  loss_dice_dn_3: 0.4479  loss_bbox_dn_3: 0.03748  loss_giou_dn_3: 0.2237  loss_ce_4: 0.8603  loss_mask_4: 0.04263  loss_dice_4: 0.5894  loss_bbox_4: 0.05624  loss_giou_4: 0.3234  loss_ce_dn_4: 0.2108  loss_mask_dn_4: 0.03907  loss_dice_dn_4: 0.4328  loss_bbox_dn_4: 0.03265  loss_giou_dn_4: 0.2173  loss_ce_5: 0.8564  loss_mask_5: 0.03985  loss_dice_5: 0.5244  loss_bbox_5: 0.05679  loss_giou_5: 0.3041  loss_ce_dn_5: 0.1979  loss_mask_dn_5: 0.03974  loss_dice_dn_5: 0.4209  loss_bbox_dn_5: 0.032  loss_giou_dn_5: 0.2095  loss_ce_6: 0.8185  loss_mask_6: 0.04121  loss_dice_6: 0.5618  loss_bbox_6: 0.06342  loss_giou_6: 0.3011  loss_ce_dn_6: 0.1919  loss_mask_dn_6: 0.03883  loss_dice_dn_6: 0.4146  loss_bbox_dn_6: 0.03115  loss_giou_dn_6: 0.2056  loss_ce_7: 0.8502  loss_mask_7: 0.03721  loss_dice_7: 0.5365  loss_bbox_7: 0.06869  loss_giou_7: 0.3004  loss_ce_dn_7: 0.1926  loss_mask_dn_7: 0.03835  loss_dice_dn_7: 0.424  loss_bbox_dn_7: 0.03137  loss_giou_dn_7: 0.2046  loss_ce_8: 0.8193  loss_mask_8: 0.04014  loss_dice_8: 0.5984  loss_bbox_8: 0.06308  loss_giou_8: 0.3032  loss_ce_dn_8: 0.1948  loss_mask_dn_8: 0.03759  loss_dice_dn_8: 0.4399  loss_bbox_dn_8: 0.03024  loss_giou_dn_8: 0.1997  loss_ce_interm: 1.144  loss_mask_interm: 0.04388  loss_dice_interm: 0.6103  loss_bbox_interm: 0.09471  loss_giou_interm: 0.4122    time: 0.7890  last_time: 0.8384  data_time: 0.0172  last_data_time: 0.0675   lr: 0.0001  max_mem: 20850M\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[07/13 11:21:18 d2.utils.events]: \u001b[0m eta: 0:21:40  iter: 739  total_loss: 35.46  loss_ce: 0.7467  loss_mask: 0.03676  loss_dice: 0.5306  loss_bbox: 0.042  loss_giou: 0.29  loss_ce_dn: 0.1668  loss_mask_dn: 0.03029  loss_dice_dn: 0.4517  loss_bbox_dn: 0.02897  loss_giou_dn: 0.2092  loss_ce_0: 1.129  loss_mask_0: 0.03759  loss_dice_0: 0.4694  loss_bbox_0: 0.06404  loss_giou_0: 0.4288  loss_ce_dn_0: 2.002  loss_mask_dn_0: 0.3318  loss_dice_dn_0: 3.021  loss_bbox_dn_0: 0.289  loss_giou_dn_0: 0.855  loss_ce_1: 1.031  loss_mask_1: 0.04093  loss_dice_1: 0.4586  loss_bbox_1: 0.05634  loss_giou_1: 0.3017  loss_ce_dn_1: 0.3004  loss_mask_dn_1: 0.03659  loss_dice_dn_1: 0.475  loss_bbox_dn_1: 0.0653  loss_giou_dn_1: 0.3231  loss_ce_2: 0.9215  loss_mask_2: 0.03785  loss_dice_2: 0.5782  loss_bbox_2: 0.04786  loss_giou_2: 0.3019  loss_ce_dn_2: 0.2278  loss_mask_dn_2: 0.03399  loss_dice_dn_2: 0.4453  loss_bbox_dn_2: 0.03955  loss_giou_dn_2: 0.233  loss_ce_3: 0.8382  loss_mask_3: 0.04053  loss_dice_3: 0.4852  loss_bbox_3: 0.04212  loss_giou_3: 0.2669  loss_ce_dn_3: 0.1987  loss_mask_dn_3: 0.03264  loss_dice_dn_3: 0.4472  loss_bbox_dn_3: 0.03362  loss_giou_dn_3: 0.2184  loss_ce_4: 0.842  loss_mask_4: 0.03557  loss_dice_4: 0.4608  loss_bbox_4: 0.04137  loss_giou_4: 0.2625  loss_ce_dn_4: 0.1795  loss_mask_dn_4: 0.0321  loss_dice_dn_4: 0.4442  loss_bbox_dn_4: 0.03052  loss_giou_dn_4: 0.2088  loss_ce_5: 0.7769  loss_mask_5: 0.03781  loss_dice_5: 0.5068  loss_bbox_5: 0.04117  loss_giou_5: 0.2669  loss_ce_dn_5: 0.1738  loss_mask_dn_5: 0.03045  loss_dice_dn_5: 0.4345  loss_bbox_dn_5: 0.03056  loss_giou_dn_5: 0.2102  loss_ce_6: 0.7843  loss_mask_6: 0.03864  loss_dice_6: 0.4528  loss_bbox_6: 0.0417  loss_giou_6: 0.2645  loss_ce_dn_6: 0.167  loss_mask_dn_6: 0.03068  loss_dice_dn_6: 0.4343  loss_bbox_dn_6: 0.02943  loss_giou_dn_6: 0.2077  loss_ce_7: 0.7565  loss_mask_7: 0.03677  loss_dice_7: 0.5323  loss_bbox_7: 0.04188  loss_giou_7: 0.2669  loss_ce_dn_7: 0.1692  loss_mask_dn_7: 0.03002  loss_dice_dn_7: 0.437  loss_bbox_dn_7: 0.02938  loss_giou_dn_7: 0.2082  loss_ce_8: 0.7373  loss_mask_8: 0.03807  loss_dice_8: 0.4488  loss_bbox_8: 0.04264  loss_giou_8: 0.2691  loss_ce_dn_8: 0.168  loss_mask_dn_8: 0.03015  loss_dice_dn_8: 0.4492  loss_bbox_dn_8: 0.02884  loss_giou_dn_8: 0.2071  loss_ce_interm: 1.138  loss_mask_interm: 0.03488  loss_dice_interm: 0.582  loss_bbox_interm: 0.08983  loss_giou_interm: 0.377    time: 0.7892  last_time: 0.7850  data_time: 0.0165  last_data_time: 0.0075   lr: 0.0001  max_mem: 20850M\n",
      "\u001b[32m[07/13 11:21:34 d2.utils.events]: \u001b[0m eta: 0:21:24  iter: 759  total_loss: 35.28  loss_ce: 0.77  loss_mask: 0.05691  loss_dice: 0.4513  loss_bbox: 0.04661  loss_giou: 0.2581  loss_ce_dn: 0.1635  loss_mask_dn: 0.05563  loss_dice_dn: 0.4281  loss_bbox_dn: 0.03101  loss_giou_dn: 0.1781  loss_ce_0: 1.103  loss_mask_0: 0.04383  loss_dice_0: 0.5033  loss_bbox_0: 0.06276  loss_giou_0: 0.3904  loss_ce_dn_0: 1.956  loss_mask_dn_0: 0.5255  loss_dice_dn_0: 2.974  loss_bbox_dn_0: 0.3458  loss_giou_dn_0: 0.8548  loss_ce_1: 1.021  loss_mask_1: 0.05529  loss_dice_1: 0.5111  loss_bbox_1: 0.04931  loss_giou_1: 0.3204  loss_ce_dn_1: 0.302  loss_mask_dn_1: 0.06158  loss_dice_dn_1: 0.507  loss_bbox_dn_1: 0.08386  loss_giou_dn_1: 0.3111  loss_ce_2: 0.9165  loss_mask_2: 0.05565  loss_dice_2: 0.4862  loss_bbox_2: 0.05518  loss_giou_2: 0.3008  loss_ce_dn_2: 0.2101  loss_mask_dn_2: 0.05786  loss_dice_dn_2: 0.4462  loss_bbox_dn_2: 0.05224  loss_giou_dn_2: 0.2236  loss_ce_3: 0.7965  loss_mask_3: 0.05583  loss_dice_3: 0.5161  loss_bbox_3: 0.04684  loss_giou_3: 0.2697  loss_ce_dn_3: 0.188  loss_mask_dn_3: 0.05268  loss_dice_dn_3: 0.4369  loss_bbox_dn_3: 0.037  loss_giou_dn_3: 0.1974  loss_ce_4: 0.7529  loss_mask_4: 0.05756  loss_dice_4: 0.5167  loss_bbox_4: 0.04638  loss_giou_4: 0.2694  loss_ce_dn_4: 0.1775  loss_mask_dn_4: 0.05444  loss_dice_dn_4: 0.4196  loss_bbox_dn_4: 0.03376  loss_giou_dn_4: 0.1855  loss_ce_5: 0.7635  loss_mask_5: 0.05215  loss_dice_5: 0.4791  loss_bbox_5: 0.04323  loss_giou_5: 0.2688  loss_ce_dn_5: 0.169  loss_mask_dn_5: 0.05126  loss_dice_dn_5: 0.4188  loss_bbox_dn_5: 0.03266  loss_giou_dn_5: 0.1829  loss_ce_6: 0.7542  loss_mask_6: 0.05078  loss_dice_6: 0.4294  loss_bbox_6: 0.044  loss_giou_6: 0.265  loss_ce_dn_6: 0.1645  loss_mask_dn_6: 0.05218  loss_dice_dn_6: 0.4123  loss_bbox_dn_6: 0.03141  loss_giou_dn_6: 0.1784  loss_ce_7: 0.7478  loss_mask_7: 0.05346  loss_dice_7: 0.5173  loss_bbox_7: 0.04406  loss_giou_7: 0.26  loss_ce_dn_7: 0.1626  loss_mask_dn_7: 0.05498  loss_dice_dn_7: 0.4251  loss_bbox_dn_7: 0.03142  loss_giou_dn_7: 0.1797  loss_ce_8: 0.756  loss_mask_8: 0.05399  loss_dice_8: 0.4609  loss_bbox_8: 0.04969  loss_giou_8: 0.261  loss_ce_dn_8: 0.1616  loss_mask_dn_8: 0.05544  loss_dice_dn_8: 0.4376  loss_bbox_dn_8: 0.03095  loss_giou_dn_8: 0.1778  loss_ce_interm: 1.063  loss_mask_interm: 0.05462  loss_dice_interm: 0.5193  loss_bbox_interm: 0.08606  loss_giou_interm: 0.3633    time: 0.7893  last_time: 0.7826  data_time: 0.0147  last_data_time: 0.0082   lr: 0.0001  max_mem: 20850M\n",
      "\u001b[32m[07/13 11:21:50 d2.utils.events]: \u001b[0m eta: 0:21:09  iter: 779  total_loss: 36.31  loss_ce: 0.7562  loss_mask: 0.05011  loss_dice: 0.5349  loss_bbox: 0.06573  loss_giou: 0.2639  loss_ce_dn: 0.1737  loss_mask_dn: 0.04829  loss_dice_dn: 0.4584  loss_bbox_dn: 0.04111  loss_giou_dn: 0.1999  loss_ce_0: 1.108  loss_mask_0: 0.0546  loss_dice_0: 0.5724  loss_bbox_0: 0.1031  loss_giou_0: 0.461  loss_ce_dn_0: 2.011  loss_mask_dn_0: 0.5385  loss_dice_dn_0: 2.977  loss_bbox_dn_0: 0.297  loss_giou_dn_0: 0.8542  loss_ce_1: 1.095  loss_mask_1: 0.05731  loss_dice_1: 0.5917  loss_bbox_1: 0.08116  loss_giou_1: 0.3642  loss_ce_dn_1: 0.2834  loss_mask_dn_1: 0.05047  loss_dice_dn_1: 0.5161  loss_bbox_dn_1: 0.08984  loss_giou_dn_1: 0.3446  loss_ce_2: 0.9656  loss_mask_2: 0.05751  loss_dice_2: 0.5762  loss_bbox_2: 0.08617  loss_giou_2: 0.2996  loss_ce_dn_2: 0.2307  loss_mask_dn_2: 0.04481  loss_dice_dn_2: 0.497  loss_bbox_dn_2: 0.05923  loss_giou_dn_2: 0.2638  loss_ce_3: 0.8406  loss_mask_3: 0.05476  loss_dice_3: 0.5455  loss_bbox_3: 0.08221  loss_giou_3: 0.29  loss_ce_dn_3: 0.1948  loss_mask_dn_3: 0.04412  loss_dice_dn_3: 0.4589  loss_bbox_dn_3: 0.05189  loss_giou_dn_3: 0.2307  loss_ce_4: 0.822  loss_mask_4: 0.05542  loss_dice_4: 0.5437  loss_bbox_4: 0.07952  loss_giou_4: 0.2806  loss_ce_dn_4: 0.1846  loss_mask_dn_4: 0.0463  loss_dice_dn_4: 0.4551  loss_bbox_dn_4: 0.04649  loss_giou_dn_4: 0.2094  loss_ce_5: 0.7735  loss_mask_5: 0.06054  loss_dice_5: 0.502  loss_bbox_5: 0.07646  loss_giou_5: 0.2741  loss_ce_dn_5: 0.1686  loss_mask_dn_5: 0.04788  loss_dice_dn_5: 0.4656  loss_bbox_dn_5: 0.04543  loss_giou_dn_5: 0.2094  loss_ce_6: 0.7731  loss_mask_6: 0.06092  loss_dice_6: 0.4825  loss_bbox_6: 0.06998  loss_giou_6: 0.2748  loss_ce_dn_6: 0.1698  loss_mask_dn_6: 0.04836  loss_dice_dn_6: 0.4487  loss_bbox_dn_6: 0.04291  loss_giou_dn_6: 0.2044  loss_ce_7: 0.7732  loss_mask_7: 0.05202  loss_dice_7: 0.4994  loss_bbox_7: 0.06992  loss_giou_7: 0.2755  loss_ce_dn_7: 0.1719  loss_mask_dn_7: 0.04854  loss_dice_dn_7: 0.4401  loss_bbox_dn_7: 0.04324  loss_giou_dn_7: 0.2038  loss_ce_8: 0.7689  loss_mask_8: 0.0574  loss_dice_8: 0.4856  loss_bbox_8: 0.06369  loss_giou_8: 0.2858  loss_ce_dn_8: 0.1697  loss_mask_dn_8: 0.04787  loss_dice_dn_8: 0.4401  loss_bbox_dn_8: 0.04115  loss_giou_dn_8: 0.1988  loss_ce_interm: 1.124  loss_mask_interm: 0.05331  loss_dice_interm: 0.5422  loss_bbox_interm: 0.1047  loss_giou_interm: 0.4111    time: 0.7894  last_time: 0.7909  data_time: 0.0181  last_data_time: 0.0078   lr: 0.0001  max_mem: 20850M\n",
      "\u001b[32m[07/13 11:22:06 d2.utils.events]: \u001b[0m eta: 0:20:53  iter: 799  total_loss: 32.68  loss_ce: 0.7075  loss_mask: 0.04496  loss_dice: 0.4339  loss_bbox: 0.05797  loss_giou: 0.2243  loss_ce_dn: 0.1583  loss_mask_dn: 0.051  loss_dice_dn: 0.3873  loss_bbox_dn: 0.03626  loss_giou_dn: 0.1748  loss_ce_0: 0.9996  loss_mask_0: 0.05033  loss_dice_0: 0.4267  loss_bbox_0: 0.0834  loss_giou_0: 0.4089  loss_ce_dn_0: 2.101  loss_mask_dn_0: 0.5781  loss_dice_dn_0: 3.023  loss_bbox_dn_0: 0.3516  loss_giou_dn_0: 0.8598  loss_ce_1: 0.9454  loss_mask_1: 0.04848  loss_dice_1: 0.3733  loss_bbox_1: 0.06831  loss_giou_1: 0.3011  loss_ce_dn_1: 0.2921  loss_mask_dn_1: 0.05268  loss_dice_dn_1: 0.4424  loss_bbox_dn_1: 0.08485  loss_giou_dn_1: 0.3008  loss_ce_2: 0.7946  loss_mask_2: 0.04887  loss_dice_2: 0.4119  loss_bbox_2: 0.0646  loss_giou_2: 0.2705  loss_ce_dn_2: 0.2203  loss_mask_dn_2: 0.05155  loss_dice_dn_2: 0.4211  loss_bbox_dn_2: 0.05286  loss_giou_dn_2: 0.2116  loss_ce_3: 0.7473  loss_mask_3: 0.05359  loss_dice_3: 0.4509  loss_bbox_3: 0.05571  loss_giou_3: 0.2628  loss_ce_dn_3: 0.1829  loss_mask_dn_3: 0.04887  loss_dice_dn_3: 0.4207  loss_bbox_dn_3: 0.04607  loss_giou_dn_3: 0.1898  loss_ce_4: 0.7432  loss_mask_4: 0.04997  loss_dice_4: 0.4205  loss_bbox_4: 0.04888  loss_giou_4: 0.253  loss_ce_dn_4: 0.1625  loss_mask_dn_4: 0.04902  loss_dice_dn_4: 0.3974  loss_bbox_dn_4: 0.04131  loss_giou_dn_4: 0.1804  loss_ce_5: 0.7081  loss_mask_5: 0.05105  loss_dice_5: 0.4254  loss_bbox_5: 0.05926  loss_giou_5: 0.241  loss_ce_dn_5: 0.1587  loss_mask_dn_5: 0.04771  loss_dice_dn_5: 0.4001  loss_bbox_dn_5: 0.03976  loss_giou_dn_5: 0.1798  loss_ce_6: 0.6989  loss_mask_6: 0.04761  loss_dice_6: 0.4486  loss_bbox_6: 0.05932  loss_giou_6: 0.2383  loss_ce_dn_6: 0.1547  loss_mask_dn_6: 0.05149  loss_dice_dn_6: 0.3794  loss_bbox_dn_6: 0.03708  loss_giou_dn_6: 0.1742  loss_ce_7: 0.6851  loss_mask_7: 0.04673  loss_dice_7: 0.4416  loss_bbox_7: 0.05844  loss_giou_7: 0.2399  loss_ce_dn_7: 0.1612  loss_mask_dn_7: 0.0524  loss_dice_dn_7: 0.3998  loss_bbox_dn_7: 0.03645  loss_giou_dn_7: 0.1742  loss_ce_8: 0.6704  loss_mask_8: 0.04679  loss_dice_8: 0.4278  loss_bbox_8: 0.05985  loss_giou_8: 0.2279  loss_ce_dn_8: 0.1557  loss_mask_dn_8: 0.05007  loss_dice_dn_8: 0.3885  loss_bbox_dn_8: 0.03632  loss_giou_dn_8: 0.1735  loss_ce_interm: 0.9801  loss_mask_interm: 0.05366  loss_dice_interm: 0.4358  loss_bbox_interm: 0.09565  loss_giou_interm: 0.3406    time: 0.7895  last_time: 0.7787  data_time: 0.0163  last_data_time: 0.0087   lr: 0.0001  max_mem: 20850M\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[07/13 11:22:22 d2.utils.events]: \u001b[0m eta: 0:20:38  iter: 819  total_loss: 35.02  loss_ce: 0.6241  loss_mask: 0.0614  loss_dice: 0.4352  loss_bbox: 0.07231  loss_giou: 0.2241  loss_ce_dn: 0.1553  loss_mask_dn: 0.06282  loss_dice_dn: 0.3822  loss_bbox_dn: 0.0366  loss_giou_dn: 0.1569  loss_ce_0: 1.016  loss_mask_0: 0.06742  loss_dice_0: 0.4698  loss_bbox_0: 0.09545  loss_giou_0: 0.3388  loss_ce_dn_0: 1.905  loss_mask_dn_0: 0.5292  loss_dice_dn_0: 2.949  loss_bbox_dn_0: 0.3729  loss_giou_dn_0: 0.8493  loss_ce_1: 0.9279  loss_mask_1: 0.06198  loss_dice_1: 0.4743  loss_bbox_1: 0.08994  loss_giou_1: 0.2539  loss_ce_dn_1: 0.2933  loss_mask_dn_1: 0.07313  loss_dice_dn_1: 0.4591  loss_bbox_dn_1: 0.08609  loss_giou_dn_1: 0.3048  loss_ce_2: 0.8151  loss_mask_2: 0.06467  loss_dice_2: 0.466  loss_bbox_2: 0.08161  loss_giou_2: 0.2417  loss_ce_dn_2: 0.2076  loss_mask_dn_2: 0.06884  loss_dice_dn_2: 0.3882  loss_bbox_dn_2: 0.05729  loss_giou_dn_2: 0.2212  loss_ce_3: 0.7478  loss_mask_3: 0.05824  loss_dice_3: 0.47  loss_bbox_3: 0.07316  loss_giou_3: 0.2349  loss_ce_dn_3: 0.184  loss_mask_dn_3: 0.0608  loss_dice_dn_3: 0.38  loss_bbox_dn_3: 0.04375  loss_giou_dn_3: 0.1896  loss_ce_4: 0.699  loss_mask_4: 0.06888  loss_dice_4: 0.4791  loss_bbox_4: 0.07702  loss_giou_4: 0.2447  loss_ce_dn_4: 0.1658  loss_mask_dn_4: 0.05721  loss_dice_dn_4: 0.3687  loss_bbox_dn_4: 0.03835  loss_giou_dn_4: 0.179  loss_ce_5: 0.684  loss_mask_5: 0.05545  loss_dice_5: 0.4426  loss_bbox_5: 0.0645  loss_giou_5: 0.2476  loss_ce_dn_5: 0.1582  loss_mask_dn_5: 0.05721  loss_dice_dn_5: 0.3754  loss_bbox_dn_5: 0.03743  loss_giou_dn_5: 0.169  loss_ce_6: 0.657  loss_mask_6: 0.05791  loss_dice_6: 0.4458  loss_bbox_6: 0.07052  loss_giou_6: 0.228  loss_ce_dn_6: 0.1503  loss_mask_dn_6: 0.05878  loss_dice_dn_6: 0.3802  loss_bbox_dn_6: 0.03729  loss_giou_dn_6: 0.1604  loss_ce_7: 0.6367  loss_mask_7: 0.06183  loss_dice_7: 0.4219  loss_bbox_7: 0.07083  loss_giou_7: 0.2263  loss_ce_dn_7: 0.1547  loss_mask_dn_7: 0.06128  loss_dice_dn_7: 0.3759  loss_bbox_dn_7: 0.03673  loss_giou_dn_7: 0.1618  loss_ce_8: 0.6358  loss_mask_8: 0.06377  loss_dice_8: 0.4329  loss_bbox_8: 0.06161  loss_giou_8: 0.2367  loss_ce_dn_8: 0.1548  loss_mask_dn_8: 0.06134  loss_dice_dn_8: 0.3827  loss_bbox_dn_8: 0.03614  loss_giou_dn_8: 0.1567  loss_ce_interm: 1.006  loss_mask_interm: 0.06291  loss_dice_interm: 0.4487  loss_bbox_interm: 0.09879  loss_giou_interm: 0.37    time: 0.7896  last_time: 0.7857  data_time: 0.0187  last_data_time: 0.0075   lr: 1e-05  max_mem: 20850M\n",
      "\u001b[32m[07/13 11:22:38 d2.utils.events]: \u001b[0m eta: 0:20:22  iter: 839  total_loss: 33.26  loss_ce: 0.6473  loss_mask: 0.06368  loss_dice: 0.4262  loss_bbox: 0.05863  loss_giou: 0.1936  loss_ce_dn: 0.1456  loss_mask_dn: 0.03904  loss_dice_dn: 0.4013  loss_bbox_dn: 0.03955  loss_giou_dn: 0.1835  loss_ce_0: 1.051  loss_mask_0: 0.08047  loss_dice_0: 0.499  loss_bbox_0: 0.07123  loss_giou_0: 0.3559  loss_ce_dn_0: 2.036  loss_mask_dn_0: 0.4543  loss_dice_dn_0: 2.8  loss_bbox_dn_0: 0.3296  loss_giou_dn_0: 0.8496  loss_ce_1: 0.9372  loss_mask_1: 0.09101  loss_dice_1: 0.4473  loss_bbox_1: 0.06357  loss_giou_1: 0.2725  loss_ce_dn_1: 0.2791  loss_mask_dn_1: 0.05209  loss_dice_dn_1: 0.4265  loss_bbox_dn_1: 0.08412  loss_giou_dn_1: 0.2897  loss_ce_2: 0.8262  loss_mask_2: 0.07928  loss_dice_2: 0.4643  loss_bbox_2: 0.06581  loss_giou_2: 0.2476  loss_ce_dn_2: 0.2081  loss_mask_dn_2: 0.04703  loss_dice_dn_2: 0.427  loss_bbox_dn_2: 0.05167  loss_giou_dn_2: 0.2245  loss_ce_3: 0.7771  loss_mask_3: 0.0831  loss_dice_3: 0.4908  loss_bbox_3: 0.07588  loss_giou_3: 0.2995  loss_ce_dn_3: 0.1788  loss_mask_dn_3: 0.04068  loss_dice_dn_3: 0.4081  loss_bbox_dn_3: 0.04655  loss_giou_dn_3: 0.1956  loss_ce_4: 0.7051  loss_mask_4: 0.06989  loss_dice_4: 0.5155  loss_bbox_4: 0.06265  loss_giou_4: 0.2426  loss_ce_dn_4: 0.161  loss_mask_dn_4: 0.03934  loss_dice_dn_4: 0.3923  loss_bbox_dn_4: 0.04245  loss_giou_dn_4: 0.1904  loss_ce_5: 0.6616  loss_mask_5: 0.06632  loss_dice_5: 0.4427  loss_bbox_5: 0.0784  loss_giou_5: 0.2365  loss_ce_dn_5: 0.1517  loss_mask_dn_5: 0.03804  loss_dice_dn_5: 0.3843  loss_bbox_dn_5: 0.04132  loss_giou_dn_5: 0.1857  loss_ce_6: 0.6408  loss_mask_6: 0.06595  loss_dice_6: 0.4093  loss_bbox_6: 0.05983  loss_giou_6: 0.2014  loss_ce_dn_6: 0.1465  loss_mask_dn_6: 0.03739  loss_dice_dn_6: 0.4015  loss_bbox_dn_6: 0.03981  loss_giou_dn_6: 0.1869  loss_ce_7: 0.6364  loss_mask_7: 0.06723  loss_dice_7: 0.4448  loss_bbox_7: 0.05111  loss_giou_7: 0.2162  loss_ce_dn_7: 0.143  loss_mask_dn_7: 0.0373  loss_dice_dn_7: 0.4017  loss_bbox_dn_7: 0.03999  loss_giou_dn_7: 0.185  loss_ce_8: 0.6426  loss_mask_8: 0.06358  loss_dice_8: 0.4705  loss_bbox_8: 0.05089  loss_giou_8: 0.2215  loss_ce_dn_8: 0.1442  loss_mask_dn_8: 0.03907  loss_dice_dn_8: 0.4136  loss_bbox_dn_8: 0.03974  loss_giou_dn_8: 0.1835  loss_ce_interm: 1.008  loss_mask_interm: 0.07921  loss_dice_interm: 0.4944  loss_bbox_interm: 0.1025  loss_giou_interm: 0.3385    time: 0.7897  last_time: 0.8398  data_time: 0.0196  last_data_time: 0.0583   lr: 1e-05  max_mem: 20850M\n",
      "\u001b[32m[07/13 11:22:54 d2.utils.events]: \u001b[0m eta: 0:20:07  iter: 859  total_loss: 30.67  loss_ce: 0.6205  loss_mask: 0.05178  loss_dice: 0.3546  loss_bbox: 0.04536  loss_giou: 0.1971  loss_ce_dn: 0.1657  loss_mask_dn: 0.04484  loss_dice_dn: 0.3246  loss_bbox_dn: 0.02962  loss_giou_dn: 0.1529  loss_ce_0: 1.001  loss_mask_0: 0.04885  loss_dice_0: 0.4099  loss_bbox_0: 0.06753  loss_giou_0: 0.3678  loss_ce_dn_0: 1.973  loss_mask_dn_0: 0.4997  loss_dice_dn_0: 2.893  loss_bbox_dn_0: 0.3299  loss_giou_dn_0: 0.8556  loss_ce_1: 0.8959  loss_mask_1: 0.05616  loss_dice_1: 0.4809  loss_bbox_1: 0.05293  loss_giou_1: 0.2876  loss_ce_dn_1: 0.2923  loss_mask_dn_1: 0.05182  loss_dice_dn_1: 0.3838  loss_bbox_dn_1: 0.07402  loss_giou_dn_1: 0.2775  loss_ce_2: 0.7918  loss_mask_2: 0.05304  loss_dice_2: 0.3612  loss_bbox_2: 0.05096  loss_giou_2: 0.2351  loss_ce_dn_2: 0.2231  loss_mask_dn_2: 0.04581  loss_dice_dn_2: 0.353  loss_bbox_dn_2: 0.04351  loss_giou_dn_2: 0.2105  loss_ce_3: 0.6868  loss_mask_3: 0.0551  loss_dice_3: 0.4108  loss_bbox_3: 0.0516  loss_giou_3: 0.2476  loss_ce_dn_3: 0.1933  loss_mask_dn_3: 0.04521  loss_dice_dn_3: 0.3513  loss_bbox_dn_3: 0.03603  loss_giou_dn_3: 0.1759  loss_ce_4: 0.6336  loss_mask_4: 0.05099  loss_dice_4: 0.3595  loss_bbox_4: 0.05318  loss_giou_4: 0.2338  loss_ce_dn_4: 0.1778  loss_mask_dn_4: 0.04781  loss_dice_dn_4: 0.3518  loss_bbox_dn_4: 0.03341  loss_giou_dn_4: 0.1699  loss_ce_5: 0.6505  loss_mask_5: 0.04888  loss_dice_5: 0.3744  loss_bbox_5: 0.04968  loss_giou_5: 0.2194  loss_ce_dn_5: 0.1731  loss_mask_dn_5: 0.04826  loss_dice_dn_5: 0.3497  loss_bbox_dn_5: 0.03144  loss_giou_dn_5: 0.1633  loss_ce_6: 0.6357  loss_mask_6: 0.05414  loss_dice_6: 0.3679  loss_bbox_6: 0.04454  loss_giou_6: 0.2169  loss_ce_dn_6: 0.1684  loss_mask_dn_6: 0.04543  loss_dice_dn_6: 0.336  loss_bbox_dn_6: 0.03035  loss_giou_dn_6: 0.1586  loss_ce_7: 0.6237  loss_mask_7: 0.05255  loss_dice_7: 0.375  loss_bbox_7: 0.04461  loss_giou_7: 0.2144  loss_ce_dn_7: 0.1668  loss_mask_dn_7: 0.04394  loss_dice_dn_7: 0.3644  loss_bbox_dn_7: 0.03053  loss_giou_dn_7: 0.1591  loss_ce_8: 0.6314  loss_mask_8: 0.04936  loss_dice_8: 0.3984  loss_bbox_8: 0.04803  loss_giou_8: 0.2162  loss_ce_dn_8: 0.1641  loss_mask_dn_8: 0.04342  loss_dice_dn_8: 0.3492  loss_bbox_dn_8: 0.0297  loss_giou_dn_8: 0.1539  loss_ce_interm: 1.004  loss_mask_interm: 0.04998  loss_dice_interm: 0.4207  loss_bbox_interm: 0.08542  loss_giou_interm: 0.3131    time: 0.7900  last_time: 0.7938  data_time: 0.0202  last_data_time: 0.0089   lr: 1e-05  max_mem: 20850M\n",
      "\u001b[32m[07/13 11:23:10 d2.utils.events]: \u001b[0m eta: 0:19:51  iter: 879  total_loss: 33.37  loss_ce: 0.6396  loss_mask: 0.04009  loss_dice: 0.4842  loss_bbox: 0.0401  loss_giou: 0.2581  loss_ce_dn: 0.1496  loss_mask_dn: 0.0353  loss_dice_dn: 0.4063  loss_bbox_dn: 0.03262  loss_giou_dn: 0.2123  loss_ce_0: 0.9975  loss_mask_0: 0.04821  loss_dice_0: 0.5304  loss_bbox_0: 0.06272  loss_giou_0: 0.4036  loss_ce_dn_0: 2.005  loss_mask_dn_0: 0.4175  loss_dice_dn_0: 2.865  loss_bbox_dn_0: 0.2886  loss_giou_dn_0: 0.8556  loss_ce_1: 0.9479  loss_mask_1: 0.04768  loss_dice_1: 0.4763  loss_bbox_1: 0.0497  loss_giou_1: 0.318  loss_ce_dn_1: 0.2925  loss_mask_dn_1: 0.04803  loss_dice_dn_1: 0.4596  loss_bbox_dn_1: 0.07547  loss_giou_dn_1: 0.3218  loss_ce_2: 0.8224  loss_mask_2: 0.05069  loss_dice_2: 0.5185  loss_bbox_2: 0.04836  loss_giou_2: 0.2983  loss_ce_dn_2: 0.2195  loss_mask_dn_2: 0.04215  loss_dice_dn_2: 0.4398  loss_bbox_dn_2: 0.04971  loss_giou_dn_2: 0.2457  loss_ce_3: 0.7237  loss_mask_3: 0.04646  loss_dice_3: 0.5227  loss_bbox_3: 0.04679  loss_giou_3: 0.2776  loss_ce_dn_3: 0.1837  loss_mask_dn_3: 0.03834  loss_dice_dn_3: 0.4142  loss_bbox_dn_3: 0.04046  loss_giou_dn_3: 0.2242  loss_ce_4: 0.749  loss_mask_4: 0.0421  loss_dice_4: 0.5353  loss_bbox_4: 0.04662  loss_giou_4: 0.2645  loss_ce_dn_4: 0.1695  loss_mask_dn_4: 0.03764  loss_dice_dn_4: 0.389  loss_bbox_dn_4: 0.03686  loss_giou_dn_4: 0.2132  loss_ce_5: 0.6666  loss_mask_5: 0.04397  loss_dice_5: 0.4393  loss_bbox_5: 0.04257  loss_giou_5: 0.2616  loss_ce_dn_5: 0.1598  loss_mask_dn_5: 0.03822  loss_dice_dn_5: 0.3985  loss_bbox_dn_5: 0.03497  loss_giou_dn_5: 0.2106  loss_ce_6: 0.6676  loss_mask_6: 0.04291  loss_dice_6: 0.4796  loss_bbox_6: 0.04099  loss_giou_6: 0.2577  loss_ce_dn_6: 0.1484  loss_mask_dn_6: 0.03696  loss_dice_dn_6: 0.4207  loss_bbox_dn_6: 0.03357  loss_giou_dn_6: 0.2114  loss_ce_7: 0.6618  loss_mask_7: 0.04043  loss_dice_7: 0.4563  loss_bbox_7: 0.04792  loss_giou_7: 0.2614  loss_ce_dn_7: 0.1481  loss_mask_dn_7: 0.03566  loss_dice_dn_7: 0.3925  loss_bbox_dn_7: 0.03316  loss_giou_dn_7: 0.211  loss_ce_8: 0.6436  loss_mask_8: 0.04141  loss_dice_8: 0.5115  loss_bbox_8: 0.04554  loss_giou_8: 0.2599  loss_ce_dn_8: 0.1469  loss_mask_dn_8: 0.0347  loss_dice_dn_8: 0.4284  loss_bbox_dn_8: 0.03245  loss_giou_dn_8: 0.2112  loss_ce_interm: 1.002  loss_mask_interm: 0.04364  loss_dice_interm: 0.5002  loss_bbox_interm: 0.08611  loss_giou_interm: 0.3511    time: 0.7899  last_time: 0.8155  data_time: 0.0102  last_data_time: 0.0512   lr: 1e-05  max_mem: 20850M\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[07/13 11:23:26 d2.utils.events]: \u001b[0m eta: 0:19:35  iter: 899  total_loss: 34.34  loss_ce: 0.6024  loss_mask: 0.03792  loss_dice: 0.5046  loss_bbox: 0.05427  loss_giou: 0.2498  loss_ce_dn: 0.1683  loss_mask_dn: 0.03389  loss_dice_dn: 0.3411  loss_bbox_dn: 0.02831  loss_giou_dn: 0.1503  loss_ce_0: 1.069  loss_mask_0: 0.04541  loss_dice_0: 0.5225  loss_bbox_0: 0.08711  loss_giou_0: 0.3907  loss_ce_dn_0: 1.904  loss_mask_dn_0: 0.4723  loss_dice_dn_0: 2.984  loss_bbox_dn_0: 0.3091  loss_giou_dn_0: 0.852  loss_ce_1: 0.928  loss_mask_1: 0.04659  loss_dice_1: 0.5285  loss_bbox_1: 0.06182  loss_giou_1: 0.2959  loss_ce_dn_1: 0.3238  loss_mask_dn_1: 0.04143  loss_dice_dn_1: 0.386  loss_bbox_dn_1: 0.06071  loss_giou_dn_1: 0.2716  loss_ce_2: 0.8115  loss_mask_2: 0.04778  loss_dice_2: 0.536  loss_bbox_2: 0.06002  loss_giou_2: 0.292  loss_ce_dn_2: 0.2391  loss_mask_dn_2: 0.03616  loss_dice_dn_2: 0.3692  loss_bbox_dn_2: 0.03804  loss_giou_dn_2: 0.187  loss_ce_3: 0.6912  loss_mask_3: 0.04252  loss_dice_3: 0.5156  loss_bbox_3: 0.05557  loss_giou_3: 0.2649  loss_ce_dn_3: 0.1865  loss_mask_dn_3: 0.03398  loss_dice_dn_3: 0.3688  loss_bbox_dn_3: 0.03115  loss_giou_dn_3: 0.1656  loss_ce_4: 0.6742  loss_mask_4: 0.03539  loss_dice_4: 0.5246  loss_bbox_4: 0.05746  loss_giou_4: 0.2786  loss_ce_dn_4: 0.1763  loss_mask_dn_4: 0.03337  loss_dice_dn_4: 0.3545  loss_bbox_dn_4: 0.02973  loss_giou_dn_4: 0.1574  loss_ce_5: 0.674  loss_mask_5: 0.04106  loss_dice_5: 0.5178  loss_bbox_5: 0.05567  loss_giou_5: 0.2634  loss_ce_dn_5: 0.176  loss_mask_dn_5: 0.03355  loss_dice_dn_5: 0.3771  loss_bbox_dn_5: 0.02917  loss_giou_dn_5: 0.1532  loss_ce_6: 0.6316  loss_mask_6: 0.03817  loss_dice_6: 0.5368  loss_bbox_6: 0.05337  loss_giou_6: 0.2534  loss_ce_dn_6: 0.1714  loss_mask_dn_6: 0.03377  loss_dice_dn_6: 0.3594  loss_bbox_dn_6: 0.02844  loss_giou_dn_6: 0.1525  loss_ce_7: 0.6562  loss_mask_7: 0.04089  loss_dice_7: 0.4284  loss_bbox_7: 0.05532  loss_giou_7: 0.2501  loss_ce_dn_7: 0.1667  loss_mask_dn_7: 0.03399  loss_dice_dn_7: 0.3585  loss_bbox_dn_7: 0.02838  loss_giou_dn_7: 0.1526  loss_ce_8: 0.6158  loss_mask_8: 0.0396  loss_dice_8: 0.4634  loss_bbox_8: 0.05424  loss_giou_8: 0.2505  loss_ce_dn_8: 0.1653  loss_mask_dn_8: 0.03354  loss_dice_dn_8: 0.35  loss_bbox_dn_8: 0.02821  loss_giou_dn_8: 0.1508  loss_ce_interm: 1.045  loss_mask_interm: 0.04595  loss_dice_interm: 0.4856  loss_bbox_interm: 0.08697  loss_giou_interm: 0.3498    time: 0.7900  last_time: 0.7727  data_time: 0.0146  last_data_time: 0.0066   lr: 1e-05  max_mem: 20850M\n",
      "\u001b[32m[07/13 11:23:41 d2.utils.events]: \u001b[0m eta: 0:19:20  iter: 919  total_loss: 28.21  loss_ce: 0.5734  loss_mask: 0.04285  loss_dice: 0.4073  loss_bbox: 0.03839  loss_giou: 0.2001  loss_ce_dn: 0.1318  loss_mask_dn: 0.03875  loss_dice_dn: 0.3081  loss_bbox_dn: 0.0268  loss_giou_dn: 0.1438  loss_ce_0: 1.026  loss_mask_0: 0.04527  loss_dice_0: 0.3275  loss_bbox_0: 0.06721  loss_giou_0: 0.3176  loss_ce_dn_0: 1.909  loss_mask_dn_0: 0.5459  loss_dice_dn_0: 2.729  loss_bbox_dn_0: 0.3159  loss_giou_dn_0: 0.8496  loss_ce_1: 0.8755  loss_mask_1: 0.05216  loss_dice_1: 0.3179  loss_bbox_1: 0.04356  loss_giou_1: 0.2543  loss_ce_dn_1: 0.2777  loss_mask_dn_1: 0.04174  loss_dice_dn_1: 0.3239  loss_bbox_dn_1: 0.06552  loss_giou_dn_1: 0.259  loss_ce_2: 0.7438  loss_mask_2: 0.04489  loss_dice_2: 0.3595  loss_bbox_2: 0.04365  loss_giou_2: 0.2341  loss_ce_dn_2: 0.2035  loss_mask_dn_2: 0.04006  loss_dice_dn_2: 0.3203  loss_bbox_dn_2: 0.0415  loss_giou_dn_2: 0.1872  loss_ce_3: 0.692  loss_mask_3: 0.05224  loss_dice_3: 0.3355  loss_bbox_3: 0.04105  loss_giou_3: 0.2225  loss_ce_dn_3: 0.1663  loss_mask_dn_3: 0.03794  loss_dice_dn_3: 0.3222  loss_bbox_dn_3: 0.03351  loss_giou_dn_3: 0.1625  loss_ce_4: 0.6388  loss_mask_4: 0.05057  loss_dice_4: 0.3719  loss_bbox_4: 0.04216  loss_giou_4: 0.2146  loss_ce_dn_4: 0.1571  loss_mask_dn_4: 0.03653  loss_dice_dn_4: 0.3213  loss_bbox_dn_4: 0.02967  loss_giou_dn_4: 0.1513  loss_ce_5: 0.6163  loss_mask_5: 0.04077  loss_dice_5: 0.3728  loss_bbox_5: 0.04123  loss_giou_5: 0.2137  loss_ce_dn_5: 0.1429  loss_mask_dn_5: 0.03674  loss_dice_dn_5: 0.315  loss_bbox_dn_5: 0.02817  loss_giou_dn_5: 0.1486  loss_ce_6: 0.5932  loss_mask_6: 0.03922  loss_dice_6: 0.362  loss_bbox_6: 0.04159  loss_giou_6: 0.1978  loss_ce_dn_6: 0.136  loss_mask_dn_6: 0.03804  loss_dice_dn_6: 0.3076  loss_bbox_dn_6: 0.02777  loss_giou_dn_6: 0.146  loss_ce_7: 0.5913  loss_mask_7: 0.04342  loss_dice_7: 0.3386  loss_bbox_7: 0.04157  loss_giou_7: 0.2036  loss_ce_dn_7: 0.136  loss_mask_dn_7: 0.03808  loss_dice_dn_7: 0.3004  loss_bbox_dn_7: 0.02798  loss_giou_dn_7: 0.1471  loss_ce_8: 0.5648  loss_mask_8: 0.04103  loss_dice_8: 0.3513  loss_bbox_8: 0.04421  loss_giou_8: 0.1993  loss_ce_dn_8: 0.1312  loss_mask_dn_8: 0.03875  loss_dice_dn_8: 0.3104  loss_bbox_dn_8: 0.02727  loss_giou_dn_8: 0.1432  loss_ce_interm: 0.9469  loss_mask_interm: 0.04188  loss_dice_interm: 0.3533  loss_bbox_interm: 0.07528  loss_giou_interm: 0.2704    time: 0.7899  last_time: 0.7789  data_time: 0.0142  last_data_time: 0.0073   lr: 1e-05  max_mem: 20850M\n",
      "\u001b[32m[07/13 11:23:57 d2.utils.events]: \u001b[0m eta: 0:19:04  iter: 939  total_loss: 33.57  loss_ce: 0.5925  loss_mask: 0.07349  loss_dice: 0.4111  loss_bbox: 0.05058  loss_giou: 0.2782  loss_ce_dn: 0.1176  loss_mask_dn: 0.05043  loss_dice_dn: 0.4045  loss_bbox_dn: 0.03456  loss_giou_dn: 0.1739  loss_ce_0: 1.006  loss_mask_0: 0.07198  loss_dice_0: 0.536  loss_bbox_0: 0.06904  loss_giou_0: 0.3829  loss_ce_dn_0: 1.997  loss_mask_dn_0: 0.6753  loss_dice_dn_0: 2.802  loss_bbox_dn_0: 0.3616  loss_giou_dn_0: 0.8513  loss_ce_1: 0.9496  loss_mask_1: 0.0892  loss_dice_1: 0.5014  loss_bbox_1: 0.05712  loss_giou_1: 0.3049  loss_ce_dn_1: 0.2436  loss_mask_dn_1: 0.05456  loss_dice_dn_1: 0.4668  loss_bbox_dn_1: 0.08551  loss_giou_dn_1: 0.2964  loss_ce_2: 0.7673  loss_mask_2: 0.08472  loss_dice_2: 0.4879  loss_bbox_2: 0.05827  loss_giou_2: 0.2879  loss_ce_dn_2: 0.159  loss_mask_dn_2: 0.05112  loss_dice_dn_2: 0.4323  loss_bbox_dn_2: 0.05326  loss_giou_dn_2: 0.2218  loss_ce_3: 0.6456  loss_mask_3: 0.08226  loss_dice_3: 0.49  loss_bbox_3: 0.05962  loss_giou_3: 0.2788  loss_ce_dn_3: 0.1317  loss_mask_dn_3: 0.04884  loss_dice_dn_3: 0.3939  loss_bbox_dn_3: 0.04264  loss_giou_dn_3: 0.2007  loss_ce_4: 0.6285  loss_mask_4: 0.0744  loss_dice_4: 0.4962  loss_bbox_4: 0.05083  loss_giou_4: 0.2839  loss_ce_dn_4: 0.1184  loss_mask_dn_4: 0.04831  loss_dice_dn_4: 0.4127  loss_bbox_dn_4: 0.03833  loss_giou_dn_4: 0.1888  loss_ce_5: 0.5896  loss_mask_5: 0.07375  loss_dice_5: 0.4909  loss_bbox_5: 0.06743  loss_giou_5: 0.2675  loss_ce_dn_5: 0.1115  loss_mask_dn_5: 0.0485  loss_dice_dn_5: 0.3895  loss_bbox_dn_5: 0.0373  loss_giou_dn_5: 0.1834  loss_ce_6: 0.5583  loss_mask_6: 0.08212  loss_dice_6: 0.4762  loss_bbox_6: 0.06433  loss_giou_6: 0.2759  loss_ce_dn_6: 0.1118  loss_mask_dn_6: 0.04959  loss_dice_dn_6: 0.4258  loss_bbox_dn_6: 0.03538  loss_giou_dn_6: 0.1786  loss_ce_7: 0.5962  loss_mask_7: 0.07769  loss_dice_7: 0.3806  loss_bbox_7: 0.06333  loss_giou_7: 0.2826  loss_ce_dn_7: 0.1294  loss_mask_dn_7: 0.04941  loss_dice_dn_7: 0.4059  loss_bbox_dn_7: 0.03532  loss_giou_dn_7: 0.1782  loss_ce_8: 0.5876  loss_mask_8: 0.07732  loss_dice_8: 0.4746  loss_bbox_8: 0.0643  loss_giou_8: 0.2679  loss_ce_dn_8: 0.1219  loss_mask_dn_8: 0.04953  loss_dice_dn_8: 0.4089  loss_bbox_dn_8: 0.03438  loss_giou_dn_8: 0.1743  loss_ce_interm: 1.011  loss_mask_interm: 0.07273  loss_dice_interm: 0.47  loss_bbox_interm: 0.09619  loss_giou_interm: 0.3845    time: 0.7901  last_time: 0.8408  data_time: 0.0179  last_data_time: 0.0677   lr: 1e-05  max_mem: 20850M\n",
      "\u001b[32m[07/13 11:24:13 d2.utils.events]: \u001b[0m eta: 0:18:48  iter: 959  total_loss: 29.52  loss_ce: 0.6043  loss_mask: 0.04661  loss_dice: 0.3751  loss_bbox: 0.03776  loss_giou: 0.1743  loss_ce_dn: 0.1323  loss_mask_dn: 0.0458  loss_dice_dn: 0.3648  loss_bbox_dn: 0.03127  loss_giou_dn: 0.1555  loss_ce_0: 0.972  loss_mask_0: 0.0503  loss_dice_0: 0.3703  loss_bbox_0: 0.0731  loss_giou_0: 0.3095  loss_ce_dn_0: 1.887  loss_mask_dn_0: 0.5596  loss_dice_dn_0: 2.804  loss_bbox_dn_0: 0.3344  loss_giou_dn_0: 0.8561  loss_ce_1: 0.8983  loss_mask_1: 0.04918  loss_dice_1: 0.3962  loss_bbox_1: 0.06395  loss_giou_1: 0.204  loss_ce_dn_1: 0.2615  loss_mask_dn_1: 0.04918  loss_dice_dn_1: 0.3809  loss_bbox_dn_1: 0.08017  loss_giou_dn_1: 0.2691  loss_ce_2: 0.728  loss_mask_2: 0.04663  loss_dice_2: 0.3988  loss_bbox_2: 0.05458  loss_giou_2: 0.2002  loss_ce_dn_2: 0.1904  loss_mask_dn_2: 0.04562  loss_dice_dn_2: 0.3725  loss_bbox_dn_2: 0.04369  loss_giou_dn_2: 0.1964  loss_ce_3: 0.675  loss_mask_3: 0.04762  loss_dice_3: 0.3714  loss_bbox_3: 0.04904  loss_giou_3: 0.1908  loss_ce_dn_3: 0.1603  loss_mask_dn_3: 0.04594  loss_dice_dn_3: 0.3663  loss_bbox_dn_3: 0.03763  loss_giou_dn_3: 0.1727  loss_ce_4: 0.6255  loss_mask_4: 0.04426  loss_dice_4: 0.3689  loss_bbox_4: 0.05502  loss_giou_4: 0.1803  loss_ce_dn_4: 0.1538  loss_mask_dn_4: 0.04246  loss_dice_dn_4: 0.3603  loss_bbox_dn_4: 0.03598  loss_giou_dn_4: 0.162  loss_ce_5: 0.6417  loss_mask_5: 0.04326  loss_dice_5: 0.3676  loss_bbox_5: 0.05326  loss_giou_5: 0.1948  loss_ce_dn_5: 0.1395  loss_mask_dn_5: 0.0428  loss_dice_dn_5: 0.3624  loss_bbox_dn_5: 0.03447  loss_giou_dn_5: 0.16  loss_ce_6: 0.6264  loss_mask_6: 0.04774  loss_dice_6: 0.3781  loss_bbox_6: 0.05023  loss_giou_6: 0.1814  loss_ce_dn_6: 0.1342  loss_mask_dn_6: 0.04358  loss_dice_dn_6: 0.3664  loss_bbox_dn_6: 0.03299  loss_giou_dn_6: 0.1582  loss_ce_7: 0.5835  loss_mask_7: 0.04436  loss_dice_7: 0.3622  loss_bbox_7: 0.04872  loss_giou_7: 0.1789  loss_ce_dn_7: 0.1338  loss_mask_dn_7: 0.04379  loss_dice_dn_7: 0.374  loss_bbox_dn_7: 0.03274  loss_giou_dn_7: 0.1588  loss_ce_8: 0.6193  loss_mask_8: 0.04487  loss_dice_8: 0.359  loss_bbox_8: 0.04044  loss_giou_8: 0.1737  loss_ce_dn_8: 0.1295  loss_mask_dn_8: 0.04474  loss_dice_dn_8: 0.3708  loss_bbox_dn_8: 0.03167  loss_giou_dn_8: 0.1549  loss_ce_interm: 0.9516  loss_mask_interm: 0.05285  loss_dice_interm: 0.3898  loss_bbox_interm: 0.08045  loss_giou_interm: 0.3067    time: 0.7901  last_time: 0.7836  data_time: 0.0139  last_data_time: 0.0083   lr: 1e-05  max_mem: 20850M\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[07/13 11:24:29 d2.utils.events]: \u001b[0m eta: 0:18:33  iter: 979  total_loss: 30.74  loss_ce: 0.5564  loss_mask: 0.03582  loss_dice: 0.3474  loss_bbox: 0.03588  loss_giou: 0.1784  loss_ce_dn: 0.1098  loss_mask_dn: 0.03692  loss_dice_dn: 0.342  loss_bbox_dn: 0.03092  loss_giou_dn: 0.1565  loss_ce_0: 0.984  loss_mask_0: 0.03714  loss_dice_0: 0.3822  loss_bbox_0: 0.05312  loss_giou_0: 0.324  loss_ce_dn_0: 1.825  loss_mask_dn_0: 0.4763  loss_dice_dn_0: 2.856  loss_bbox_dn_0: 0.3658  loss_giou_dn_0: 0.8602  loss_ce_1: 0.9022  loss_mask_1: 0.04006  loss_dice_1: 0.3731  loss_bbox_1: 0.04312  loss_giou_1: 0.2411  loss_ce_dn_1: 0.2582  loss_mask_dn_1: 0.043  loss_dice_dn_1: 0.3753  loss_bbox_dn_1: 0.07934  loss_giou_dn_1: 0.281  loss_ce_2: 0.7411  loss_mask_2: 0.04362  loss_dice_2: 0.3788  loss_bbox_2: 0.03888  loss_giou_2: 0.2109  loss_ce_dn_2: 0.1732  loss_mask_dn_2: 0.04153  loss_dice_dn_2: 0.3521  loss_bbox_dn_2: 0.04756  loss_giou_dn_2: 0.2033  loss_ce_3: 0.6238  loss_mask_3: 0.04092  loss_dice_3: 0.348  loss_bbox_3: 0.03923  loss_giou_3: 0.207  loss_ce_dn_3: 0.1381  loss_mask_dn_3: 0.03951  loss_dice_dn_3: 0.3669  loss_bbox_dn_3: 0.03932  loss_giou_dn_3: 0.1767  loss_ce_4: 0.5916  loss_mask_4: 0.03844  loss_dice_4: 0.3256  loss_bbox_4: 0.03754  loss_giou_4: 0.1874  loss_ce_dn_4: 0.1391  loss_mask_dn_4: 0.03967  loss_dice_dn_4: 0.3609  loss_bbox_dn_4: 0.03501  loss_giou_dn_4: 0.1664  loss_ce_5: 0.5768  loss_mask_5: 0.03557  loss_dice_5: 0.3293  loss_bbox_5: 0.03839  loss_giou_5: 0.189  loss_ce_dn_5: 0.1231  loss_mask_dn_5: 0.03968  loss_dice_dn_5: 0.3373  loss_bbox_dn_5: 0.03263  loss_giou_dn_5: 0.1632  loss_ce_6: 0.5655  loss_mask_6: 0.03729  loss_dice_6: 0.3559  loss_bbox_6: 0.03844  loss_giou_6: 0.1919  loss_ce_dn_6: 0.1242  loss_mask_dn_6: 0.03709  loss_dice_dn_6: 0.3363  loss_bbox_dn_6: 0.03023  loss_giou_dn_6: 0.1579  loss_ce_7: 0.554  loss_mask_7: 0.03446  loss_dice_7: 0.3482  loss_bbox_7: 0.03795  loss_giou_7: 0.1809  loss_ce_dn_7: 0.1163  loss_mask_dn_7: 0.03776  loss_dice_dn_7: 0.3468  loss_bbox_dn_7: 0.03036  loss_giou_dn_7: 0.1586  loss_ce_8: 0.5394  loss_mask_8: 0.03431  loss_dice_8: 0.3273  loss_bbox_8: 0.03739  loss_giou_8: 0.192  loss_ce_dn_8: 0.1136  loss_mask_dn_8: 0.03753  loss_dice_dn_8: 0.3503  loss_bbox_dn_8: 0.03071  loss_giou_dn_8: 0.1558  loss_ce_interm: 0.9496  loss_mask_interm: 0.04361  loss_dice_interm: 0.4105  loss_bbox_interm: 0.07881  loss_giou_interm: 0.3153    time: 0.7901  last_time: 0.7902  data_time: 0.0153  last_data_time: 0.0083   lr: 1e-05  max_mem: 20850M\n",
      "\u001b[32m[07/13 11:24:47 d2.data.build]: \u001b[0mDistribution of instances among all 39 categories:\n",
      "\u001b[36m|   category    | #instances   |   category    | #instances   |   category    | #instances   |\n",
      "|:-------------:|:-------------|:-------------:|:-------------|:-------------:|:-------------|\n",
      "|     apple     | 13           |    banana     | 11           |   baseball    | 10           |\n",
      "|    cereals    | 34           |    cheezit    | 10           | chocolate_j.. | 13           |\n",
      "|   cleanser    | 12           | coffee_grou.. | 8            |     cola      | 15           |\n",
      "|  couch_table  | 5            |     dice      | 7            |     fork      | 9            |\n",
      "|   iced_tea    | 16           |  juice_pack   | 18           |     knife     | 12           |\n",
      "|     lemon     | 11           |     milk      | 33           |    mustard    | 10           |\n",
      "|    orange     | 8            | orange_juice  | 18           |     peach     | 5            |\n",
      "|     pear      | 13           |     plum      | 12           |   pringles    | 10           |\n",
      "|   red_wine    | 10           |  rubiks_cube  | 12           |     shelf     | 22           |\n",
      "|  shelf_door   | 12           |  soccer_ball  | 10           |     spam      | 12           |\n",
      "|    sponge     | 15           |     spoon     | 11           |  strawberry   | 8            |\n",
      "| strawberry_.. | 10           |     sugar     | 9            |  tennis_ball  | 5            |\n",
      "|  tomato_soup  | 14           | tropical_ju.. | 28           |     tuna      | 13           |\n",
      "|               |              |               |              |               |              |\n",
      "|     total     | 504          |               |              |               |              |\u001b[0m\n",
      "\u001b[32m[07/13 11:24:47 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
      "\u001b[32m[07/13 11:24:47 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001b[32m[07/13 11:24:47 d2.data.common]: \u001b[0mSerializing 58 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[07/13 11:24:47 d2.data.common]: \u001b[0mSerialized dataset takes 0.14 MiB\n",
      "\u001b[32m[07/13 11:24:47 d2.evaluation.coco_evaluation]: \u001b[0mTrying to convert 'fiftyone_valid' to COCO format ...\n",
      "\u001b[32m[07/13 11:24:47 d2.data.datasets.coco]: \u001b[0mConverting annotations of dataset 'fiftyone_valid' to COCO format ...)\n",
      "\u001b[32m[07/13 11:24:48 d2.data.datasets.coco]: \u001b[0mConverting dataset dicts into COCO format\n",
      "\u001b[32m[07/13 11:24:48 d2.data.datasets.coco]: \u001b[0mConversion finished, #images: 58, #annotations: 504\n",
      "\u001b[32m[07/13 11:24:48 d2.data.datasets.coco]: \u001b[0mCaching COCO format annotations at './output/inference/fiftyone_valid_coco_format.json' ...\n",
      "\u001b[32m[07/13 11:24:48 d2.evaluation.evaluator]: \u001b[0mStart inference on 58 batches\n",
      "\u001b[32m[07/13 11:24:51 d2.evaluation.evaluator]: \u001b[0mInference done 11/58. Dataloading: 0.0010 s/iter. Inference: 0.1712 s/iter. Eval: 0.0973 s/iter. Total: 0.2696 s/iter. ETA=0:00:12\n",
      "\u001b[32m[07/13 11:24:57 d2.evaluation.evaluator]: \u001b[0mInference done 30/58. Dataloading: 0.0011 s/iter. Inference: 0.1767 s/iter. Eval: 0.0972 s/iter. Total: 0.2751 s/iter. ETA=0:00:07\n",
      "\u001b[32m[07/13 11:25:02 d2.evaluation.evaluator]: \u001b[0mInference done 49/58. Dataloading: 0.0011 s/iter. Inference: 0.1742 s/iter. Eval: 0.0971 s/iter. Total: 0.2724 s/iter. ETA=0:00:02\n",
      "\u001b[32m[07/13 11:25:04 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:14.485853 (0.273318 s / iter per device, on 1 devices)\n",
      "\u001b[32m[07/13 11:25:04 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:09 (0.173544 s / iter per device, on 1 devices)\n",
      "\u001b[32m[07/13 11:25:04 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
      "\u001b[32m[07/13 11:25:04 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/coco_instances_results.json\n",
      "\u001b[32m[07/13 11:25:04 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "\u001b[32m[07/13 11:25:04 d2.evaluation.fast_eval_api]: \u001b[0mEvaluate annotation type *bbox*\n",
      "\u001b[32m[07/13 11:25:04 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.evaluate() finished in 0.06 seconds.\n",
      "\u001b[32m[07/13 11:25:04 d2.evaluation.fast_eval_api]: \u001b[0mAccumulating evaluation results...\n",
      "\u001b[32m[07/13 11:25:05 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.accumulate() finished in 0.07 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.800\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.871\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.847\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.594\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.897\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.902\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.796\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.869\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.869\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.693\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.934\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.916\n",
      "\u001b[32m[07/13 11:25:05 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
      "|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl   |\n",
      "|:------:|:------:|:------:|:------:|:------:|:------:|\n",
      "| 79.988 | 87.076 | 84.681 | 59.362 | 89.699 | 90.210 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[07/13 11:25:05 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
      "| category         | AP     | category       | AP     | category        | AP     |\n",
      "|:-----------------|:-------|:---------------|:-------|:----------------|:-------|\n",
      "| apple            | 85.561 | banana         | 94.990 | baseball        | 71.931 |\n",
      "| cereals          | 82.016 | cheezit        | 84.017 | chocolate_jello | 76.124 |\n",
      "| cleanser         | 78.422 | coffee_grounds | 69.196 | cola            | 87.309 |\n",
      "| couch_table      | 93.069 | dice           | 70.728 | fork            | 82.589 |\n",
      "| iced_tea         | 99.233 | juice_pack     | 85.815 | knife           | 75.195 |\n",
      "| lemon            | 88.112 | milk           | 82.198 | mustard         | 86.694 |\n",
      "| orange           | 65.759 | orange_juice   | 98.216 | peach           | 76.609 |\n",
      "| pear             | 85.901 | plum           | 89.311 | pringles        | 94.137 |\n",
      "| red_wine         | 90.089 | rubiks_cube    | 84.841 | shelf           | 96.179 |\n",
      "| shelf_door       | 55.789 | soccer_ball    | 87.294 | spam            | 77.628 |\n",
      "| sponge           | 73.068 | spoon          | 50.222 | strawberry      | 85.375 |\n",
      "| strawberry_jello | 69.448 | sugar          | 74.939 | tennis_ball     | 54.399 |\n",
      "| tomato_soup      | 52.894 | tropical_juice | 85.194 | tuna            | 79.044 |\n",
      "Loading and preparing results...\n",
      "DONE (t=0.03s)\n",
      "creating index...\n",
      "index created!\n",
      "\u001b[32m[07/13 11:25:05 d2.evaluation.fast_eval_api]: \u001b[0mEvaluate annotation type *segm*\n",
      "\u001b[32m[07/13 11:25:05 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.evaluate() finished in 0.08 seconds.\n",
      "\u001b[32m[07/13 11:25:05 d2.evaluation.fast_eval_api]: \u001b[0mAccumulating evaluation results...\n",
      "\u001b[32m[07/13 11:25:05 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.accumulate() finished in 0.06 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.760\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.862\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.830\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.479\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.887\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.873\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.765\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.835\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.835\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.618\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.914\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.909\n",
      "\u001b[32m[07/13 11:25:05 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for segm: \n",
      "|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl   |\n",
      "|:------:|:------:|:------:|:------:|:------:|:------:|\n",
      "| 76.025 | 86.236 | 83.013 | 47.932 | 88.732 | 87.313 |\n",
      "\u001b[32m[07/13 11:25:05 d2.evaluation.coco_evaluation]: \u001b[0mPer-category segm AP: \n",
      "| category         | AP     | category       | AP     | category        | AP     |\n",
      "|:-----------------|:-------|:---------------|:-------|:----------------|:-------|\n",
      "| apple            | 83.677 | banana         | 91.452 | baseball        | 72.574 |\n",
      "| cereals          | 79.716 | cheezit        | 84.017 | chocolate_jello | 76.191 |\n",
      "| cleanser         | 70.102 | coffee_grounds | 77.941 | cola            | 77.957 |\n",
      "| couch_table      | 46.297 | dice           | 58.132 | fork            | 71.124 |\n",
      "| iced_tea         | 97.470 | juice_pack     | 82.495 | knife           | 70.899 |\n",
      "| lemon            | 88.997 | milk           | 81.698 | mustard         | 90.767 |\n",
      "| orange           | 65.759 | orange_juice   | 98.905 | peach           | 73.965 |\n",
      "| pear             | 86.367 | plum           | 85.050 | pringles        | 86.764 |\n",
      "| red_wine         | 87.516 | rubiks_cube    | 85.104 | shelf           | 78.679 |\n",
      "| shelf_door       | 61.390 | soccer_ball    | 87.064 | spam            | 78.040 |\n",
      "| sponge           | 71.212 | spoon          | 41.577 | strawberry      | 79.225 |\n",
      "| strawberry_jello | 65.332 | sugar          | 74.296 | tennis_ball     | 45.946 |\n",
      "| tomato_soup      | 49.128 | tropical_juice | 83.165 | tuna            | 78.994 |\n",
      "\u001b[32m[07/13 11:25:05 d2.engine.defaults]: \u001b[0mEvaluation results for fiftyone_valid in csv format:\n",
      "\u001b[32m[07/13 11:25:05 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
      "\u001b[32m[07/13 11:25:05 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
      "\u001b[32m[07/13 11:25:05 d2.evaluation.testing]: \u001b[0mcopypaste: 79.9882,87.0758,84.6806,59.3618,89.6990,90.2098\n",
      "\u001b[32m[07/13 11:25:05 d2.evaluation.testing]: \u001b[0mcopypaste: Task: segm\n",
      "\u001b[32m[07/13 11:25:05 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
      "\u001b[32m[07/13 11:25:05 d2.evaluation.testing]: \u001b[0mcopypaste: 76.0251,86.2359,83.0134,47.9317,88.7321,87.3134\n",
      "\u001b[32m[07/13 11:25:05 d2.utils.events]: \u001b[0m eta: 0:18:17  iter: 999  total_loss: 35.17  loss_ce: 0.6552  loss_mask: 0.03901  loss_dice: 0.4692  loss_bbox: 0.05343  loss_giou: 0.272  loss_ce_dn: 0.1298  loss_mask_dn: 0.03799  loss_dice_dn: 0.4717  loss_bbox_dn: 0.02913  loss_giou_dn: 0.2073  loss_ce_0: 1.084  loss_mask_0: 0.04491  loss_dice_0: 0.5542  loss_bbox_0: 0.06728  loss_giou_0: 0.4822  loss_ce_dn_0: 1.878  loss_mask_dn_0: 0.3918  loss_dice_dn_0: 3.007  loss_bbox_dn_0: 0.3009  loss_giou_dn_0: 0.8627  loss_ce_1: 0.9773  loss_mask_1: 0.04589  loss_dice_1: 0.468  loss_bbox_1: 0.05287  loss_giou_1: 0.334  loss_ce_dn_1: 0.3084  loss_mask_dn_1: 0.04294  loss_dice_dn_1: 0.5224  loss_bbox_dn_1: 0.06405  loss_giou_dn_1: 0.3269  loss_ce_2: 0.8779  loss_mask_2: 0.04126  loss_dice_2: 0.5188  loss_bbox_2: 0.05694  loss_giou_2: 0.3009  loss_ce_dn_2: 0.2153  loss_mask_dn_2: 0.03973  loss_dice_dn_2: 0.4765  loss_bbox_dn_2: 0.04128  loss_giou_dn_2: 0.2468  loss_ce_3: 0.7655  loss_mask_3: 0.0397  loss_dice_3: 0.4636  loss_bbox_3: 0.05711  loss_giou_3: 0.2903  loss_ce_dn_3: 0.1836  loss_mask_dn_3: 0.03921  loss_dice_dn_3: 0.4662  loss_bbox_dn_3: 0.03406  loss_giou_dn_3: 0.2269  loss_ce_4: 0.754  loss_mask_4: 0.03928  loss_dice_4: 0.4636  loss_bbox_4: 0.05265  loss_giou_4: 0.27  loss_ce_dn_4: 0.161  loss_mask_dn_4: 0.03754  loss_dice_dn_4: 0.4658  loss_bbox_dn_4: 0.03185  loss_giou_dn_4: 0.2187  loss_ce_5: 0.7149  loss_mask_5: 0.03872  loss_dice_5: 0.4804  loss_bbox_5: 0.05665  loss_giou_5: 0.2712  loss_ce_dn_5: 0.1504  loss_mask_dn_5: 0.03805  loss_dice_dn_5: 0.4664  loss_bbox_dn_5: 0.03038  loss_giou_dn_5: 0.2111  loss_ce_6: 0.7424  loss_mask_6: 0.04096  loss_dice_6: 0.4914  loss_bbox_6: 0.04487  loss_giou_6: 0.2634  loss_ce_dn_6: 0.1402  loss_mask_dn_6: 0.03832  loss_dice_dn_6: 0.4456  loss_bbox_dn_6: 0.02965  loss_giou_dn_6: 0.2094  loss_ce_7: 0.681  loss_mask_7: 0.03991  loss_dice_7: 0.4671  loss_bbox_7: 0.05468  loss_giou_7: 0.2632  loss_ce_dn_7: 0.1357  loss_mask_dn_7: 0.03834  loss_dice_dn_7: 0.4619  loss_bbox_dn_7: 0.02976  loss_giou_dn_7: 0.211  loss_ce_8: 0.6991  loss_mask_8: 0.04068  loss_dice_8: 0.5355  loss_bbox_8: 0.05415  loss_giou_8: 0.2682  loss_ce_dn_8: 0.1324  loss_mask_dn_8: 0.03828  loss_dice_dn_8: 0.4576  loss_bbox_dn_8: 0.0288  loss_giou_dn_8: 0.2063  loss_ce_interm: 1.091  loss_mask_interm: 0.04324  loss_dice_interm: 0.5842  loss_bbox_interm: 0.07372  loss_giou_interm: 0.3907    time: 0.7902  last_time: 0.8202  data_time: 0.0214  last_data_time: 0.0446   lr: 1e-05  max_mem: 20850M\n",
      "\u001b[32m[07/13 11:25:21 d2.utils.events]: \u001b[0m eta: 0:18:02  iter: 1019  total_loss: 32.77  loss_ce: 0.6233  loss_mask: 0.05565  loss_dice: 0.4421  loss_bbox: 0.05188  loss_giou: 0.2492  loss_ce_dn: 0.1482  loss_mask_dn: 0.04078  loss_dice_dn: 0.408  loss_bbox_dn: 0.03006  loss_giou_dn: 0.1692  loss_ce_0: 0.9918  loss_mask_0: 0.06128  loss_dice_0: 0.533  loss_bbox_0: 0.07694  loss_giou_0: 0.4288  loss_ce_dn_0: 1.923  loss_mask_dn_0: 0.4541  loss_dice_dn_0: 2.888  loss_bbox_dn_0: 0.3183  loss_giou_dn_0: 0.8551  loss_ce_1: 0.8572  loss_mask_1: 0.06372  loss_dice_1: 0.5704  loss_bbox_1: 0.0677  loss_giou_1: 0.2813  loss_ce_dn_1: 0.297  loss_mask_dn_1: 0.0433  loss_dice_dn_1: 0.4318  loss_bbox_dn_1: 0.0681  loss_giou_dn_1: 0.2974  loss_ce_2: 0.7585  loss_mask_2: 0.06333  loss_dice_2: 0.5406  loss_bbox_2: 0.06109  loss_giou_2: 0.2665  loss_ce_dn_2: 0.2148  loss_mask_dn_2: 0.04496  loss_dice_dn_2: 0.3899  loss_bbox_dn_2: 0.04282  loss_giou_dn_2: 0.2203  loss_ce_3: 0.6985  loss_mask_3: 0.06222  loss_dice_3: 0.4345  loss_bbox_3: 0.05812  loss_giou_3: 0.25  loss_ce_dn_3: 0.1829  loss_mask_dn_3: 0.04205  loss_dice_dn_3: 0.3992  loss_bbox_dn_3: 0.03404  loss_giou_dn_3: 0.1885  loss_ce_4: 0.6153  loss_mask_4: 0.06265  loss_dice_4: 0.5213  loss_bbox_4: 0.05697  loss_giou_4: 0.2502  loss_ce_dn_4: 0.1683  loss_mask_dn_4: 0.04137  loss_dice_dn_4: 0.4081  loss_bbox_dn_4: 0.03173  loss_giou_dn_4: 0.1771  loss_ce_5: 0.6351  loss_mask_5: 0.05641  loss_dice_5: 0.5045  loss_bbox_5: 0.05516  loss_giou_5: 0.2468  loss_ce_dn_5: 0.1621  loss_mask_dn_5: 0.04263  loss_dice_dn_5: 0.4015  loss_bbox_dn_5: 0.03123  loss_giou_dn_5: 0.1713  loss_ce_6: 0.6205  loss_mask_6: 0.06495  loss_dice_6: 0.4393  loss_bbox_6: 0.05578  loss_giou_6: 0.2494  loss_ce_dn_6: 0.155  loss_mask_dn_6: 0.04148  loss_dice_dn_6: 0.4101  loss_bbox_dn_6: 0.03035  loss_giou_dn_6: 0.1705  loss_ce_7: 0.643  loss_mask_7: 0.05796  loss_dice_7: 0.4697  loss_bbox_7: 0.05168  loss_giou_7: 0.2465  loss_ce_dn_7: 0.1489  loss_mask_dn_7: 0.04068  loss_dice_dn_7: 0.4198  loss_bbox_dn_7: 0.03014  loss_giou_dn_7: 0.1698  loss_ce_8: 0.6195  loss_mask_8: 0.05547  loss_dice_8: 0.4619  loss_bbox_8: 0.05483  loss_giou_8: 0.2451  loss_ce_dn_8: 0.1501  loss_mask_dn_8: 0.04141  loss_dice_dn_8: 0.4184  loss_bbox_dn_8: 0.03002  loss_giou_dn_8: 0.1697  loss_ce_interm: 1.016  loss_mask_interm: 0.06207  loss_dice_interm: 0.5153  loss_bbox_interm: 0.08406  loss_giou_interm: 0.3806    time: 0.7902  last_time: 0.7820  data_time: 0.0137  last_data_time: 0.0085   lr: 1e-05  max_mem: 20850M\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[07/13 11:25:36 d2.utils.events]: \u001b[0m eta: 0:17:46  iter: 1039  total_loss: 25.17  loss_ce: 0.4579  loss_mask: 0.05114  loss_dice: 0.2886  loss_bbox: 0.03645  loss_giou: 0.1312  loss_ce_dn: 0.1208  loss_mask_dn: 0.05071  loss_dice_dn: 0.2785  loss_bbox_dn: 0.03127  loss_giou_dn: 0.1219  loss_ce_0: 0.8757  loss_mask_0: 0.054  loss_dice_0: 0.2924  loss_bbox_0: 0.04873  loss_giou_0: 0.2318  loss_ce_dn_0: 1.829  loss_mask_dn_0: 0.5828  loss_dice_dn_0: 2.76  loss_bbox_dn_0: 0.3797  loss_giou_dn_0: 0.8492  loss_ce_1: 0.7379  loss_mask_1: 0.05165  loss_dice_1: 0.2887  loss_bbox_1: 0.04237  loss_giou_1: 0.1667  loss_ce_dn_1: 0.2558  loss_mask_dn_1: 0.06421  loss_dice_dn_1: 0.3173  loss_bbox_dn_1: 0.07361  loss_giou_dn_1: 0.2448  loss_ce_2: 0.6163  loss_mask_2: 0.05172  loss_dice_2: 0.2972  loss_bbox_2: 0.03949  loss_giou_2: 0.1461  loss_ce_dn_2: 0.1833  loss_mask_dn_2: 0.05836  loss_dice_dn_2: 0.2963  loss_bbox_dn_2: 0.04736  loss_giou_dn_2: 0.16  loss_ce_3: 0.5413  loss_mask_3: 0.05009  loss_dice_3: 0.2985  loss_bbox_3: 0.03816  loss_giou_3: 0.1417  loss_ce_dn_3: 0.1582  loss_mask_dn_3: 0.05457  loss_dice_dn_3: 0.2814  loss_bbox_dn_3: 0.03943  loss_giou_dn_3: 0.1403  loss_ce_4: 0.5209  loss_mask_4: 0.05043  loss_dice_4: 0.2849  loss_bbox_4: 0.03616  loss_giou_4: 0.1334  loss_ce_dn_4: 0.1488  loss_mask_dn_4: 0.05442  loss_dice_dn_4: 0.2778  loss_bbox_dn_4: 0.03618  loss_giou_dn_4: 0.1299  loss_ce_5: 0.4798  loss_mask_5: 0.0507  loss_dice_5: 0.2758  loss_bbox_5: 0.03674  loss_giou_5: 0.1324  loss_ce_dn_5: 0.1312  loss_mask_dn_5: 0.05219  loss_dice_dn_5: 0.2837  loss_bbox_dn_5: 0.03461  loss_giou_dn_5: 0.125  loss_ce_6: 0.46  loss_mask_6: 0.04901  loss_dice_6: 0.2741  loss_bbox_6: 0.03627  loss_giou_6: 0.1305  loss_ce_dn_6: 0.125  loss_mask_dn_6: 0.05001  loss_dice_dn_6: 0.2802  loss_bbox_dn_6: 0.03217  loss_giou_dn_6: 0.1228  loss_ce_7: 0.4556  loss_mask_7: 0.04813  loss_dice_7: 0.2932  loss_bbox_7: 0.03651  loss_giou_7: 0.1304  loss_ce_dn_7: 0.1182  loss_mask_dn_7: 0.05167  loss_dice_dn_7: 0.2743  loss_bbox_dn_7: 0.03182  loss_giou_dn_7: 0.1216  loss_ce_8: 0.4618  loss_mask_8: 0.05215  loss_dice_8: 0.2664  loss_bbox_8: 0.03632  loss_giou_8: 0.1302  loss_ce_dn_8: 0.1176  loss_mask_dn_8: 0.05069  loss_dice_dn_8: 0.2761  loss_bbox_dn_8: 0.03113  loss_giou_dn_8: 0.1215  loss_ce_interm: 0.8688  loss_mask_interm: 0.05452  loss_dice_interm: 0.2994  loss_bbox_interm: 0.07763  loss_giou_interm: 0.2424    time: 0.7902  last_time: 0.7782  data_time: 0.0113  last_data_time: 0.0087   lr: 1e-05  max_mem: 20850M\n",
      "\u001b[32m[07/13 11:25:52 d2.utils.events]: \u001b[0m eta: 0:17:31  iter: 1059  total_loss: 31.1  loss_ce: 0.6366  loss_mask: 0.04678  loss_dice: 0.4335  loss_bbox: 0.04619  loss_giou: 0.2645  loss_ce_dn: 0.1757  loss_mask_dn: 0.03938  loss_dice_dn: 0.3725  loss_bbox_dn: 0.0318  loss_giou_dn: 0.1732  loss_ce_0: 1.079  loss_mask_0: 0.04883  loss_dice_0: 0.4515  loss_bbox_0: 0.06824  loss_giou_0: 0.3884  loss_ce_dn_0: 2.008  loss_mask_dn_0: 0.458  loss_dice_dn_0: 3.128  loss_bbox_dn_0: 0.3397  loss_giou_dn_0: 0.8541  loss_ce_1: 1.008  loss_mask_1: 0.04798  loss_dice_1: 0.5003  loss_bbox_1: 0.05006  loss_giou_1: 0.3412  loss_ce_dn_1: 0.3068  loss_mask_dn_1: 0.04063  loss_dice_dn_1: 0.4469  loss_bbox_dn_1: 0.07723  loss_giou_dn_1: 0.2909  loss_ce_2: 0.8739  loss_mask_2: 0.04545  loss_dice_2: 0.4741  loss_bbox_2: 0.04995  loss_giou_2: 0.2938  loss_ce_dn_2: 0.2565  loss_mask_dn_2: 0.03905  loss_dice_dn_2: 0.4028  loss_bbox_dn_2: 0.04908  loss_giou_dn_2: 0.2059  loss_ce_3: 0.7405  loss_mask_3: 0.04143  loss_dice_3: 0.4324  loss_bbox_3: 0.05299  loss_giou_3: 0.2544  loss_ce_dn_3: 0.2162  loss_mask_dn_3: 0.0374  loss_dice_dn_3: 0.3867  loss_bbox_dn_3: 0.04153  loss_giou_dn_3: 0.1857  loss_ce_4: 0.6916  loss_mask_4: 0.0461  loss_dice_4: 0.4673  loss_bbox_4: 0.0458  loss_giou_4: 0.2681  loss_ce_dn_4: 0.1944  loss_mask_dn_4: 0.0383  loss_dice_dn_4: 0.3614  loss_bbox_dn_4: 0.03738  loss_giou_dn_4: 0.185  loss_ce_5: 0.689  loss_mask_5: 0.04461  loss_dice_5: 0.4507  loss_bbox_5: 0.04433  loss_giou_5: 0.2713  loss_ce_dn_5: 0.1826  loss_mask_dn_5: 0.03897  loss_dice_dn_5: 0.3598  loss_bbox_dn_5: 0.03504  loss_giou_dn_5: 0.1805  loss_ce_6: 0.681  loss_mask_6: 0.04526  loss_dice_6: 0.4151  loss_bbox_6: 0.04169  loss_giou_6: 0.2742  loss_ce_dn_6: 0.1784  loss_mask_dn_6: 0.03826  loss_dice_dn_6: 0.374  loss_bbox_dn_6: 0.03183  loss_giou_dn_6: 0.178  loss_ce_7: 0.662  loss_mask_7: 0.04721  loss_dice_7: 0.4429  loss_bbox_7: 0.04383  loss_giou_7: 0.2527  loss_ce_dn_7: 0.1701  loss_mask_dn_7: 0.03854  loss_dice_dn_7: 0.3641  loss_bbox_dn_7: 0.03211  loss_giou_dn_7: 0.1764  loss_ce_8: 0.6769  loss_mask_8: 0.04602  loss_dice_8: 0.4612  loss_bbox_8: 0.04614  loss_giou_8: 0.2533  loss_ce_dn_8: 0.1726  loss_mask_dn_8: 0.03924  loss_dice_dn_8: 0.3679  loss_bbox_dn_8: 0.03172  loss_giou_dn_8: 0.1736  loss_ce_interm: 1.065  loss_mask_interm: 0.04594  loss_dice_interm: 0.4464  loss_bbox_interm: 0.07716  loss_giou_interm: 0.3409    time: 0.7902  last_time: 0.7816  data_time: 0.0160  last_data_time: 0.0085   lr: 1e-05  max_mem: 20850M\n",
      "\u001b[32m[07/13 11:26:08 d2.utils.events]: \u001b[0m eta: 0:17:15  iter: 1079  total_loss: 31.98  loss_ce: 0.5766  loss_mask: 0.04502  loss_dice: 0.4077  loss_bbox: 0.05213  loss_giou: 0.2257  loss_ce_dn: 0.1424  loss_mask_dn: 0.04244  loss_dice_dn: 0.4176  loss_bbox_dn: 0.03472  loss_giou_dn: 0.1652  loss_ce_0: 1.026  loss_mask_0: 0.05829  loss_dice_0: 0.4388  loss_bbox_0: 0.08114  loss_giou_0: 0.3177  loss_ce_dn_0: 1.968  loss_mask_dn_0: 0.561  loss_dice_dn_0: 2.867  loss_bbox_dn_0: 0.3607  loss_giou_dn_0: 0.8569  loss_ce_1: 0.9606  loss_mask_1: 0.05571  loss_dice_1: 0.4627  loss_bbox_1: 0.05698  loss_giou_1: 0.2699  loss_ce_dn_1: 0.3106  loss_mask_dn_1: 0.04463  loss_dice_dn_1: 0.4556  loss_bbox_dn_1: 0.08098  loss_giou_dn_1: 0.2902  loss_ce_2: 0.7781  loss_mask_2: 0.05612  loss_dice_2: 0.4112  loss_bbox_2: 0.06554  loss_giou_2: 0.2503  loss_ce_dn_2: 0.2198  loss_mask_dn_2: 0.04315  loss_dice_dn_2: 0.4163  loss_bbox_dn_2: 0.05349  loss_giou_dn_2: 0.206  loss_ce_3: 0.7168  loss_mask_3: 0.04552  loss_dice_3: 0.4449  loss_bbox_3: 0.05544  loss_giou_3: 0.2377  loss_ce_dn_3: 0.1861  loss_mask_dn_3: 0.04214  loss_dice_dn_3: 0.3999  loss_bbox_dn_3: 0.04102  loss_giou_dn_3: 0.1871  loss_ce_4: 0.6617  loss_mask_4: 0.04589  loss_dice_4: 0.4434  loss_bbox_4: 0.05402  loss_giou_4: 0.2293  loss_ce_dn_4: 0.1667  loss_mask_dn_4: 0.04372  loss_dice_dn_4: 0.3753  loss_bbox_dn_4: 0.03888  loss_giou_dn_4: 0.1711  loss_ce_5: 0.6547  loss_mask_5: 0.04242  loss_dice_5: 0.4244  loss_bbox_5: 0.05014  loss_giou_5: 0.2298  loss_ce_dn_5: 0.1606  loss_mask_dn_5: 0.04415  loss_dice_dn_5: 0.3838  loss_bbox_dn_5: 0.03728  loss_giou_dn_5: 0.1672  loss_ce_6: 0.607  loss_mask_6: 0.04467  loss_dice_6: 0.4561  loss_bbox_6: 0.05246  loss_giou_6: 0.2388  loss_ce_dn_6: 0.1491  loss_mask_dn_6: 0.04432  loss_dice_dn_6: 0.4015  loss_bbox_dn_6: 0.03478  loss_giou_dn_6: 0.1666  loss_ce_7: 0.6143  loss_mask_7: 0.04683  loss_dice_7: 0.435  loss_bbox_7: 0.05241  loss_giou_7: 0.2268  loss_ce_dn_7: 0.1462  loss_mask_dn_7: 0.04307  loss_dice_dn_7: 0.4028  loss_bbox_dn_7: 0.03572  loss_giou_dn_7: 0.1665  loss_ce_8: 0.6038  loss_mask_8: 0.04538  loss_dice_8: 0.4354  loss_bbox_8: 0.05202  loss_giou_8: 0.2246  loss_ce_dn_8: 0.1429  loss_mask_dn_8: 0.04188  loss_dice_dn_8: 0.4253  loss_bbox_dn_8: 0.03439  loss_giou_dn_8: 0.165  loss_ce_interm: 1.04  loss_mask_interm: 0.05812  loss_dice_interm: 0.4329  loss_bbox_interm: 0.09714  loss_giou_interm: 0.3255    time: 0.7903  last_time: 0.8261  data_time: 0.0203  last_data_time: 0.0573   lr: 1e-05  max_mem: 20850M\n",
      "\u001b[32m[07/13 11:26:24 d2.utils.events]: \u001b[0m eta: 0:17:00  iter: 1099  total_loss: 31.48  loss_ce: 0.5088  loss_mask: 0.04471  loss_dice: 0.3668  loss_bbox: 0.03017  loss_giou: 0.1908  loss_ce_dn: 0.1102  loss_mask_dn: 0.03914  loss_dice_dn: 0.3786  loss_bbox_dn: 0.02726  loss_giou_dn: 0.1614  loss_ce_0: 0.9312  loss_mask_0: 0.04234  loss_dice_0: 0.4032  loss_bbox_0: 0.05631  loss_giou_0: 0.3351  loss_ce_dn_0: 1.932  loss_mask_dn_0: 0.403  loss_dice_dn_0: 2.89  loss_bbox_dn_0: 0.3088  loss_giou_dn_0: 0.8514  loss_ce_1: 0.8296  loss_mask_1: 0.04521  loss_dice_1: 0.4108  loss_bbox_1: 0.03529  loss_giou_1: 0.2395  loss_ce_dn_1: 0.2931  loss_mask_dn_1: 0.0419  loss_dice_dn_1: 0.4041  loss_bbox_dn_1: 0.06056  loss_giou_dn_1: 0.2871  loss_ce_2: 0.7234  loss_mask_2: 0.0411  loss_dice_2: 0.3696  loss_bbox_2: 0.03456  loss_giou_2: 0.2064  loss_ce_dn_2: 0.2115  loss_mask_dn_2: 0.04039  loss_dice_dn_2: 0.3896  loss_bbox_dn_2: 0.0388  loss_giou_dn_2: 0.2014  loss_ce_3: 0.6111  loss_mask_3: 0.03956  loss_dice_3: 0.4173  loss_bbox_3: 0.03294  loss_giou_3: 0.2037  loss_ce_dn_3: 0.1697  loss_mask_dn_3: 0.03733  loss_dice_dn_3: 0.3932  loss_bbox_dn_3: 0.03411  loss_giou_dn_3: 0.1793  loss_ce_4: 0.583  loss_mask_4: 0.04788  loss_dice_4: 0.4077  loss_bbox_4: 0.0338  loss_giou_4: 0.1921  loss_ce_dn_4: 0.1492  loss_mask_dn_4: 0.03983  loss_dice_dn_4: 0.3778  loss_bbox_dn_4: 0.03002  loss_giou_dn_4: 0.172  loss_ce_5: 0.5681  loss_mask_5: 0.04552  loss_dice_5: 0.413  loss_bbox_5: 0.03092  loss_giou_5: 0.1998  loss_ce_dn_5: 0.1312  loss_mask_dn_5: 0.03863  loss_dice_dn_5: 0.3718  loss_bbox_dn_5: 0.02983  loss_giou_dn_5: 0.1663  loss_ce_6: 0.5234  loss_mask_6: 0.04121  loss_dice_6: 0.415  loss_bbox_6: 0.02966  loss_giou_6: 0.1892  loss_ce_dn_6: 0.1233  loss_mask_dn_6: 0.03924  loss_dice_dn_6: 0.3761  loss_bbox_dn_6: 0.0281  loss_giou_dn_6: 0.1643  loss_ce_7: 0.4988  loss_mask_7: 0.03944  loss_dice_7: 0.3734  loss_bbox_7: 0.02945  loss_giou_7: 0.1973  loss_ce_dn_7: 0.1162  loss_mask_dn_7: 0.039  loss_dice_dn_7: 0.3802  loss_bbox_dn_7: 0.0283  loss_giou_dn_7: 0.165  loss_ce_8: 0.5075  loss_mask_8: 0.04528  loss_dice_8: 0.3879  loss_bbox_8: 0.03197  loss_giou_8: 0.1865  loss_ce_dn_8: 0.1114  loss_mask_dn_8: 0.03741  loss_dice_dn_8: 0.3848  loss_bbox_dn_8: 0.0272  loss_giou_dn_8: 0.1616  loss_ce_interm: 0.9436  loss_mask_interm: 0.0431  loss_dice_interm: 0.4324  loss_bbox_interm: 0.07099  loss_giou_interm: 0.328    time: 0.7904  last_time: 0.7665  data_time: 0.0141  last_data_time: 0.0070   lr: 1e-05  max_mem: 20850M\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[07/13 11:26:40 d2.utils.events]: \u001b[0m eta: 0:16:44  iter: 1119  total_loss: 32.48  loss_ce: 0.6272  loss_mask: 0.05121  loss_dice: 0.453  loss_bbox: 0.03895  loss_giou: 0.2359  loss_ce_dn: 0.1404  loss_mask_dn: 0.0423  loss_dice_dn: 0.3595  loss_bbox_dn: 0.03164  loss_giou_dn: 0.1812  loss_ce_0: 1.053  loss_mask_0: 0.06274  loss_dice_0: 0.5752  loss_bbox_0: 0.05985  loss_giou_0: 0.4224  loss_ce_dn_0: 1.95  loss_mask_dn_0: 0.5  loss_dice_dn_0: 3.002  loss_bbox_dn_0: 0.3224  loss_giou_dn_0: 0.8537  loss_ce_1: 0.9329  loss_mask_1: 0.04607  loss_dice_1: 0.4705  loss_bbox_1: 0.05273  loss_giou_1: 0.2917  loss_ce_dn_1: 0.2923  loss_mask_dn_1: 0.04151  loss_dice_dn_1: 0.3783  loss_bbox_dn_1: 0.06872  loss_giou_dn_1: 0.291  loss_ce_2: 0.8137  loss_mask_2: 0.05009  loss_dice_2: 0.5049  loss_bbox_2: 0.05195  loss_giou_2: 0.2658  loss_ce_dn_2: 0.2218  loss_mask_dn_2: 0.03991  loss_dice_dn_2: 0.3719  loss_bbox_dn_2: 0.0411  loss_giou_dn_2: 0.2229  loss_ce_3: 0.732  loss_mask_3: 0.04949  loss_dice_3: 0.4959  loss_bbox_3: 0.05037  loss_giou_3: 0.2575  loss_ce_dn_3: 0.1886  loss_mask_dn_3: 0.03934  loss_dice_dn_3: 0.3829  loss_bbox_dn_3: 0.03536  loss_giou_dn_3: 0.2014  loss_ce_4: 0.6742  loss_mask_4: 0.04859  loss_dice_4: 0.4476  loss_bbox_4: 0.04211  loss_giou_4: 0.2504  loss_ce_dn_4: 0.1708  loss_mask_dn_4: 0.03886  loss_dice_dn_4: 0.3714  loss_bbox_dn_4: 0.03284  loss_giou_dn_4: 0.188  loss_ce_5: 0.6567  loss_mask_5: 0.04473  loss_dice_5: 0.4692  loss_bbox_5: 0.03721  loss_giou_5: 0.2429  loss_ce_dn_5: 0.1545  loss_mask_dn_5: 0.03972  loss_dice_dn_5: 0.3709  loss_bbox_dn_5: 0.03252  loss_giou_dn_5: 0.1843  loss_ce_6: 0.6382  loss_mask_6: 0.04569  loss_dice_6: 0.3677  loss_bbox_6: 0.0411  loss_giou_6: 0.2698  loss_ce_dn_6: 0.1407  loss_mask_dn_6: 0.04156  loss_dice_dn_6: 0.3594  loss_bbox_dn_6: 0.03229  loss_giou_dn_6: 0.1815  loss_ce_7: 0.6402  loss_mask_7: 0.05023  loss_dice_7: 0.4692  loss_bbox_7: 0.03617  loss_giou_7: 0.2346  loss_ce_dn_7: 0.1359  loss_mask_dn_7: 0.0419  loss_dice_dn_7: 0.3407  loss_bbox_dn_7: 0.03145  loss_giou_dn_7: 0.1808  loss_ce_8: 0.6279  loss_mask_8: 0.05339  loss_dice_8: 0.433  loss_bbox_8: 0.03985  loss_giou_8: 0.2377  loss_ce_dn_8: 0.1377  loss_mask_dn_8: 0.04266  loss_dice_dn_8: 0.3726  loss_bbox_dn_8: 0.03123  loss_giou_dn_8: 0.181  loss_ce_interm: 1.072  loss_mask_interm: 0.05394  loss_dice_interm: 0.5136  loss_bbox_interm: 0.07625  loss_giou_interm: 0.3407    time: 0.7904  last_time: 0.8063  data_time: 0.0131  last_data_time: 0.0077   lr: 1e-05  max_mem: 20850M\n",
      "\u001b[32m[07/13 11:26:56 d2.utils.events]: \u001b[0m eta: 0:16:28  iter: 1139  total_loss: 29.28  loss_ce: 0.569  loss_mask: 0.04273  loss_dice: 0.3905  loss_bbox: 0.04079  loss_giou: 0.2202  loss_ce_dn: 0.1548  loss_mask_dn: 0.0325  loss_dice_dn: 0.3732  loss_bbox_dn: 0.02759  loss_giou_dn: 0.1559  loss_ce_0: 1.04  loss_mask_0: 0.04642  loss_dice_0: 0.4335  loss_bbox_0: 0.05879  loss_giou_0: 0.3409  loss_ce_dn_0: 1.924  loss_mask_dn_0: 0.4784  loss_dice_dn_0: 2.91  loss_bbox_dn_0: 0.3048  loss_giou_dn_0: 0.8497  loss_ce_1: 0.875  loss_mask_1: 0.05241  loss_dice_1: 0.4469  loss_bbox_1: 0.04703  loss_giou_1: 0.2561  loss_ce_dn_1: 0.2736  loss_mask_dn_1: 0.04007  loss_dice_dn_1: 0.3831  loss_bbox_dn_1: 0.06574  loss_giou_dn_1: 0.2776  loss_ce_2: 0.7849  loss_mask_2: 0.05133  loss_dice_2: 0.4336  loss_bbox_2: 0.0426  loss_giou_2: 0.2311  loss_ce_dn_2: 0.2089  loss_mask_dn_2: 0.03483  loss_dice_dn_2: 0.3804  loss_bbox_dn_2: 0.04377  loss_giou_dn_2: 0.1978  loss_ce_3: 0.6486  loss_mask_3: 0.04484  loss_dice_3: 0.4683  loss_bbox_3: 0.04094  loss_giou_3: 0.2324  loss_ce_dn_3: 0.1855  loss_mask_dn_3: 0.03296  loss_dice_dn_3: 0.371  loss_bbox_dn_3: 0.03543  loss_giou_dn_3: 0.1737  loss_ce_4: 0.6229  loss_mask_4: 0.04135  loss_dice_4: 0.4459  loss_bbox_4: 0.04822  loss_giou_4: 0.248  loss_ce_dn_4: 0.1718  loss_mask_dn_4: 0.03271  loss_dice_dn_4: 0.3575  loss_bbox_dn_4: 0.0327  loss_giou_dn_4: 0.1655  loss_ce_5: 0.6166  loss_mask_5: 0.0424  loss_dice_5: 0.3982  loss_bbox_5: 0.03919  loss_giou_5: 0.2222  loss_ce_dn_5: 0.1661  loss_mask_dn_5: 0.03343  loss_dice_dn_5: 0.3767  loss_bbox_dn_5: 0.03061  loss_giou_dn_5: 0.163  loss_ce_6: 0.5826  loss_mask_6: 0.03842  loss_dice_6: 0.4377  loss_bbox_6: 0.0396  loss_giou_6: 0.221  loss_ce_dn_6: 0.1613  loss_mask_dn_6: 0.03383  loss_dice_dn_6: 0.3658  loss_bbox_dn_6: 0.02946  loss_giou_dn_6: 0.1592  loss_ce_7: 0.5709  loss_mask_7: 0.04024  loss_dice_7: 0.4257  loss_bbox_7: 0.03928  loss_giou_7: 0.2202  loss_ce_dn_7: 0.1565  loss_mask_dn_7: 0.03401  loss_dice_dn_7: 0.3744  loss_bbox_dn_7: 0.02863  loss_giou_dn_7: 0.1584  loss_ce_8: 0.5546  loss_mask_8: 0.04195  loss_dice_8: 0.4143  loss_bbox_8: 0.04126  loss_giou_8: 0.221  loss_ce_dn_8: 0.1556  loss_mask_dn_8: 0.03303  loss_dice_dn_8: 0.3662  loss_bbox_dn_8: 0.02751  loss_giou_dn_8: 0.1551  loss_ce_interm: 1.022  loss_mask_interm: 0.05106  loss_dice_interm: 0.4024  loss_bbox_interm: 0.0675  loss_giou_interm: 0.3014    time: 0.7905  last_time: 0.7802  data_time: 0.0209  last_data_time: 0.0094   lr: 1e-05  max_mem: 20850M\n",
      "\u001b[32m[07/13 11:27:12 d2.utils.events]: \u001b[0m eta: 0:16:13  iter: 1159  total_loss: 29.51  loss_ce: 0.5827  loss_mask: 0.04756  loss_dice: 0.4052  loss_bbox: 0.03656  loss_giou: 0.2304  loss_ce_dn: 0.1309  loss_mask_dn: 0.05063  loss_dice_dn: 0.3481  loss_bbox_dn: 0.02896  loss_giou_dn: 0.1638  loss_ce_0: 0.9495  loss_mask_0: 0.05136  loss_dice_0: 0.4219  loss_bbox_0: 0.07692  loss_giou_0: 0.3594  loss_ce_dn_0: 1.83  loss_mask_dn_0: 0.4386  loss_dice_dn_0: 2.84  loss_bbox_dn_0: 0.3515  loss_giou_dn_0: 0.8504  loss_ce_1: 0.8548  loss_mask_1: 0.04788  loss_dice_1: 0.3876  loss_bbox_1: 0.05434  loss_giou_1: 0.2845  loss_ce_dn_1: 0.2757  loss_mask_dn_1: 0.05071  loss_dice_dn_1: 0.3761  loss_bbox_dn_1: 0.07601  loss_giou_dn_1: 0.281  loss_ce_2: 0.7701  loss_mask_2: 0.04892  loss_dice_2: 0.3732  loss_bbox_2: 0.04844  loss_giou_2: 0.2538  loss_ce_dn_2: 0.1964  loss_mask_dn_2: 0.05036  loss_dice_dn_2: 0.3503  loss_bbox_dn_2: 0.04572  loss_giou_dn_2: 0.2094  loss_ce_3: 0.6285  loss_mask_3: 0.04311  loss_dice_3: 0.4057  loss_bbox_3: 0.0476  loss_giou_3: 0.232  loss_ce_dn_3: 0.1727  loss_mask_dn_3: 0.04515  loss_dice_dn_3: 0.3509  loss_bbox_dn_3: 0.03686  loss_giou_dn_3: 0.1866  loss_ce_4: 0.6735  loss_mask_4: 0.04565  loss_dice_4: 0.3664  loss_bbox_4: 0.04512  loss_giou_4: 0.2326  loss_ce_dn_4: 0.1576  loss_mask_dn_4: 0.0501  loss_dice_dn_4: 0.3445  loss_bbox_dn_4: 0.03389  loss_giou_dn_4: 0.1743  loss_ce_5: 0.6173  loss_mask_5: 0.04267  loss_dice_5: 0.388  loss_bbox_5: 0.04269  loss_giou_5: 0.2342  loss_ce_dn_5: 0.1477  loss_mask_dn_5: 0.04884  loss_dice_dn_5: 0.3504  loss_bbox_dn_5: 0.03317  loss_giou_dn_5: 0.1684  loss_ce_6: 0.5956  loss_mask_6: 0.04391  loss_dice_6: 0.3333  loss_bbox_6: 0.04072  loss_giou_6: 0.2269  loss_ce_dn_6: 0.144  loss_mask_dn_6: 0.04911  loss_dice_dn_6: 0.3383  loss_bbox_dn_6: 0.0324  loss_giou_dn_6: 0.1659  loss_ce_7: 0.6006  loss_mask_7: 0.04616  loss_dice_7: 0.3856  loss_bbox_7: 0.04248  loss_giou_7: 0.2275  loss_ce_dn_7: 0.1399  loss_mask_dn_7: 0.04946  loss_dice_dn_7: 0.3391  loss_bbox_dn_7: 0.03206  loss_giou_dn_7: 0.1651  loss_ce_8: 0.5999  loss_mask_8: 0.0454  loss_dice_8: 0.4289  loss_bbox_8: 0.04093  loss_giou_8: 0.2142  loss_ce_dn_8: 0.1331  loss_mask_dn_8: 0.04974  loss_dice_dn_8: 0.3475  loss_bbox_dn_8: 0.02911  loss_giou_dn_8: 0.1629  loss_ce_interm: 0.9765  loss_mask_interm: 0.04583  loss_dice_interm: 0.43  loss_bbox_interm: 0.08141  loss_giou_interm: 0.325    time: 0.7904  last_time: 0.7990  data_time: 0.0092  last_data_time: 0.0072   lr: 1e-05  max_mem: 20850M\n",
      "\u001b[32m[07/13 11:27:27 d2.utils.events]: \u001b[0m eta: 0:15:57  iter: 1179  total_loss: 26.1  loss_ce: 0.536  loss_mask: 0.03831  loss_dice: 0.283  loss_bbox: 0.03065  loss_giou: 0.1441  loss_ce_dn: 0.1235  loss_mask_dn: 0.04233  loss_dice_dn: 0.2989  loss_bbox_dn: 0.02818  loss_giou_dn: 0.1291  loss_ce_0: 0.9168  loss_mask_0: 0.03908  loss_dice_0: 0.292  loss_bbox_0: 0.05008  loss_giou_0: 0.2597  loss_ce_dn_0: 1.794  loss_mask_dn_0: 0.4493  loss_dice_dn_0: 2.729  loss_bbox_dn_0: 0.3911  loss_giou_dn_0: 0.8578  loss_ce_1: 0.8195  loss_mask_1: 0.03989  loss_dice_1: 0.2943  loss_bbox_1: 0.03958  loss_giou_1: 0.175  loss_ce_dn_1: 0.254  loss_mask_dn_1: 0.04344  loss_dice_dn_1: 0.3275  loss_bbox_dn_1: 0.071  loss_giou_dn_1: 0.2371  loss_ce_2: 0.7095  loss_mask_2: 0.0363  loss_dice_2: 0.2913  loss_bbox_2: 0.03661  loss_giou_2: 0.149  loss_ce_dn_2: 0.1759  loss_mask_dn_2: 0.04042  loss_dice_dn_2: 0.3102  loss_bbox_dn_2: 0.03908  loss_giou_dn_2: 0.1689  loss_ce_3: 0.6316  loss_mask_3: 0.03708  loss_dice_3: 0.3005  loss_bbox_3: 0.03328  loss_giou_3: 0.1447  loss_ce_dn_3: 0.1371  loss_mask_dn_3: 0.03954  loss_dice_dn_3: 0.3071  loss_bbox_dn_3: 0.03296  loss_giou_dn_3: 0.1476  loss_ce_4: 0.5841  loss_mask_4: 0.03831  loss_dice_4: 0.3048  loss_bbox_4: 0.03195  loss_giou_4: 0.1458  loss_ce_dn_4: 0.1312  loss_mask_dn_4: 0.03897  loss_dice_dn_4: 0.309  loss_bbox_dn_4: 0.0294  loss_giou_dn_4: 0.1378  loss_ce_5: 0.5533  loss_mask_5: 0.03766  loss_dice_5: 0.298  loss_bbox_5: 0.03237  loss_giou_5: 0.1503  loss_ce_dn_5: 0.1171  loss_mask_dn_5: 0.04175  loss_dice_dn_5: 0.2952  loss_bbox_dn_5: 0.02821  loss_giou_dn_5: 0.1341  loss_ce_6: 0.5478  loss_mask_6: 0.03868  loss_dice_6: 0.2988  loss_bbox_6: 0.03223  loss_giou_6: 0.146  loss_ce_dn_6: 0.1179  loss_mask_dn_6: 0.03959  loss_dice_dn_6: 0.2987  loss_bbox_dn_6: 0.02834  loss_giou_dn_6: 0.1323  loss_ce_7: 0.5414  loss_mask_7: 0.03586  loss_dice_7: 0.3079  loss_bbox_7: 0.03143  loss_giou_7: 0.1468  loss_ce_dn_7: 0.1244  loss_mask_dn_7: 0.04169  loss_dice_dn_7: 0.2962  loss_bbox_dn_7: 0.02843  loss_giou_dn_7: 0.132  loss_ce_8: 0.5386  loss_mask_8: 0.0385  loss_dice_8: 0.3033  loss_bbox_8: 0.03045  loss_giou_8: 0.144  loss_ce_dn_8: 0.1246  loss_mask_dn_8: 0.04305  loss_dice_dn_8: 0.2942  loss_bbox_dn_8: 0.0283  loss_giou_dn_8: 0.1284  loss_ce_interm: 0.8815  loss_mask_interm: 0.04022  loss_dice_interm: 0.3296  loss_bbox_interm: 0.06745  loss_giou_interm: 0.2496    time: 0.7902  last_time: 0.7781  data_time: 0.0094  last_data_time: 0.0075   lr: 1e-05  max_mem: 20850M\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[07/13 11:27:43 d2.utils.events]: \u001b[0m eta: 0:15:41  iter: 1199  total_loss: 30.88  loss_ce: 0.5689  loss_mask: 0.03992  loss_dice: 0.4282  loss_bbox: 0.04898  loss_giou: 0.2269  loss_ce_dn: 0.144  loss_mask_dn: 0.03631  loss_dice_dn: 0.402  loss_bbox_dn: 0.0289  loss_giou_dn: 0.1794  loss_ce_0: 0.965  loss_mask_0: 0.03847  loss_dice_0: 0.4618  loss_bbox_0: 0.07707  loss_giou_0: 0.3951  loss_ce_dn_0: 1.958  loss_mask_dn_0: 0.4711  loss_dice_dn_0: 2.853  loss_bbox_dn_0: 0.286  loss_giou_dn_0: 0.8602  loss_ce_1: 0.8654  loss_mask_1: 0.04665  loss_dice_1: 0.4631  loss_bbox_1: 0.0701  loss_giou_1: 0.2626  loss_ce_dn_1: 0.2863  loss_mask_dn_1: 0.0348  loss_dice_dn_1: 0.4338  loss_bbox_dn_1: 0.07031  loss_giou_dn_1: 0.2862  loss_ce_2: 0.7329  loss_mask_2: 0.03991  loss_dice_2: 0.4698  loss_bbox_2: 0.05435  loss_giou_2: 0.2484  loss_ce_dn_2: 0.2026  loss_mask_dn_2: 0.03552  loss_dice_dn_2: 0.4166  loss_bbox_dn_2: 0.04177  loss_giou_dn_2: 0.2192  loss_ce_3: 0.6429  loss_mask_3: 0.03733  loss_dice_3: 0.4253  loss_bbox_3: 0.0505  loss_giou_3: 0.2326  loss_ce_dn_3: 0.1689  loss_mask_dn_3: 0.03533  loss_dice_dn_3: 0.4092  loss_bbox_dn_3: 0.03525  loss_giou_dn_3: 0.1882  loss_ce_4: 0.5981  loss_mask_4: 0.04203  loss_dice_4: 0.4463  loss_bbox_4: 0.04575  loss_giou_4: 0.2211  loss_ce_dn_4: 0.1535  loss_mask_dn_4: 0.03543  loss_dice_dn_4: 0.3915  loss_bbox_dn_4: 0.03115  loss_giou_dn_4: 0.1776  loss_ce_5: 0.5745  loss_mask_5: 0.0406  loss_dice_5: 0.4255  loss_bbox_5: 0.04845  loss_giou_5: 0.2251  loss_ce_dn_5: 0.144  loss_mask_dn_5: 0.03573  loss_dice_dn_5: 0.399  loss_bbox_dn_5: 0.02986  loss_giou_dn_5: 0.1763  loss_ce_6: 0.5769  loss_mask_6: 0.04253  loss_dice_6: 0.4255  loss_bbox_6: 0.04982  loss_giou_6: 0.2326  loss_ce_dn_6: 0.1422  loss_mask_dn_6: 0.03597  loss_dice_dn_6: 0.3924  loss_bbox_dn_6: 0.02908  loss_giou_dn_6: 0.1781  loss_ce_7: 0.5769  loss_mask_7: 0.03672  loss_dice_7: 0.4156  loss_bbox_7: 0.04973  loss_giou_7: 0.2259  loss_ce_dn_7: 0.1403  loss_mask_dn_7: 0.03525  loss_dice_dn_7: 0.3857  loss_bbox_dn_7: 0.02909  loss_giou_dn_7: 0.179  loss_ce_8: 0.5729  loss_mask_8: 0.03806  loss_dice_8: 0.4531  loss_bbox_8: 0.04766  loss_giou_8: 0.2311  loss_ce_dn_8: 0.1395  loss_mask_dn_8: 0.03629  loss_dice_dn_8: 0.3829  loss_bbox_dn_8: 0.02883  loss_giou_dn_8: 0.1789  loss_ce_interm: 0.9676  loss_mask_interm: 0.03885  loss_dice_interm: 0.4311  loss_bbox_interm: 0.0826  loss_giou_interm: 0.3192    time: 0.7901  last_time: 0.7713  data_time: 0.0127  last_data_time: 0.0078   lr: 1e-05  max_mem: 20850M\n",
      "\u001b[32m[07/13 11:27:59 d2.utils.events]: \u001b[0m eta: 0:15:25  iter: 1219  total_loss: 29.23  loss_ce: 0.5075  loss_mask: 0.04694  loss_dice: 0.3543  loss_bbox: 0.0435  loss_giou: 0.1664  loss_ce_dn: 0.09965  loss_mask_dn: 0.04388  loss_dice_dn: 0.326  loss_bbox_dn: 0.03434  loss_giou_dn: 0.1374  loss_ce_0: 0.9237  loss_mask_0: 0.04914  loss_dice_0: 0.3236  loss_bbox_0: 0.05232  loss_giou_0: 0.3011  loss_ce_dn_0: 1.697  loss_mask_dn_0: 0.4895  loss_dice_dn_0: 2.905  loss_bbox_dn_0: 0.3604  loss_giou_dn_0: 0.8528  loss_ce_1: 0.831  loss_mask_1: 0.04827  loss_dice_1: 0.3715  loss_bbox_1: 0.0469  loss_giou_1: 0.2156  loss_ce_dn_1: 0.2422  loss_mask_dn_1: 0.05344  loss_dice_dn_1: 0.3758  loss_bbox_dn_1: 0.07518  loss_giou_dn_1: 0.2485  loss_ce_2: 0.7174  loss_mask_2: 0.04661  loss_dice_2: 0.3234  loss_bbox_2: 0.04368  loss_giou_2: 0.1886  loss_ce_dn_2: 0.1746  loss_mask_dn_2: 0.04918  loss_dice_dn_2: 0.3385  loss_bbox_dn_2: 0.05009  loss_giou_dn_2: 0.1721  loss_ce_3: 0.6219  loss_mask_3: 0.04617  loss_dice_3: 0.4018  loss_bbox_3: 0.04263  loss_giou_3: 0.1796  loss_ce_dn_3: 0.147  loss_mask_dn_3: 0.04662  loss_dice_dn_3: 0.3306  loss_bbox_dn_3: 0.04232  loss_giou_dn_3: 0.1501  loss_ce_4: 0.5827  loss_mask_4: 0.04721  loss_dice_4: 0.4112  loss_bbox_4: 0.03807  loss_giou_4: 0.1783  loss_ce_dn_4: 0.1264  loss_mask_dn_4: 0.04577  loss_dice_dn_4: 0.3361  loss_bbox_dn_4: 0.03864  loss_giou_dn_4: 0.1432  loss_ce_5: 0.5364  loss_mask_5: 0.04754  loss_dice_5: 0.3508  loss_bbox_5: 0.0448  loss_giou_5: 0.1751  loss_ce_dn_5: 0.113  loss_mask_dn_5: 0.04626  loss_dice_dn_5: 0.3352  loss_bbox_dn_5: 0.03785  loss_giou_dn_5: 0.1382  loss_ce_6: 0.5289  loss_mask_6: 0.04716  loss_dice_6: 0.3305  loss_bbox_6: 0.04233  loss_giou_6: 0.171  loss_ce_dn_6: 0.1084  loss_mask_dn_6: 0.04465  loss_dice_dn_6: 0.3185  loss_bbox_dn_6: 0.03611  loss_giou_dn_6: 0.1372  loss_ce_7: 0.5357  loss_mask_7: 0.04707  loss_dice_7: 0.3432  loss_bbox_7: 0.04361  loss_giou_7: 0.1699  loss_ce_dn_7: 0.1034  loss_mask_dn_7: 0.04504  loss_dice_dn_7: 0.3248  loss_bbox_dn_7: 0.0352  loss_giou_dn_7: 0.1378  loss_ce_8: 0.5263  loss_mask_8: 0.04892  loss_dice_8: 0.3845  loss_bbox_8: 0.04043  loss_giou_8: 0.1663  loss_ce_dn_8: 0.1006  loss_mask_dn_8: 0.0436  loss_dice_dn_8: 0.3244  loss_bbox_dn_8: 0.03455  loss_giou_dn_8: 0.1372  loss_ce_interm: 0.9003  loss_mask_interm: 0.04647  loss_dice_interm: 0.3966  loss_bbox_interm: 0.07355  loss_giou_interm: 0.26    time: 0.7900  last_time: 0.7766  data_time: 0.0126  last_data_time: 0.0091   lr: 1e-05  max_mem: 20850M\n",
      "\u001b[32m[07/13 11:28:15 d2.utils.events]: \u001b[0m eta: 0:15:10  iter: 1239  total_loss: 27.73  loss_ce: 0.4754  loss_mask: 0.04818  loss_dice: 0.3464  loss_bbox: 0.0378  loss_giou: 0.1801  loss_ce_dn: 0.1355  loss_mask_dn: 0.04302  loss_dice_dn: 0.3546  loss_bbox_dn: 0.02534  loss_giou_dn: 0.1369  loss_ce_0: 0.9036  loss_mask_0: 0.04106  loss_dice_0: 0.3849  loss_bbox_0: 0.05632  loss_giou_0: 0.3496  loss_ce_dn_0: 1.841  loss_mask_dn_0: 0.4171  loss_dice_dn_0: 2.829  loss_bbox_dn_0: 0.3041  loss_giou_dn_0: 0.8495  loss_ce_1: 0.7594  loss_mask_1: 0.05277  loss_dice_1: 0.3378  loss_bbox_1: 0.04629  loss_giou_1: 0.217  loss_ce_dn_1: 0.259  loss_mask_dn_1: 0.05162  loss_dice_dn_1: 0.385  loss_bbox_dn_1: 0.06139  loss_giou_dn_1: 0.2639  loss_ce_2: 0.6377  loss_mask_2: 0.04952  loss_dice_2: 0.3758  loss_bbox_2: 0.03954  loss_giou_2: 0.196  loss_ce_dn_2: 0.2076  loss_mask_dn_2: 0.04829  loss_dice_dn_2: 0.3595  loss_bbox_dn_2: 0.03856  loss_giou_dn_2: 0.181  loss_ce_3: 0.5523  loss_mask_3: 0.04674  loss_dice_3: 0.3415  loss_bbox_3: 0.03805  loss_giou_3: 0.1808  loss_ce_dn_3: 0.1652  loss_mask_dn_3: 0.04311  loss_dice_dn_3: 0.352  loss_bbox_dn_3: 0.032  loss_giou_dn_3: 0.1573  loss_ce_4: 0.508  loss_mask_4: 0.05007  loss_dice_4: 0.349  loss_bbox_4: 0.0372  loss_giou_4: 0.1804  loss_ce_dn_4: 0.1484  loss_mask_dn_4: 0.04543  loss_dice_dn_4: 0.3577  loss_bbox_dn_4: 0.02923  loss_giou_dn_4: 0.1488  loss_ce_5: 0.4965  loss_mask_5: 0.0515  loss_dice_5: 0.3588  loss_bbox_5: 0.03378  loss_giou_5: 0.1787  loss_ce_dn_5: 0.1403  loss_mask_dn_5: 0.04589  loss_dice_dn_5: 0.3626  loss_bbox_dn_5: 0.02738  loss_giou_dn_5: 0.1436  loss_ce_6: 0.4937  loss_mask_6: 0.04612  loss_dice_6: 0.3358  loss_bbox_6: 0.03599  loss_giou_6: 0.184  loss_ce_dn_6: 0.137  loss_mask_dn_6: 0.04426  loss_dice_dn_6: 0.374  loss_bbox_dn_6: 0.02638  loss_giou_dn_6: 0.1382  loss_ce_7: 0.4787  loss_mask_7: 0.04235  loss_dice_7: 0.3711  loss_bbox_7: 0.03484  loss_giou_7: 0.1733  loss_ce_dn_7: 0.1353  loss_mask_dn_7: 0.04403  loss_dice_dn_7: 0.371  loss_bbox_dn_7: 0.02674  loss_giou_dn_7: 0.1381  loss_ce_8: 0.4795  loss_mask_8: 0.05092  loss_dice_8: 0.3845  loss_bbox_8: 0.03562  loss_giou_8: 0.1745  loss_ce_dn_8: 0.1351  loss_mask_dn_8: 0.04385  loss_dice_dn_8: 0.3756  loss_bbox_dn_8: 0.02556  loss_giou_dn_8: 0.137  loss_ce_interm: 0.9105  loss_mask_interm: 0.0437  loss_dice_interm: 0.3569  loss_bbox_interm: 0.06826  loss_giou_interm: 0.2987    time: 0.7900  last_time: 0.7678  data_time: 0.0127  last_data_time: 0.0085   lr: 1e-05  max_mem: 20850M\n",
      "\u001b[32m[07/13 11:28:30 d2.utils.events]: \u001b[0m eta: 0:14:54  iter: 1259  total_loss: 31.16  loss_ce: 0.6183  loss_mask: 0.03597  loss_dice: 0.4132  loss_bbox: 0.03944  loss_giou: 0.2179  loss_ce_dn: 0.1165  loss_mask_dn: 0.04047  loss_dice_dn: 0.3285  loss_bbox_dn: 0.02838  loss_giou_dn: 0.1699  loss_ce_0: 0.984  loss_mask_0: 0.04074  loss_dice_0: 0.3911  loss_bbox_0: 0.06121  loss_giou_0: 0.3576  loss_ce_dn_0: 1.913  loss_mask_dn_0: 0.4727  loss_dice_dn_0: 2.831  loss_bbox_dn_0: 0.3235  loss_giou_dn_0: 0.8546  loss_ce_1: 0.8908  loss_mask_1: 0.04015  loss_dice_1: 0.4185  loss_bbox_1: 0.04775  loss_giou_1: 0.2713  loss_ce_dn_1: 0.2645  loss_mask_dn_1: 0.04364  loss_dice_dn_1: 0.3734  loss_bbox_dn_1: 0.06546  loss_giou_dn_1: 0.2701  loss_ce_2: 0.7889  loss_mask_2: 0.0453  loss_dice_2: 0.4673  loss_bbox_2: 0.04204  loss_giou_2: 0.2476  loss_ce_dn_2: 0.1814  loss_mask_dn_2: 0.04487  loss_dice_dn_2: 0.3475  loss_bbox_dn_2: 0.03967  loss_giou_dn_2: 0.2015  loss_ce_3: 0.6754  loss_mask_3: 0.04363  loss_dice_3: 0.4359  loss_bbox_3: 0.04164  loss_giou_3: 0.232  loss_ce_dn_3: 0.1586  loss_mask_dn_3: 0.04054  loss_dice_dn_3: 0.3386  loss_bbox_dn_3: 0.03231  loss_giou_dn_3: 0.1783  loss_ce_4: 0.6375  loss_mask_4: 0.04453  loss_dice_4: 0.4489  loss_bbox_4: 0.04026  loss_giou_4: 0.2246  loss_ce_dn_4: 0.1408  loss_mask_dn_4: 0.03969  loss_dice_dn_4: 0.3186  loss_bbox_dn_4: 0.02991  loss_giou_dn_4: 0.1709  loss_ce_5: 0.6033  loss_mask_5: 0.04296  loss_dice_5: 0.426  loss_bbox_5: 0.03862  loss_giou_5: 0.226  loss_ce_dn_5: 0.1285  loss_mask_dn_5: 0.03732  loss_dice_dn_5: 0.3433  loss_bbox_dn_5: 0.02941  loss_giou_dn_5: 0.1684  loss_ce_6: 0.5844  loss_mask_6: 0.03715  loss_dice_6: 0.3733  loss_bbox_6: 0.03808  loss_giou_6: 0.2239  loss_ce_dn_6: 0.1209  loss_mask_dn_6: 0.03876  loss_dice_dn_6: 0.3329  loss_bbox_dn_6: 0.02799  loss_giou_dn_6: 0.1684  loss_ce_7: 0.5754  loss_mask_7: 0.03822  loss_dice_7: 0.3848  loss_bbox_7: 0.03844  loss_giou_7: 0.2197  loss_ce_dn_7: 0.1161  loss_mask_dn_7: 0.03807  loss_dice_dn_7: 0.3263  loss_bbox_dn_7: 0.0282  loss_giou_dn_7: 0.1694  loss_ce_8: 0.6138  loss_mask_8: 0.0384  loss_dice_8: 0.4009  loss_bbox_8: 0.04129  loss_giou_8: 0.2181  loss_ce_dn_8: 0.1156  loss_mask_dn_8: 0.03939  loss_dice_dn_8: 0.3374  loss_bbox_dn_8: 0.02805  loss_giou_dn_8: 0.1692  loss_ce_interm: 0.9921  loss_mask_interm: 0.04511  loss_dice_interm: 0.381  loss_bbox_interm: 0.08029  loss_giou_interm: 0.299    time: 0.7899  last_time: 0.7734  data_time: 0.0122  last_data_time: 0.0078   lr: 1e-05  max_mem: 20850M\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[07/13 11:28:46 d2.utils.events]: \u001b[0m eta: 0:14:38  iter: 1279  total_loss: 28.89  loss_ce: 0.4499  loss_mask: 0.06212  loss_dice: 0.417  loss_bbox: 0.04129  loss_giou: 0.1882  loss_ce_dn: 0.09904  loss_mask_dn: 0.06102  loss_dice_dn: 0.3661  loss_bbox_dn: 0.03062  loss_giou_dn: 0.1568  loss_ce_0: 0.9191  loss_mask_0: 0.06742  loss_dice_0: 0.464  loss_bbox_0: 0.07549  loss_giou_0: 0.3351  loss_ce_dn_0: 1.812  loss_mask_dn_0: 0.5081  loss_dice_dn_0: 2.997  loss_bbox_dn_0: 0.3941  loss_giou_dn_0: 0.8535  loss_ce_1: 0.8191  loss_mask_1: 0.06179  loss_dice_1: 0.4534  loss_bbox_1: 0.04976  loss_giou_1: 0.2515  loss_ce_dn_1: 0.2471  loss_mask_dn_1: 0.07308  loss_dice_dn_1: 0.4257  loss_bbox_dn_1: 0.07588  loss_giou_dn_1: 0.2693  loss_ce_2: 0.6605  loss_mask_2: 0.05989  loss_dice_2: 0.3972  loss_bbox_2: 0.05494  loss_giou_2: 0.2302  loss_ce_dn_2: 0.1728  loss_mask_dn_2: 0.06483  loss_dice_dn_2: 0.4005  loss_bbox_dn_2: 0.04941  loss_giou_dn_2: 0.199  loss_ce_3: 0.5579  loss_mask_3: 0.06179  loss_dice_3: 0.3951  loss_bbox_3: 0.05395  loss_giou_3: 0.2215  loss_ce_dn_3: 0.138  loss_mask_dn_3: 0.06309  loss_dice_dn_3: 0.4027  loss_bbox_dn_3: 0.04158  loss_giou_dn_3: 0.178  loss_ce_4: 0.51  loss_mask_4: 0.05614  loss_dice_4: 0.4138  loss_bbox_4: 0.05391  loss_giou_4: 0.2115  loss_ce_dn_4: 0.1238  loss_mask_dn_4: 0.06285  loss_dice_dn_4: 0.3867  loss_bbox_dn_4: 0.03724  loss_giou_dn_4: 0.1632  loss_ce_5: 0.5111  loss_mask_5: 0.05897  loss_dice_5: 0.3804  loss_bbox_5: 0.04907  loss_giou_5: 0.2106  loss_ce_dn_5: 0.1141  loss_mask_dn_5: 0.05989  loss_dice_dn_5: 0.4009  loss_bbox_dn_5: 0.0352  loss_giou_dn_5: 0.1601  loss_ce_6: 0.4931  loss_mask_6: 0.05893  loss_dice_6: 0.3624  loss_bbox_6: 0.04494  loss_giou_6: 0.2286  loss_ce_dn_6: 0.1061  loss_mask_dn_6: 0.05852  loss_dice_dn_6: 0.3822  loss_bbox_dn_6: 0.03198  loss_giou_dn_6: 0.1568  loss_ce_7: 0.4942  loss_mask_7: 0.05938  loss_dice_7: 0.4203  loss_bbox_7: 0.0426  loss_giou_7: 0.2185  loss_ce_dn_7: 0.1001  loss_mask_dn_7: 0.05933  loss_dice_dn_7: 0.372  loss_bbox_dn_7: 0.03079  loss_giou_dn_7: 0.156  loss_ce_8: 0.4505  loss_mask_8: 0.06178  loss_dice_8: 0.4119  loss_bbox_8: 0.04132  loss_giou_8: 0.2039  loss_ce_dn_8: 0.09812  loss_mask_dn_8: 0.0602  loss_dice_dn_8: 0.3766  loss_bbox_dn_8: 0.03063  loss_giou_dn_8: 0.1567  loss_ce_interm: 0.9154  loss_mask_interm: 0.06193  loss_dice_interm: 0.4513  loss_bbox_interm: 0.07424  loss_giou_interm: 0.2899    time: 0.7900  last_time: 0.8270  data_time: 0.0156  last_data_time: 0.0500   lr: 1e-05  max_mem: 20850M\n",
      "\u001b[32m[07/13 11:29:02 d2.utils.events]: \u001b[0m eta: 0:14:22  iter: 1299  total_loss: 28.3  loss_ce: 0.4996  loss_mask: 0.07416  loss_dice: 0.3479  loss_bbox: 0.04072  loss_giou: 0.2169  loss_ce_dn: 0.1005  loss_mask_dn: 0.05303  loss_dice_dn: 0.3128  loss_bbox_dn: 0.02935  loss_giou_dn: 0.1645  loss_ce_0: 0.9225  loss_mask_0: 0.0552  loss_dice_0: 0.3756  loss_bbox_0: 0.06382  loss_giou_0: 0.3824  loss_ce_dn_0: 1.818  loss_mask_dn_0: 0.5822  loss_dice_dn_0: 2.895  loss_bbox_dn_0: 0.3857  loss_giou_dn_0: 0.8499  loss_ce_1: 0.7949  loss_mask_1: 0.05665  loss_dice_1: 0.3678  loss_bbox_1: 0.04514  loss_giou_1: 0.2692  loss_ce_dn_1: 0.2719  loss_mask_dn_1: 0.05363  loss_dice_dn_1: 0.3317  loss_bbox_dn_1: 0.07988  loss_giou_dn_1: 0.2713  loss_ce_2: 0.6787  loss_mask_2: 0.05866  loss_dice_2: 0.3953  loss_bbox_2: 0.04285  loss_giou_2: 0.2372  loss_ce_dn_2: 0.1816  loss_mask_dn_2: 0.0521  loss_dice_dn_2: 0.3326  loss_bbox_dn_2: 0.04507  loss_giou_dn_2: 0.1955  loss_ce_3: 0.5799  loss_mask_3: 0.05528  loss_dice_3: 0.365  loss_bbox_3: 0.04214  loss_giou_3: 0.2278  loss_ce_dn_3: 0.1425  loss_mask_dn_3: 0.05278  loss_dice_dn_3: 0.3101  loss_bbox_dn_3: 0.03534  loss_giou_dn_3: 0.1836  loss_ce_4: 0.5029  loss_mask_4: 0.07221  loss_dice_4: 0.4233  loss_bbox_4: 0.03772  loss_giou_4: 0.2256  loss_ce_dn_4: 0.126  loss_mask_dn_4: 0.05229  loss_dice_dn_4: 0.3067  loss_bbox_dn_4: 0.03275  loss_giou_dn_4: 0.1713  loss_ce_5: 0.5319  loss_mask_5: 0.07179  loss_dice_5: 0.4163  loss_bbox_5: 0.03734  loss_giou_5: 0.2236  loss_ce_dn_5: 0.1102  loss_mask_dn_5: 0.04993  loss_dice_dn_5: 0.3244  loss_bbox_dn_5: 0.03099  loss_giou_dn_5: 0.1695  loss_ce_6: 0.4894  loss_mask_6: 0.07133  loss_dice_6: 0.4087  loss_bbox_6: 0.03805  loss_giou_6: 0.2216  loss_ce_dn_6: 0.1062  loss_mask_dn_6: 0.05181  loss_dice_dn_6: 0.323  loss_bbox_dn_6: 0.02927  loss_giou_dn_6: 0.1644  loss_ce_7: 0.4863  loss_mask_7: 0.07043  loss_dice_7: 0.3852  loss_bbox_7: 0.04376  loss_giou_7: 0.2202  loss_ce_dn_7: 0.1034  loss_mask_dn_7: 0.05184  loss_dice_dn_7: 0.3099  loss_bbox_dn_7: 0.02943  loss_giou_dn_7: 0.166  loss_ce_8: 0.5036  loss_mask_8: 0.0734  loss_dice_8: 0.3488  loss_bbox_8: 0.03843  loss_giou_8: 0.2193  loss_ce_dn_8: 0.1015  loss_mask_dn_8: 0.05275  loss_dice_dn_8: 0.2981  loss_bbox_dn_8: 0.02893  loss_giou_dn_8: 0.1643  loss_ce_interm: 0.9204  loss_mask_interm: 0.05814  loss_dice_interm: 0.3959  loss_bbox_interm: 0.09008  loss_giou_interm: 0.314    time: 0.7900  last_time: 0.7907  data_time: 0.0133  last_data_time: 0.0093   lr: 1e-05  max_mem: 20850M\n",
      "\u001b[32m[07/13 11:29:18 d2.utils.events]: \u001b[0m eta: 0:14:07  iter: 1319  total_loss: 28.23  loss_ce: 0.5117  loss_mask: 0.03985  loss_dice: 0.3275  loss_bbox: 0.03684  loss_giou: 0.1684  loss_ce_dn: 0.128  loss_mask_dn: 0.03474  loss_dice_dn: 0.2986  loss_bbox_dn: 0.02638  loss_giou_dn: 0.1346  loss_ce_0: 0.8817  loss_mask_0: 0.03694  loss_dice_0: 0.3306  loss_bbox_0: 0.06147  loss_giou_0: 0.2719  loss_ce_dn_0: 1.814  loss_mask_dn_0: 0.4397  loss_dice_dn_0: 2.752  loss_bbox_dn_0: 0.3286  loss_giou_dn_0: 0.8547  loss_ce_1: 0.8224  loss_mask_1: 0.04143  loss_dice_1: 0.3271  loss_bbox_1: 0.04952  loss_giou_1: 0.1869  loss_ce_dn_1: 0.2702  loss_mask_dn_1: 0.04143  loss_dice_dn_1: 0.3829  loss_bbox_dn_1: 0.07189  loss_giou_dn_1: 0.2731  loss_ce_2: 0.6639  loss_mask_2: 0.04043  loss_dice_2: 0.3997  loss_bbox_2: 0.05057  loss_giou_2: 0.1854  loss_ce_dn_2: 0.207  loss_mask_dn_2: 0.03867  loss_dice_dn_2: 0.3375  loss_bbox_dn_2: 0.04224  loss_giou_dn_2: 0.1928  loss_ce_3: 0.6395  loss_mask_3: 0.03878  loss_dice_3: 0.3903  loss_bbox_3: 0.04595  loss_giou_3: 0.1756  loss_ce_dn_3: 0.1842  loss_mask_dn_3: 0.03816  loss_dice_dn_3: 0.3212  loss_bbox_dn_3: 0.03273  loss_giou_dn_3: 0.1616  loss_ce_4: 0.6046  loss_mask_4: 0.03757  loss_dice_4: 0.3575  loss_bbox_4: 0.03918  loss_giou_4: 0.1737  loss_ce_dn_4: 0.1586  loss_mask_dn_4: 0.03558  loss_dice_dn_4: 0.3131  loss_bbox_dn_4: 0.02937  loss_giou_dn_4: 0.1393  loss_ce_5: 0.5955  loss_mask_5: 0.04241  loss_dice_5: 0.3493  loss_bbox_5: 0.04105  loss_giou_5: 0.1731  loss_ce_dn_5: 0.1456  loss_mask_dn_5: 0.03513  loss_dice_dn_5: 0.3121  loss_bbox_dn_5: 0.02916  loss_giou_dn_5: 0.1376  loss_ce_6: 0.5268  loss_mask_6: 0.03989  loss_dice_6: 0.3611  loss_bbox_6: 0.0399  loss_giou_6: 0.17  loss_ce_dn_6: 0.1378  loss_mask_dn_6: 0.03531  loss_dice_dn_6: 0.306  loss_bbox_dn_6: 0.0269  loss_giou_dn_6: 0.1321  loss_ce_7: 0.5202  loss_mask_7: 0.03825  loss_dice_7: 0.336  loss_bbox_7: 0.04219  loss_giou_7: 0.1648  loss_ce_dn_7: 0.1316  loss_mask_dn_7: 0.03515  loss_dice_dn_7: 0.289  loss_bbox_dn_7: 0.02654  loss_giou_dn_7: 0.133  loss_ce_8: 0.5145  loss_mask_8: 0.0379  loss_dice_8: 0.3051  loss_bbox_8: 0.03709  loss_giou_8: 0.1706  loss_ce_dn_8: 0.1286  loss_mask_dn_8: 0.03551  loss_dice_dn_8: 0.2914  loss_bbox_dn_8: 0.02624  loss_giou_dn_8: 0.1342  loss_ce_interm: 0.8661  loss_mask_interm: 0.04066  loss_dice_interm: 0.3381  loss_bbox_interm: 0.07297  loss_giou_interm: 0.2783    time: 0.7900  last_time: 0.7992  data_time: 0.0175  last_data_time: 0.0253   lr: 1e-05  max_mem: 20850M\n",
      "\u001b[32m[07/13 11:29:34 d2.utils.events]: \u001b[0m eta: 0:13:51  iter: 1339  total_loss: 30.51  loss_ce: 0.5376  loss_mask: 0.04928  loss_dice: 0.3601  loss_bbox: 0.04113  loss_giou: 0.2072  loss_ce_dn: 0.08907  loss_mask_dn: 0.0453  loss_dice_dn: 0.3405  loss_bbox_dn: 0.03029  loss_giou_dn: 0.1624  loss_ce_0: 0.9929  loss_mask_0: 0.05246  loss_dice_0: 0.4132  loss_bbox_0: 0.05784  loss_giou_0: 0.3302  loss_ce_dn_0: 1.863  loss_mask_dn_0: 0.6085  loss_dice_dn_0: 2.768  loss_bbox_dn_0: 0.3429  loss_giou_dn_0: 0.8537  loss_ce_1: 0.849  loss_mask_1: 0.05132  loss_dice_1: 0.3981  loss_bbox_1: 0.04825  loss_giou_1: 0.2332  loss_ce_dn_1: 0.2806  loss_mask_dn_1: 0.04778  loss_dice_dn_1: 0.3891  loss_bbox_dn_1: 0.08187  loss_giou_dn_1: 0.2728  loss_ce_2: 0.6939  loss_mask_2: 0.04936  loss_dice_2: 0.4672  loss_bbox_2: 0.04424  loss_giou_2: 0.233  loss_ce_dn_2: 0.1781  loss_mask_dn_2: 0.0474  loss_dice_dn_2: 0.3691  loss_bbox_dn_2: 0.04811  loss_giou_dn_2: 0.202  loss_ce_3: 0.6574  loss_mask_3: 0.05017  loss_dice_3: 0.431  loss_bbox_3: 0.04819  loss_giou_3: 0.2148  loss_ce_dn_3: 0.143  loss_mask_dn_3: 0.04793  loss_dice_dn_3: 0.3562  loss_bbox_dn_3: 0.03923  loss_giou_dn_3: 0.1762  loss_ce_4: 0.6188  loss_mask_4: 0.04777  loss_dice_4: 0.3885  loss_bbox_4: 0.04165  loss_giou_4: 0.2131  loss_ce_dn_4: 0.1261  loss_mask_dn_4: 0.04722  loss_dice_dn_4: 0.3551  loss_bbox_dn_4: 0.03418  loss_giou_dn_4: 0.1683  loss_ce_5: 0.5973  loss_mask_5: 0.05225  loss_dice_5: 0.4381  loss_bbox_5: 0.04339  loss_giou_5: 0.2159  loss_ce_dn_5: 0.1131  loss_mask_dn_5: 0.04623  loss_dice_dn_5: 0.3489  loss_bbox_dn_5: 0.03311  loss_giou_dn_5: 0.1694  loss_ce_6: 0.5797  loss_mask_6: 0.04943  loss_dice_6: 0.4258  loss_bbox_6: 0.04285  loss_giou_6: 0.2084  loss_ce_dn_6: 0.1085  loss_mask_dn_6: 0.04636  loss_dice_dn_6: 0.3389  loss_bbox_dn_6: 0.03163  loss_giou_dn_6: 0.1661  loss_ce_7: 0.5659  loss_mask_7: 0.05023  loss_dice_7: 0.4023  loss_bbox_7: 0.04165  loss_giou_7: 0.2122  loss_ce_dn_7: 0.09619  loss_mask_dn_7: 0.04598  loss_dice_dn_7: 0.344  loss_bbox_dn_7: 0.03141  loss_giou_dn_7: 0.1659  loss_ce_8: 0.5593  loss_mask_8: 0.04904  loss_dice_8: 0.4135  loss_bbox_8: 0.0411  loss_giou_8: 0.2044  loss_ce_dn_8: 0.09208  loss_mask_dn_8: 0.04606  loss_dice_dn_8: 0.3426  loss_bbox_dn_8: 0.03022  loss_giou_dn_8: 0.162  loss_ce_interm: 0.9326  loss_mask_interm: 0.05296  loss_dice_interm: 0.4141  loss_bbox_interm: 0.0872  loss_giou_interm: 0.3126    time: 0.7901  last_time: 0.8473  data_time: 0.0216  last_data_time: 0.0671   lr: 1e-05  max_mem: 20850M\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[07/13 11:29:50 d2.utils.events]: \u001b[0m eta: 0:13:35  iter: 1359  total_loss: 32.62  loss_ce: 0.5713  loss_mask: 0.05508  loss_dice: 0.4716  loss_bbox: 0.03819  loss_giou: 0.2156  loss_ce_dn: 0.1486  loss_mask_dn: 0.04271  loss_dice_dn: 0.414  loss_bbox_dn: 0.0353  loss_giou_dn: 0.1778  loss_ce_0: 0.9645  loss_mask_0: 0.04884  loss_dice_0: 0.5663  loss_bbox_0: 0.07992  loss_giou_0: 0.3998  loss_ce_dn_0: 1.783  loss_mask_dn_0: 0.5149  loss_dice_dn_0: 2.869  loss_bbox_dn_0: 0.3981  loss_giou_dn_0: 0.8531  loss_ce_1: 0.8448  loss_mask_1: 0.06465  loss_dice_1: 0.4835  loss_bbox_1: 0.06483  loss_giou_1: 0.2739  loss_ce_dn_1: 0.2873  loss_mask_dn_1: 0.05096  loss_dice_dn_1: 0.4035  loss_bbox_dn_1: 0.07706  loss_giou_dn_1: 0.288  loss_ce_2: 0.7077  loss_mask_2: 0.05045  loss_dice_2: 0.4185  loss_bbox_2: 0.05616  loss_giou_2: 0.241  loss_ce_dn_2: 0.2155  loss_mask_dn_2: 0.04501  loss_dice_dn_2: 0.4077  loss_bbox_dn_2: 0.04719  loss_giou_dn_2: 0.2118  loss_ce_3: 0.6159  loss_mask_3: 0.0568  loss_dice_3: 0.4474  loss_bbox_3: 0.04799  loss_giou_3: 0.2303  loss_ce_dn_3: 0.1855  loss_mask_dn_3: 0.04369  loss_dice_dn_3: 0.4132  loss_bbox_dn_3: 0.03908  loss_giou_dn_3: 0.1827  loss_ce_4: 0.6193  loss_mask_4: 0.0499  loss_dice_4: 0.5141  loss_bbox_4: 0.04976  loss_giou_4: 0.2449  loss_ce_dn_4: 0.172  loss_mask_dn_4: 0.04238  loss_dice_dn_4: 0.3876  loss_bbox_dn_4: 0.03594  loss_giou_dn_4: 0.1767  loss_ce_5: 0.63  loss_mask_5: 0.05137  loss_dice_5: 0.4068  loss_bbox_5: 0.04198  loss_giou_5: 0.2296  loss_ce_dn_5: 0.1559  loss_mask_dn_5: 0.0442  loss_dice_dn_5: 0.4121  loss_bbox_dn_5: 0.03529  loss_giou_dn_5: 0.1749  loss_ce_6: 0.5998  loss_mask_6: 0.05309  loss_dice_6: 0.4785  loss_bbox_6: 0.04087  loss_giou_6: 0.2099  loss_ce_dn_6: 0.1525  loss_mask_dn_6: 0.0429  loss_dice_dn_6: 0.416  loss_bbox_dn_6: 0.03571  loss_giou_dn_6: 0.1762  loss_ce_7: 0.5708  loss_mask_7: 0.05611  loss_dice_7: 0.4329  loss_bbox_7: 0.0396  loss_giou_7: 0.2131  loss_ce_dn_7: 0.1504  loss_mask_dn_7: 0.04302  loss_dice_dn_7: 0.4271  loss_bbox_dn_7: 0.03533  loss_giou_dn_7: 0.1768  loss_ce_8: 0.5572  loss_mask_8: 0.05193  loss_dice_8: 0.4309  loss_bbox_8: 0.04431  loss_giou_8: 0.221  loss_ce_dn_8: 0.1494  loss_mask_dn_8: 0.04407  loss_dice_dn_8: 0.4032  loss_bbox_dn_8: 0.03528  loss_giou_dn_8: 0.1775  loss_ce_interm: 0.937  loss_mask_interm: 0.05221  loss_dice_interm: 0.5149  loss_bbox_interm: 0.07904  loss_giou_interm: 0.3767    time: 0.7901  last_time: 0.7813  data_time: 0.0164  last_data_time: 0.0080   lr: 1e-05  max_mem: 20850M\n",
      "\u001b[32m[07/13 11:30:06 d2.utils.events]: \u001b[0m eta: 0:13:20  iter: 1379  total_loss: 30.26  loss_ce: 0.603  loss_mask: 0.03691  loss_dice: 0.4664  loss_bbox: 0.0386  loss_giou: 0.2363  loss_ce_dn: 0.1057  loss_mask_dn: 0.03816  loss_dice_dn: 0.4227  loss_bbox_dn: 0.03181  loss_giou_dn: 0.1887  loss_ce_0: 1.029  loss_mask_0: 0.03861  loss_dice_0: 0.5009  loss_bbox_0: 0.05322  loss_giou_0: 0.3956  loss_ce_dn_0: 1.774  loss_mask_dn_0: 0.4617  loss_dice_dn_0: 2.807  loss_bbox_dn_0: 0.2749  loss_giou_dn_0: 0.8545  loss_ce_1: 0.9416  loss_mask_1: 0.0432  loss_dice_1: 0.4507  loss_bbox_1: 0.04691  loss_giou_1: 0.2716  loss_ce_dn_1: 0.2557  loss_mask_dn_1: 0.03986  loss_dice_dn_1: 0.4644  loss_bbox_dn_1: 0.0696  loss_giou_dn_1: 0.2912  loss_ce_2: 0.8371  loss_mask_2: 0.04203  loss_dice_2: 0.4309  loss_bbox_2: 0.04163  loss_giou_2: 0.2469  loss_ce_dn_2: 0.177  loss_mask_dn_2: 0.03838  loss_dice_dn_2: 0.4534  loss_bbox_dn_2: 0.04357  loss_giou_dn_2: 0.2225  loss_ce_3: 0.6996  loss_mask_3: 0.03817  loss_dice_3: 0.4124  loss_bbox_3: 0.04015  loss_giou_3: 0.2377  loss_ce_dn_3: 0.1422  loss_mask_dn_3: 0.0375  loss_dice_dn_3: 0.4617  loss_bbox_dn_3: 0.03704  loss_giou_dn_3: 0.1983  loss_ce_4: 0.6631  loss_mask_4: 0.03931  loss_dice_4: 0.4342  loss_bbox_4: 0.04059  loss_giou_4: 0.227  loss_ce_dn_4: 0.1324  loss_mask_dn_4: 0.03771  loss_dice_dn_4: 0.4452  loss_bbox_dn_4: 0.0343  loss_giou_dn_4: 0.1908  loss_ce_5: 0.6095  loss_mask_5: 0.04029  loss_dice_5: 0.4233  loss_bbox_5: 0.03952  loss_giou_5: 0.2337  loss_ce_dn_5: 0.1174  loss_mask_dn_5: 0.03676  loss_dice_dn_5: 0.3996  loss_bbox_dn_5: 0.03332  loss_giou_dn_5: 0.1899  loss_ce_6: 0.6251  loss_mask_6: 0.03532  loss_dice_6: 0.4736  loss_bbox_6: 0.03667  loss_giou_6: 0.226  loss_ce_dn_6: 0.1105  loss_mask_dn_6: 0.03675  loss_dice_dn_6: 0.4372  loss_bbox_dn_6: 0.03187  loss_giou_dn_6: 0.1894  loss_ce_7: 0.6146  loss_mask_7: 0.0394  loss_dice_7: 0.4072  loss_bbox_7: 0.03687  loss_giou_7: 0.2309  loss_ce_dn_7: 0.1096  loss_mask_dn_7: 0.03673  loss_dice_dn_7: 0.3966  loss_bbox_dn_7: 0.03173  loss_giou_dn_7: 0.1906  loss_ce_8: 0.6051  loss_mask_8: 0.03824  loss_dice_8: 0.4307  loss_bbox_8: 0.03676  loss_giou_8: 0.2324  loss_ce_dn_8: 0.1056  loss_mask_dn_8: 0.03706  loss_dice_dn_8: 0.4044  loss_bbox_dn_8: 0.03173  loss_giou_dn_8: 0.1893  loss_ce_interm: 0.9773  loss_mask_interm: 0.04194  loss_dice_interm: 0.4723  loss_bbox_interm: 0.0742  loss_giou_interm: 0.3072    time: 0.7902  last_time: 0.7890  data_time: 0.0226  last_data_time: 0.0101   lr: 1e-05  max_mem: 20850M\n",
      "\u001b[32m[07/13 11:30:22 d2.utils.events]: \u001b[0m eta: 0:13:04  iter: 1399  total_loss: 31.73  loss_ce: 0.6318  loss_mask: 0.03546  loss_dice: 0.4884  loss_bbox: 0.0471  loss_giou: 0.2462  loss_ce_dn: 0.1287  loss_mask_dn: 0.03147  loss_dice_dn: 0.426  loss_bbox_dn: 0.02906  loss_giou_dn: 0.1754  loss_ce_0: 1.09  loss_mask_0: 0.03665  loss_dice_0: 0.4699  loss_bbox_0: 0.07072  loss_giou_0: 0.4135  loss_ce_dn_0: 1.816  loss_mask_dn_0: 0.3698  loss_dice_dn_0: 2.95  loss_bbox_dn_0: 0.2686  loss_giou_dn_0: 0.8545  loss_ce_1: 0.9191  loss_mask_1: 0.03766  loss_dice_1: 0.5113  loss_bbox_1: 0.05542  loss_giou_1: 0.3008  loss_ce_dn_1: 0.2728  loss_mask_dn_1: 0.03791  loss_dice_dn_1: 0.5136  loss_bbox_dn_1: 0.05784  loss_giou_dn_1: 0.297  loss_ce_2: 0.7515  loss_mask_2: 0.03812  loss_dice_2: 0.5021  loss_bbox_2: 0.05407  loss_giou_2: 0.275  loss_ce_dn_2: 0.2025  loss_mask_dn_2: 0.0347  loss_dice_dn_2: 0.4556  loss_bbox_dn_2: 0.03882  loss_giou_dn_2: 0.2211  loss_ce_3: 0.7223  loss_mask_3: 0.0357  loss_dice_3: 0.4737  loss_bbox_3: 0.0503  loss_giou_3: 0.2616  loss_ce_dn_3: 0.1695  loss_mask_dn_3: 0.03252  loss_dice_dn_3: 0.4363  loss_bbox_dn_3: 0.03327  loss_giou_dn_3: 0.1978  loss_ce_4: 0.7246  loss_mask_4: 0.03622  loss_dice_4: 0.4766  loss_bbox_4: 0.05354  loss_giou_4: 0.2558  loss_ce_dn_4: 0.1527  loss_mask_dn_4: 0.03219  loss_dice_dn_4: 0.4412  loss_bbox_dn_4: 0.03031  loss_giou_dn_4: 0.1867  loss_ce_5: 0.6674  loss_mask_5: 0.03409  loss_dice_5: 0.4469  loss_bbox_5: 0.05039  loss_giou_5: 0.2525  loss_ce_dn_5: 0.1465  loss_mask_dn_5: 0.03102  loss_dice_dn_5: 0.4442  loss_bbox_dn_5: 0.0292  loss_giou_dn_5: 0.1798  loss_ce_6: 0.6627  loss_mask_6: 0.03569  loss_dice_6: 0.4826  loss_bbox_6: 0.04767  loss_giou_6: 0.2476  loss_ce_dn_6: 0.1364  loss_mask_dn_6: 0.03163  loss_dice_dn_6: 0.4486  loss_bbox_dn_6: 0.02894  loss_giou_dn_6: 0.1788  loss_ce_7: 0.6186  loss_mask_7: 0.03587  loss_dice_7: 0.4721  loss_bbox_7: 0.04825  loss_giou_7: 0.2473  loss_ce_dn_7: 0.1354  loss_mask_dn_7: 0.03239  loss_dice_dn_7: 0.4369  loss_bbox_dn_7: 0.02881  loss_giou_dn_7: 0.1788  loss_ce_8: 0.641  loss_mask_8: 0.03653  loss_dice_8: 0.4298  loss_bbox_8: 0.04869  loss_giou_8: 0.2467  loss_ce_dn_8: 0.1292  loss_mask_dn_8: 0.0322  loss_dice_dn_8: 0.4384  loss_bbox_dn_8: 0.02906  loss_giou_dn_8: 0.1758  loss_ce_interm: 1.011  loss_mask_interm: 0.0338  loss_dice_interm: 0.4792  loss_bbox_interm: 0.07567  loss_giou_interm: 0.349    time: 0.7902  last_time: 0.7773  data_time: 0.0119  last_data_time: 0.0094   lr: 1e-05  max_mem: 20850M\n",
      "\u001b[32m[07/13 11:30:38 d2.utils.events]: \u001b[0m eta: 0:12:49  iter: 1419  total_loss: 29.32  loss_ce: 0.5142  loss_mask: 0.04969  loss_dice: 0.3965  loss_bbox: 0.0345  loss_giou: 0.1929  loss_ce_dn: 0.122  loss_mask_dn: 0.04554  loss_dice_dn: 0.3481  loss_bbox_dn: 0.02548  loss_giou_dn: 0.1444  loss_ce_0: 0.9107  loss_mask_0: 0.04995  loss_dice_0: 0.4156  loss_bbox_0: 0.06075  loss_giou_0: 0.3696  loss_ce_dn_0: 1.838  loss_mask_dn_0: 0.4081  loss_dice_dn_0: 2.902  loss_bbox_dn_0: 0.3312  loss_giou_dn_0: 0.856  loss_ce_1: 0.8464  loss_mask_1: 0.0546  loss_dice_1: 0.4054  loss_bbox_1: 0.06307  loss_giou_1: 0.2526  loss_ce_dn_1: 0.2525  loss_mask_dn_1: 0.04491  loss_dice_dn_1: 0.3705  loss_bbox_dn_1: 0.06296  loss_giou_dn_1: 0.2538  loss_ce_2: 0.7163  loss_mask_2: 0.05552  loss_dice_2: 0.3663  loss_bbox_2: 0.04497  loss_giou_2: 0.2276  loss_ce_dn_2: 0.1778  loss_mask_dn_2: 0.0402  loss_dice_dn_2: 0.3543  loss_bbox_dn_2: 0.03696  loss_giou_dn_2: 0.1841  loss_ce_3: 0.6249  loss_mask_3: 0.05334  loss_dice_3: 0.4007  loss_bbox_3: 0.03942  loss_giou_3: 0.2215  loss_ce_dn_3: 0.1469  loss_mask_dn_3: 0.04206  loss_dice_dn_3: 0.343  loss_bbox_dn_3: 0.03101  loss_giou_dn_3: 0.1668  loss_ce_4: 0.5846  loss_mask_4: 0.05321  loss_dice_4: 0.365  loss_bbox_4: 0.04201  loss_giou_4: 0.2129  loss_ce_dn_4: 0.1343  loss_mask_dn_4: 0.04241  loss_dice_dn_4: 0.3406  loss_bbox_dn_4: 0.02916  loss_giou_dn_4: 0.1544  loss_ce_5: 0.5629  loss_mask_5: 0.04868  loss_dice_5: 0.3319  loss_bbox_5: 0.03633  loss_giou_5: 0.2108  loss_ce_dn_5: 0.1242  loss_mask_dn_5: 0.0458  loss_dice_dn_5: 0.3566  loss_bbox_dn_5: 0.02797  loss_giou_dn_5: 0.1503  loss_ce_6: 0.5363  loss_mask_6: 0.0523  loss_dice_6: 0.4036  loss_bbox_6: 0.03762  loss_giou_6: 0.2063  loss_ce_dn_6: 0.118  loss_mask_dn_6: 0.04483  loss_dice_dn_6: 0.3524  loss_bbox_dn_6: 0.02655  loss_giou_dn_6: 0.1486  loss_ce_7: 0.5232  loss_mask_7: 0.04852  loss_dice_7: 0.3722  loss_bbox_7: 0.03724  loss_giou_7: 0.2039  loss_ce_dn_7: 0.121  loss_mask_dn_7: 0.04287  loss_dice_dn_7: 0.3497  loss_bbox_dn_7: 0.02599  loss_giou_dn_7: 0.1484  loss_ce_8: 0.5147  loss_mask_8: 0.04738  loss_dice_8: 0.4042  loss_bbox_8: 0.03184  loss_giou_8: 0.1945  loss_ce_dn_8: 0.1199  loss_mask_dn_8: 0.0451  loss_dice_dn_8: 0.3463  loss_bbox_dn_8: 0.02554  loss_giou_dn_8: 0.1446  loss_ce_interm: 0.908  loss_mask_interm: 0.04289  loss_dice_interm: 0.3912  loss_bbox_interm: 0.06501  loss_giou_interm: 0.2945    time: 0.7902  last_time: 0.7703  data_time: 0.0169  last_data_time: 0.0049   lr: 1e-06  max_mem: 20850M\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[07/13 11:30:54 d2.utils.events]: \u001b[0m eta: 0:12:33  iter: 1439  total_loss: 34.49  loss_ce: 0.6593  loss_mask: 0.04811  loss_dice: 0.4682  loss_bbox: 0.0554  loss_giou: 0.2422  loss_ce_dn: 0.1696  loss_mask_dn: 0.0421  loss_dice_dn: 0.355  loss_bbox_dn: 0.03138  loss_giou_dn: 0.1462  loss_ce_0: 1.039  loss_mask_0: 0.05102  loss_dice_0: 0.4284  loss_bbox_0: 0.07617  loss_giou_0: 0.3207  loss_ce_dn_0: 1.782  loss_mask_dn_0: 0.5479  loss_dice_dn_0: 2.956  loss_bbox_dn_0: 0.399  loss_giou_dn_0: 0.8534  loss_ce_1: 0.9174  loss_mask_1: 0.0581  loss_dice_1: 0.4265  loss_bbox_1: 0.0552  loss_giou_1: 0.2451  loss_ce_dn_1: 0.2858  loss_mask_dn_1: 0.05281  loss_dice_dn_1: 0.3966  loss_bbox_dn_1: 0.08182  loss_giou_dn_1: 0.2707  loss_ce_2: 0.8087  loss_mask_2: 0.0527  loss_dice_2: 0.4372  loss_bbox_2: 0.06719  loss_giou_2: 0.2547  loss_ce_dn_2: 0.2175  loss_mask_dn_2: 0.04685  loss_dice_dn_2: 0.3621  loss_bbox_dn_2: 0.04539  loss_giou_dn_2: 0.1903  loss_ce_3: 0.6946  loss_mask_3: 0.04866  loss_dice_3: 0.4828  loss_bbox_3: 0.05492  loss_giou_3: 0.2477  loss_ce_dn_3: 0.201  loss_mask_dn_3: 0.04259  loss_dice_dn_3: 0.3563  loss_bbox_dn_3: 0.03748  loss_giou_dn_3: 0.1675  loss_ce_4: 0.6848  loss_mask_4: 0.05041  loss_dice_4: 0.4711  loss_bbox_4: 0.05661  loss_giou_4: 0.2692  loss_ce_dn_4: 0.1786  loss_mask_dn_4: 0.04304  loss_dice_dn_4: 0.3592  loss_bbox_dn_4: 0.03483  loss_giou_dn_4: 0.1558  loss_ce_5: 0.7146  loss_mask_5: 0.05112  loss_dice_5: 0.4534  loss_bbox_5: 0.05588  loss_giou_5: 0.2658  loss_ce_dn_5: 0.1738  loss_mask_dn_5: 0.04271  loss_dice_dn_5: 0.3432  loss_bbox_dn_5: 0.03286  loss_giou_dn_5: 0.1486  loss_ce_6: 0.6746  loss_mask_6: 0.05021  loss_dice_6: 0.457  loss_bbox_6: 0.05264  loss_giou_6: 0.255  loss_ce_dn_6: 0.1654  loss_mask_dn_6: 0.04297  loss_dice_dn_6: 0.3427  loss_bbox_dn_6: 0.03185  loss_giou_dn_6: 0.1477  loss_ce_7: 0.6535  loss_mask_7: 0.04842  loss_dice_7: 0.4617  loss_bbox_7: 0.05546  loss_giou_7: 0.233  loss_ce_dn_7: 0.1632  loss_mask_dn_7: 0.04274  loss_dice_dn_7: 0.3367  loss_bbox_dn_7: 0.03145  loss_giou_dn_7: 0.1483  loss_ce_8: 0.6502  loss_mask_8: 0.05114  loss_dice_8: 0.4479  loss_bbox_8: 0.05576  loss_giou_8: 0.2411  loss_ce_dn_8: 0.1648  loss_mask_dn_8: 0.04216  loss_dice_dn_8: 0.3499  loss_bbox_dn_8: 0.03155  loss_giou_dn_8: 0.1455  loss_ce_interm: 1.043  loss_mask_interm: 0.05173  loss_dice_interm: 0.4332  loss_bbox_interm: 0.08612  loss_giou_interm: 0.3017    time: 0.7902  last_time: 0.7920  data_time: 0.0149  last_data_time: 0.0127   lr: 1e-06  max_mem: 20850M\n",
      "\u001b[32m[07/13 11:31:09 d2.utils.events]: \u001b[0m eta: 0:12:17  iter: 1459  total_loss: 32.23  loss_ce: 0.5144  loss_mask: 0.04216  loss_dice: 0.4237  loss_bbox: 0.03507  loss_giou: 0.2376  loss_ce_dn: 0.1066  loss_mask_dn: 0.03556  loss_dice_dn: 0.3669  loss_bbox_dn: 0.0293  loss_giou_dn: 0.1617  loss_ce_0: 0.9716  loss_mask_0: 0.03946  loss_dice_0: 0.4696  loss_bbox_0: 0.05692  loss_giou_0: 0.3156  loss_ce_dn_0: 1.772  loss_mask_dn_0: 0.3671  loss_dice_dn_0: 2.842  loss_bbox_dn_0: 0.3399  loss_giou_dn_0: 0.8528  loss_ce_1: 0.8136  loss_mask_1: 0.04016  loss_dice_1: 0.4814  loss_bbox_1: 0.04137  loss_giou_1: 0.2749  loss_ce_dn_1: 0.2767  loss_mask_dn_1: 0.0441  loss_dice_dn_1: 0.4172  loss_bbox_dn_1: 0.06719  loss_giou_dn_1: 0.2574  loss_ce_2: 0.6967  loss_mask_2: 0.04353  loss_dice_2: 0.4752  loss_bbox_2: 0.04146  loss_giou_2: 0.2523  loss_ce_dn_2: 0.1978  loss_mask_dn_2: 0.0374  loss_dice_dn_2: 0.3793  loss_bbox_dn_2: 0.04026  loss_giou_dn_2: 0.1848  loss_ce_3: 0.6557  loss_mask_3: 0.04599  loss_dice_3: 0.4674  loss_bbox_3: 0.03749  loss_giou_3: 0.2409  loss_ce_dn_3: 0.1665  loss_mask_dn_3: 0.03604  loss_dice_dn_3: 0.3783  loss_bbox_dn_3: 0.0329  loss_giou_dn_3: 0.1692  loss_ce_4: 0.6274  loss_mask_4: 0.03938  loss_dice_4: 0.4724  loss_bbox_4: 0.04017  loss_giou_4: 0.2494  loss_ce_dn_4: 0.1376  loss_mask_dn_4: 0.03483  loss_dice_dn_4: 0.3691  loss_bbox_dn_4: 0.0314  loss_giou_dn_4: 0.1607  loss_ce_5: 0.5865  loss_mask_5: 0.03857  loss_dice_5: 0.4934  loss_bbox_5: 0.037  loss_giou_5: 0.2353  loss_ce_dn_5: 0.1256  loss_mask_dn_5: 0.03526  loss_dice_dn_5: 0.3686  loss_bbox_dn_5: 0.03085  loss_giou_dn_5: 0.1567  loss_ce_6: 0.5636  loss_mask_6: 0.0405  loss_dice_6: 0.4749  loss_bbox_6: 0.04424  loss_giou_6: 0.2371  loss_ce_dn_6: 0.1175  loss_mask_dn_6: 0.03437  loss_dice_dn_6: 0.3854  loss_bbox_dn_6: 0.02993  loss_giou_dn_6: 0.1598  loss_ce_7: 0.5672  loss_mask_7: 0.04003  loss_dice_7: 0.4091  loss_bbox_7: 0.03683  loss_giou_7: 0.2494  loss_ce_dn_7: 0.1087  loss_mask_dn_7: 0.03484  loss_dice_dn_7: 0.3825  loss_bbox_dn_7: 0.02985  loss_giou_dn_7: 0.1581  loss_ce_8: 0.5388  loss_mask_8: 0.04061  loss_dice_8: 0.4325  loss_bbox_8: 0.03516  loss_giou_8: 0.2317  loss_ce_dn_8: 0.1075  loss_mask_dn_8: 0.03567  loss_dice_dn_8: 0.3771  loss_bbox_dn_8: 0.02943  loss_giou_dn_8: 0.1613  loss_ce_interm: 0.9606  loss_mask_interm: 0.0398  loss_dice_interm: 0.4949  loss_bbox_interm: 0.07675  loss_giou_interm: 0.3268    time: 0.7903  last_time: 0.7804  data_time: 0.0189  last_data_time: 0.0077   lr: 1e-06  max_mem: 20850M\n",
      "\u001b[32m[07/13 11:31:26 d2.utils.events]: \u001b[0m eta: 0:12:02  iter: 1479  total_loss: 37.12  loss_ce: 0.6607  loss_mask: 0.04476  loss_dice: 0.4971  loss_bbox: 0.05515  loss_giou: 0.3147  loss_ce_dn: 0.1303  loss_mask_dn: 0.0408  loss_dice_dn: 0.4199  loss_bbox_dn: 0.02967  loss_giou_dn: 0.1776  loss_ce_0: 0.9832  loss_mask_0: 0.04723  loss_dice_0: 0.4498  loss_bbox_0: 0.09639  loss_giou_0: 0.4449  loss_ce_dn_0: 1.832  loss_mask_dn_0: 0.4247  loss_dice_dn_0: 2.855  loss_bbox_dn_0: 0.3044  loss_giou_dn_0: 0.8536  loss_ce_1: 0.9241  loss_mask_1: 0.05234  loss_dice_1: 0.4911  loss_bbox_1: 0.05921  loss_giou_1: 0.3488  loss_ce_dn_1: 0.2797  loss_mask_dn_1: 0.04344  loss_dice_dn_1: 0.48  loss_bbox_dn_1: 0.06396  loss_giou_dn_1: 0.2846  loss_ce_2: 0.7747  loss_mask_2: 0.04936  loss_dice_2: 0.5269  loss_bbox_2: 0.06317  loss_giou_2: 0.3316  loss_ce_dn_2: 0.1952  loss_mask_dn_2: 0.04237  loss_dice_dn_2: 0.4418  loss_bbox_dn_2: 0.04004  loss_giou_dn_2: 0.2143  loss_ce_3: 0.7544  loss_mask_3: 0.05411  loss_dice_3: 0.5068  loss_bbox_3: 0.06106  loss_giou_3: 0.3234  loss_ce_dn_3: 0.1608  loss_mask_dn_3: 0.0424  loss_dice_dn_3: 0.4178  loss_bbox_dn_3: 0.03395  loss_giou_dn_3: 0.1851  loss_ce_4: 0.7009  loss_mask_4: 0.04699  loss_dice_4: 0.4668  loss_bbox_4: 0.0587  loss_giou_4: 0.3119  loss_ce_dn_4: 0.1467  loss_mask_dn_4: 0.04261  loss_dice_dn_4: 0.4266  loss_bbox_dn_4: 0.03123  loss_giou_dn_4: 0.1813  loss_ce_5: 0.6765  loss_mask_5: 0.04324  loss_dice_5: 0.4649  loss_bbox_5: 0.05723  loss_giou_5: 0.2962  loss_ce_dn_5: 0.1407  loss_mask_dn_5: 0.04097  loss_dice_dn_5: 0.4128  loss_bbox_dn_5: 0.03088  loss_giou_dn_5: 0.1765  loss_ce_6: 0.6777  loss_mask_6: 0.04361  loss_dice_6: 0.4537  loss_bbox_6: 0.05643  loss_giou_6: 0.307  loss_ce_dn_6: 0.1385  loss_mask_dn_6: 0.04028  loss_dice_dn_6: 0.4213  loss_bbox_dn_6: 0.03023  loss_giou_dn_6: 0.1766  loss_ce_7: 0.6528  loss_mask_7: 0.04342  loss_dice_7: 0.515  loss_bbox_7: 0.05512  loss_giou_7: 0.3108  loss_ce_dn_7: 0.1338  loss_mask_dn_7: 0.04032  loss_dice_dn_7: 0.4174  loss_bbox_dn_7: 0.03016  loss_giou_dn_7: 0.1761  loss_ce_8: 0.6461  loss_mask_8: 0.04394  loss_dice_8: 0.458  loss_bbox_8: 0.05995  loss_giou_8: 0.2953  loss_ce_dn_8: 0.1316  loss_mask_dn_8: 0.04057  loss_dice_dn_8: 0.4213  loss_bbox_dn_8: 0.02972  loss_giou_dn_8: 0.1774  loss_ce_interm: 0.999  loss_mask_interm: 0.04505  loss_dice_interm: 0.4465  loss_bbox_interm: 0.08068  loss_giou_interm: 0.3866    time: 0.7905  last_time: 0.7845  data_time: 0.0186  last_data_time: 0.0081   lr: 1e-06  max_mem: 20850M\n",
      "\u001b[32m[07/13 11:31:41 d2.utils.events]: \u001b[0m eta: 0:11:46  iter: 1499  total_loss: 27.88  loss_ce: 0.5355  loss_mask: 0.04407  loss_dice: 0.3335  loss_bbox: 0.03102  loss_giou: 0.1609  loss_ce_dn: 0.1189  loss_mask_dn: 0.0366  loss_dice_dn: 0.2873  loss_bbox_dn: 0.02834  loss_giou_dn: 0.1173  loss_ce_0: 0.9531  loss_mask_0: 0.04022  loss_dice_0: 0.3253  loss_bbox_0: 0.05046  loss_giou_0: 0.2751  loss_ce_dn_0: 1.79  loss_mask_dn_0: 0.4923  loss_dice_dn_0: 2.685  loss_bbox_dn_0: 0.3369  loss_giou_dn_0: 0.8497  loss_ce_1: 0.8507  loss_mask_1: 0.0455  loss_dice_1: 0.3242  loss_bbox_1: 0.04127  loss_giou_1: 0.2003  loss_ce_dn_1: 0.2878  loss_mask_dn_1: 0.0425  loss_dice_dn_1: 0.3494  loss_bbox_dn_1: 0.066  loss_giou_dn_1: 0.2453  loss_ce_2: 0.71  loss_mask_2: 0.04353  loss_dice_2: 0.3293  loss_bbox_2: 0.03843  loss_giou_2: 0.1902  loss_ce_dn_2: 0.2112  loss_mask_dn_2: 0.03657  loss_dice_dn_2: 0.2969  loss_bbox_dn_2: 0.04092  loss_giou_dn_2: 0.1601  loss_ce_3: 0.6283  loss_mask_3: 0.0424  loss_dice_3: 0.3452  loss_bbox_3: 0.03473  loss_giou_3: 0.1695  loss_ce_dn_3: 0.1678  loss_mask_dn_3: 0.03603  loss_dice_dn_3: 0.2879  loss_bbox_dn_3: 0.0353  loss_giou_dn_3: 0.133  loss_ce_4: 0.5916  loss_mask_4: 0.0416  loss_dice_4: 0.3204  loss_bbox_4: 0.0333  loss_giou_4: 0.1683  loss_ce_dn_4: 0.1493  loss_mask_dn_4: 0.03646  loss_dice_dn_4: 0.2904  loss_bbox_dn_4: 0.03209  loss_giou_dn_4: 0.1243  loss_ce_5: 0.5861  loss_mask_5: 0.03894  loss_dice_5: 0.3154  loss_bbox_5: 0.0324  loss_giou_5: 0.1635  loss_ce_dn_5: 0.1369  loss_mask_dn_5: 0.03495  loss_dice_dn_5: 0.2681  loss_bbox_dn_5: 0.03006  loss_giou_dn_5: 0.1217  loss_ce_6: 0.5696  loss_mask_6: 0.04028  loss_dice_6: 0.3365  loss_bbox_6: 0.03186  loss_giou_6: 0.159  loss_ce_dn_6: 0.1231  loss_mask_dn_6: 0.03585  loss_dice_dn_6: 0.2819  loss_bbox_dn_6: 0.02952  loss_giou_dn_6: 0.1184  loss_ce_7: 0.5494  loss_mask_7: 0.04183  loss_dice_7: 0.3355  loss_bbox_7: 0.0322  loss_giou_7: 0.1624  loss_ce_dn_7: 0.123  loss_mask_dn_7: 0.03721  loss_dice_dn_7: 0.2831  loss_bbox_dn_7: 0.02989  loss_giou_dn_7: 0.118  loss_ce_8: 0.536  loss_mask_8: 0.04119  loss_dice_8: 0.3029  loss_bbox_8: 0.03286  loss_giou_8: 0.1601  loss_ce_dn_8: 0.1167  loss_mask_dn_8: 0.03697  loss_dice_dn_8: 0.279  loss_bbox_dn_8: 0.02833  loss_giou_dn_8: 0.1163  loss_ce_interm: 0.9579  loss_mask_interm: 0.04014  loss_dice_interm: 0.3238  loss_bbox_interm: 0.07831  loss_giou_interm: 0.2582    time: 0.7905  last_time: 0.7777  data_time: 0.0165  last_data_time: 0.0074   lr: 1e-06  max_mem: 20850M\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[07/13 11:31:57 d2.utils.events]: \u001b[0m eta: 0:11:30  iter: 1519  total_loss: 22.76  loss_ce: 0.4168  loss_mask: 0.05527  loss_dice: 0.2486  loss_bbox: 0.036  loss_giou: 0.1184  loss_ce_dn: 0.08531  loss_mask_dn: 0.05942  loss_dice_dn: 0.2342  loss_bbox_dn: 0.03321  loss_giou_dn: 0.1134  loss_ce_0: 0.8847  loss_mask_0: 0.0604  loss_dice_0: 0.2714  loss_bbox_0: 0.04984  loss_giou_0: 0.2006  loss_ce_dn_0: 1.771  loss_mask_dn_0: 0.7653  loss_dice_dn_0: 2.49  loss_bbox_dn_0: 0.4456  loss_giou_dn_0: 0.8499  loss_ce_1: 0.706  loss_mask_1: 0.06289  loss_dice_1: 0.2686  loss_bbox_1: 0.04422  loss_giou_1: 0.1524  loss_ce_dn_1: 0.2276  loss_mask_dn_1: 0.06097  loss_dice_dn_1: 0.2755  loss_bbox_dn_1: 0.09357  loss_giou_dn_1: 0.2257  loss_ce_2: 0.5801  loss_mask_2: 0.06056  loss_dice_2: 0.2536  loss_bbox_2: 0.04199  loss_giou_2: 0.1336  loss_ce_dn_2: 0.1539  loss_mask_dn_2: 0.05632  loss_dice_dn_2: 0.253  loss_bbox_dn_2: 0.05207  loss_giou_dn_2: 0.1457  loss_ce_3: 0.5231  loss_mask_3: 0.06261  loss_dice_3: 0.2805  loss_bbox_3: 0.03911  loss_giou_3: 0.124  loss_ce_dn_3: 0.1266  loss_mask_dn_3: 0.05465  loss_dice_dn_3: 0.2302  loss_bbox_dn_3: 0.04091  loss_giou_dn_3: 0.1239  loss_ce_4: 0.4841  loss_mask_4: 0.06449  loss_dice_4: 0.2488  loss_bbox_4: 0.03857  loss_giou_4: 0.1219  loss_ce_dn_4: 0.1136  loss_mask_dn_4: 0.05751  loss_dice_dn_4: 0.2302  loss_bbox_dn_4: 0.03754  loss_giou_dn_4: 0.115  loss_ce_5: 0.4598  loss_mask_5: 0.05788  loss_dice_5: 0.2645  loss_bbox_5: 0.03735  loss_giou_5: 0.1191  loss_ce_dn_5: 0.1037  loss_mask_dn_5: 0.05886  loss_dice_dn_5: 0.2409  loss_bbox_dn_5: 0.03505  loss_giou_dn_5: 0.1146  loss_ce_6: 0.4349  loss_mask_6: 0.05862  loss_dice_6: 0.2664  loss_bbox_6: 0.03583  loss_giou_6: 0.1143  loss_ce_dn_6: 0.09351  loss_mask_dn_6: 0.05968  loss_dice_dn_6: 0.2309  loss_bbox_dn_6: 0.0343  loss_giou_dn_6: 0.1123  loss_ce_7: 0.4149  loss_mask_7: 0.05831  loss_dice_7: 0.2408  loss_bbox_7: 0.03621  loss_giou_7: 0.1217  loss_ce_dn_7: 0.09052  loss_mask_dn_7: 0.05913  loss_dice_dn_7: 0.227  loss_bbox_dn_7: 0.03261  loss_giou_dn_7: 0.1128  loss_ce_8: 0.4166  loss_mask_8: 0.0592  loss_dice_8: 0.2285  loss_bbox_8: 0.03584  loss_giou_8: 0.1181  loss_ce_dn_8: 0.08847  loss_mask_dn_8: 0.06009  loss_dice_dn_8: 0.2276  loss_bbox_dn_8: 0.03309  loss_giou_dn_8: 0.1124  loss_ce_interm: 0.8542  loss_mask_interm: 0.05783  loss_dice_interm: 0.2331  loss_bbox_interm: 0.08342  loss_giou_interm: 0.2232    time: 0.7904  last_time: 0.7829  data_time: 0.0107  last_data_time: 0.0080   lr: 1e-06  max_mem: 20850M\n",
      "\u001b[32m[07/13 11:32:13 d2.utils.events]: \u001b[0m eta: 0:11:15  iter: 1539  total_loss: 29.43  loss_ce: 0.5428  loss_mask: 0.04142  loss_dice: 0.4179  loss_bbox: 0.05216  loss_giou: 0.2414  loss_ce_dn: 0.1083  loss_mask_dn: 0.0365  loss_dice_dn: 0.3595  loss_bbox_dn: 0.02461  loss_giou_dn: 0.1545  loss_ce_0: 0.9858  loss_mask_0: 0.04292  loss_dice_0: 0.4572  loss_bbox_0: 0.06544  loss_giou_0: 0.3645  loss_ce_dn_0: 1.776  loss_mask_dn_0: 0.433  loss_dice_dn_0: 2.797  loss_bbox_dn_0: 0.3367  loss_giou_dn_0: 0.8505  loss_ce_1: 0.8406  loss_mask_1: 0.04138  loss_dice_1: 0.4644  loss_bbox_1: 0.05415  loss_giou_1: 0.2816  loss_ce_dn_1: 0.2736  loss_mask_dn_1: 0.03891  loss_dice_dn_1: 0.4043  loss_bbox_dn_1: 0.06934  loss_giou_dn_1: 0.2692  loss_ce_2: 0.7025  loss_mask_2: 0.04137  loss_dice_2: 0.3855  loss_bbox_2: 0.05416  loss_giou_2: 0.2672  loss_ce_dn_2: 0.1872  loss_mask_dn_2: 0.03645  loss_dice_dn_2: 0.3657  loss_bbox_dn_2: 0.04091  loss_giou_dn_2: 0.201  loss_ce_3: 0.6323  loss_mask_3: 0.0418  loss_dice_3: 0.445  loss_bbox_3: 0.05473  loss_giou_3: 0.2472  loss_ce_dn_3: 0.1476  loss_mask_dn_3: 0.03601  loss_dice_dn_3: 0.3602  loss_bbox_dn_3: 0.03206  loss_giou_dn_3: 0.1764  loss_ce_4: 0.5681  loss_mask_4: 0.04272  loss_dice_4: 0.4065  loss_bbox_4: 0.04931  loss_giou_4: 0.2483  loss_ce_dn_4: 0.1303  loss_mask_dn_4: 0.03483  loss_dice_dn_4: 0.3646  loss_bbox_dn_4: 0.0289  loss_giou_dn_4: 0.1656  loss_ce_5: 0.5591  loss_mask_5: 0.04022  loss_dice_5: 0.4224  loss_bbox_5: 0.04878  loss_giou_5: 0.2458  loss_ce_dn_5: 0.1188  loss_mask_dn_5: 0.03528  loss_dice_dn_5: 0.3562  loss_bbox_dn_5: 0.02659  loss_giou_dn_5: 0.1602  loss_ce_6: 0.509  loss_mask_6: 0.03919  loss_dice_6: 0.3768  loss_bbox_6: 0.05009  loss_giou_6: 0.2321  loss_ce_dn_6: 0.1117  loss_mask_dn_6: 0.0351  loss_dice_dn_6: 0.3622  loss_bbox_dn_6: 0.02546  loss_giou_dn_6: 0.1569  loss_ce_7: 0.5384  loss_mask_7: 0.0415  loss_dice_7: 0.3882  loss_bbox_7: 0.0501  loss_giou_7: 0.2309  loss_ce_dn_7: 0.1077  loss_mask_dn_7: 0.03558  loss_dice_dn_7: 0.3402  loss_bbox_dn_7: 0.02527  loss_giou_dn_7: 0.1567  loss_ce_8: 0.533  loss_mask_8: 0.04094  loss_dice_8: 0.4135  loss_bbox_8: 0.04958  loss_giou_8: 0.232  loss_ce_dn_8: 0.1089  loss_mask_dn_8: 0.03533  loss_dice_dn_8: 0.3528  loss_bbox_dn_8: 0.0245  loss_giou_dn_8: 0.1545  loss_ce_interm: 0.9744  loss_mask_interm: 0.04147  loss_dice_interm: 0.4132  loss_bbox_interm: 0.07333  loss_giou_interm: 0.3557    time: 0.7904  last_time: 0.7885  data_time: 0.0187  last_data_time: 0.0071   lr: 1e-06  max_mem: 20850M\n",
      "\u001b[32m[07/13 11:32:29 d2.utils.events]: \u001b[0m eta: 0:10:59  iter: 1559  total_loss: 28.38  loss_ce: 0.5506  loss_mask: 0.04789  loss_dice: 0.3606  loss_bbox: 0.03794  loss_giou: 0.1527  loss_ce_dn: 0.1201  loss_mask_dn: 0.03587  loss_dice_dn: 0.373  loss_bbox_dn: 0.028  loss_giou_dn: 0.128  loss_ce_0: 0.9694  loss_mask_0: 0.05037  loss_dice_0: 0.402  loss_bbox_0: 0.05498  loss_giou_0: 0.2921  loss_ce_dn_0: 1.8  loss_mask_dn_0: 0.5101  loss_dice_dn_0: 2.696  loss_bbox_dn_0: 0.3609  loss_giou_dn_0: 0.8555  loss_ce_1: 0.8757  loss_mask_1: 0.04907  loss_dice_1: 0.3608  loss_bbox_1: 0.04361  loss_giou_1: 0.1922  loss_ce_dn_1: 0.2618  loss_mask_dn_1: 0.04089  loss_dice_dn_1: 0.3982  loss_bbox_dn_1: 0.07146  loss_giou_dn_1: 0.2489  loss_ce_2: 0.6856  loss_mask_2: 0.04801  loss_dice_2: 0.3623  loss_bbox_2: 0.04395  loss_giou_2: 0.1707  loss_ce_dn_2: 0.1792  loss_mask_dn_2: 0.0386  loss_dice_dn_2: 0.3816  loss_bbox_dn_2: 0.04392  loss_giou_dn_2: 0.1724  loss_ce_3: 0.6428  loss_mask_3: 0.04751  loss_dice_3: 0.3564  loss_bbox_3: 0.04116  loss_giou_3: 0.1537  loss_ce_dn_3: 0.1568  loss_mask_dn_3: 0.03824  loss_dice_dn_3: 0.3717  loss_bbox_dn_3: 0.03438  loss_giou_dn_3: 0.1425  loss_ce_4: 0.5844  loss_mask_4: 0.05359  loss_dice_4: 0.3522  loss_bbox_4: 0.03864  loss_giou_4: 0.1628  loss_ce_dn_4: 0.1388  loss_mask_dn_4: 0.0377  loss_dice_dn_4: 0.3747  loss_bbox_dn_4: 0.03186  loss_giou_dn_4: 0.1349  loss_ce_5: 0.5909  loss_mask_5: 0.04835  loss_dice_5: 0.3285  loss_bbox_5: 0.03779  loss_giou_5: 0.1514  loss_ce_dn_5: 0.1318  loss_mask_dn_5: 0.03708  loss_dice_dn_5: 0.3698  loss_bbox_dn_5: 0.02955  loss_giou_dn_5: 0.132  loss_ce_6: 0.5625  loss_mask_6: 0.04781  loss_dice_6: 0.3288  loss_bbox_6: 0.03537  loss_giou_6: 0.154  loss_ce_dn_6: 0.1257  loss_mask_dn_6: 0.0358  loss_dice_dn_6: 0.3649  loss_bbox_dn_6: 0.02816  loss_giou_dn_6: 0.1315  loss_ce_7: 0.5545  loss_mask_7: 0.04606  loss_dice_7: 0.3504  loss_bbox_7: 0.03578  loss_giou_7: 0.1543  loss_ce_dn_7: 0.1198  loss_mask_dn_7: 0.03665  loss_dice_dn_7: 0.385  loss_bbox_dn_7: 0.02813  loss_giou_dn_7: 0.1308  loss_ce_8: 0.5536  loss_mask_8: 0.04533  loss_dice_8: 0.3307  loss_bbox_8: 0.03996  loss_giou_8: 0.1535  loss_ce_dn_8: 0.1205  loss_mask_dn_8: 0.0364  loss_dice_dn_8: 0.3693  loss_bbox_dn_8: 0.02775  loss_giou_dn_8: 0.1288  loss_ce_interm: 0.8883  loss_mask_interm: 0.04661  loss_dice_interm: 0.3638  loss_bbox_interm: 0.07966  loss_giou_interm: 0.2904    time: 0.7903  last_time: 0.7853  data_time: 0.0131  last_data_time: 0.0085   lr: 1e-06  max_mem: 20850M\n",
      "\u001b[32m[07/13 11:32:45 d2.utils.events]: \u001b[0m eta: 0:10:43  iter: 1579  total_loss: 31.25  loss_ce: 0.5876  loss_mask: 0.04954  loss_dice: 0.4273  loss_bbox: 0.04426  loss_giou: 0.236  loss_ce_dn: 0.1192  loss_mask_dn: 0.04722  loss_dice_dn: 0.4089  loss_bbox_dn: 0.03276  loss_giou_dn: 0.1758  loss_ce_0: 0.9919  loss_mask_0: 0.0445  loss_dice_0: 0.455  loss_bbox_0: 0.07219  loss_giou_0: 0.3768  loss_ce_dn_0: 1.702  loss_mask_dn_0: 0.4615  loss_dice_dn_0: 2.784  loss_bbox_dn_0: 0.3189  loss_giou_dn_0: 0.8543  loss_ce_1: 0.8765  loss_mask_1: 0.05669  loss_dice_1: 0.4387  loss_bbox_1: 0.0573  loss_giou_1: 0.2941  loss_ce_dn_1: 0.2858  loss_mask_dn_1: 0.05154  loss_dice_dn_1: 0.4388  loss_bbox_dn_1: 0.07539  loss_giou_dn_1: 0.2878  loss_ce_2: 0.7498  loss_mask_2: 0.04951  loss_dice_2: 0.4452  loss_bbox_2: 0.04829  loss_giou_2: 0.2744  loss_ce_dn_2: 0.1894  loss_mask_dn_2: 0.04882  loss_dice_dn_2: 0.4116  loss_bbox_dn_2: 0.05058  loss_giou_dn_2: 0.2205  loss_ce_3: 0.6636  loss_mask_3: 0.04742  loss_dice_3: 0.4596  loss_bbox_3: 0.04377  loss_giou_3: 0.2662  loss_ce_dn_3: 0.1611  loss_mask_dn_3: 0.04901  loss_dice_dn_3: 0.4107  loss_bbox_dn_3: 0.04218  loss_giou_dn_3: 0.1951  loss_ce_4: 0.6613  loss_mask_4: 0.04781  loss_dice_4: 0.4749  loss_bbox_4: 0.0447  loss_giou_4: 0.2591  loss_ce_dn_4: 0.1449  loss_mask_dn_4: 0.04639  loss_dice_dn_4: 0.3944  loss_bbox_dn_4: 0.03881  loss_giou_dn_4: 0.1848  loss_ce_5: 0.6216  loss_mask_5: 0.04378  loss_dice_5: 0.4762  loss_bbox_5: 0.04371  loss_giou_5: 0.2502  loss_ce_dn_5: 0.1317  loss_mask_dn_5: 0.0455  loss_dice_dn_5: 0.3986  loss_bbox_dn_5: 0.03586  loss_giou_dn_5: 0.1811  loss_ce_6: 0.5767  loss_mask_6: 0.04802  loss_dice_6: 0.415  loss_bbox_6: 0.04473  loss_giou_6: 0.242  loss_ce_dn_6: 0.1242  loss_mask_dn_6: 0.0466  loss_dice_dn_6: 0.4047  loss_bbox_dn_6: 0.03307  loss_giou_dn_6: 0.1809  loss_ce_7: 0.5703  loss_mask_7: 0.048  loss_dice_7: 0.4254  loss_bbox_7: 0.04416  loss_giou_7: 0.2383  loss_ce_dn_7: 0.121  loss_mask_dn_7: 0.04732  loss_dice_dn_7: 0.4091  loss_bbox_dn_7: 0.03349  loss_giou_dn_7: 0.1805  loss_ce_8: 0.5709  loss_mask_8: 0.04825  loss_dice_8: 0.4158  loss_bbox_8: 0.04499  loss_giou_8: 0.2385  loss_ce_dn_8: 0.117  loss_mask_dn_8: 0.04825  loss_dice_dn_8: 0.3951  loss_bbox_dn_8: 0.03271  loss_giou_dn_8: 0.1763  loss_ce_interm: 1.02  loss_mask_interm: 0.04273  loss_dice_interm: 0.4384  loss_bbox_interm: 0.07187  loss_giou_interm: 0.3373    time: 0.7904  last_time: 0.7735  data_time: 0.0116  last_data_time: 0.0044   lr: 1e-06  max_mem: 20850M\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[07/13 11:33:01 d2.utils.events]: \u001b[0m eta: 0:10:28  iter: 1599  total_loss: 27.36  loss_ce: 0.5044  loss_mask: 0.03814  loss_dice: 0.3479  loss_bbox: 0.03582  loss_giou: 0.1658  loss_ce_dn: 0.1058  loss_mask_dn: 0.04249  loss_dice_dn: 0.3194  loss_bbox_dn: 0.02876  loss_giou_dn: 0.1453  loss_ce_0: 0.9505  loss_mask_0: 0.04004  loss_dice_0: 0.3797  loss_bbox_0: 0.06091  loss_giou_0: 0.2828  loss_ce_dn_0: 1.773  loss_mask_dn_0: 0.3923  loss_dice_dn_0: 2.723  loss_bbox_dn_0: 0.3093  loss_giou_dn_0: 0.8502  loss_ce_1: 0.8023  loss_mask_1: 0.04002  loss_dice_1: 0.3866  loss_bbox_1: 0.04841  loss_giou_1: 0.2204  loss_ce_dn_1: 0.2488  loss_mask_dn_1: 0.04094  loss_dice_dn_1: 0.3648  loss_bbox_dn_1: 0.06952  loss_giou_dn_1: 0.2667  loss_ce_2: 0.6544  loss_mask_2: 0.03804  loss_dice_2: 0.3801  loss_bbox_2: 0.04016  loss_giou_2: 0.1939  loss_ce_dn_2: 0.1711  loss_mask_dn_2: 0.04195  loss_dice_dn_2: 0.3454  loss_bbox_dn_2: 0.04048  loss_giou_dn_2: 0.1783  loss_ce_3: 0.5717  loss_mask_3: 0.03833  loss_dice_3: 0.359  loss_bbox_3: 0.03912  loss_giou_3: 0.1858  loss_ce_dn_3: 0.1425  loss_mask_dn_3: 0.04251  loss_dice_dn_3: 0.3437  loss_bbox_dn_3: 0.03399  loss_giou_dn_3: 0.1614  loss_ce_4: 0.5019  loss_mask_4: 0.03722  loss_dice_4: 0.3947  loss_bbox_4: 0.03815  loss_giou_4: 0.1821  loss_ce_dn_4: 0.1299  loss_mask_dn_4: 0.04273  loss_dice_dn_4: 0.3368  loss_bbox_dn_4: 0.03115  loss_giou_dn_4: 0.1524  loss_ce_5: 0.4951  loss_mask_5: 0.03633  loss_dice_5: 0.3802  loss_bbox_5: 0.03663  loss_giou_5: 0.1866  loss_ce_dn_5: 0.1095  loss_mask_dn_5: 0.04258  loss_dice_dn_5: 0.3377  loss_bbox_dn_5: 0.02953  loss_giou_dn_5: 0.149  loss_ce_6: 0.4981  loss_mask_6: 0.03739  loss_dice_6: 0.35  loss_bbox_6: 0.03548  loss_giou_6: 0.1829  loss_ce_dn_6: 0.1105  loss_mask_dn_6: 0.04248  loss_dice_dn_6: 0.3433  loss_bbox_dn_6: 0.02857  loss_giou_dn_6: 0.1465  loss_ce_7: 0.4945  loss_mask_7: 0.03865  loss_dice_7: 0.3573  loss_bbox_7: 0.03655  loss_giou_7: 0.1828  loss_ce_dn_7: 0.103  loss_mask_dn_7: 0.04156  loss_dice_dn_7: 0.3361  loss_bbox_dn_7: 0.02931  loss_giou_dn_7: 0.1467  loss_ce_8: 0.4948  loss_mask_8: 0.0392  loss_dice_8: 0.3612  loss_bbox_8: 0.03491  loss_giou_8: 0.1805  loss_ce_dn_8: 0.1042  loss_mask_dn_8: 0.04179  loss_dice_dn_8: 0.3282  loss_bbox_dn_8: 0.0285  loss_giou_dn_8: 0.1454  loss_ce_interm: 0.9509  loss_mask_interm: 0.04147  loss_dice_interm: 0.3839  loss_bbox_interm: 0.07097  loss_giou_interm: 0.2834    time: 0.7903  last_time: 0.7805  data_time: 0.0150  last_data_time: 0.0076   lr: 1e-06  max_mem: 20850M\n",
      "\u001b[32m[07/13 11:33:16 d2.utils.events]: \u001b[0m eta: 0:10:12  iter: 1619  total_loss: 30.36  loss_ce: 0.5126  loss_mask: 0.05939  loss_dice: 0.394  loss_bbox: 0.0495  loss_giou: 0.2027  loss_ce_dn: 0.1452  loss_mask_dn: 0.05389  loss_dice_dn: 0.3849  loss_bbox_dn: 0.03067  loss_giou_dn: 0.1628  loss_ce_0: 0.9389  loss_mask_0: 0.06291  loss_dice_0: 0.3986  loss_bbox_0: 0.06257  loss_giou_0: 0.3213  loss_ce_dn_0: 1.783  loss_mask_dn_0: 0.4235  loss_dice_dn_0: 2.931  loss_bbox_dn_0: 0.3437  loss_giou_dn_0: 0.8482  loss_ce_1: 0.8584  loss_mask_1: 0.0691  loss_dice_1: 0.39  loss_bbox_1: 0.05123  loss_giou_1: 0.2367  loss_ce_dn_1: 0.2963  loss_mask_dn_1: 0.0692  loss_dice_dn_1: 0.3885  loss_bbox_dn_1: 0.06706  loss_giou_dn_1: 0.2676  loss_ce_2: 0.642  loss_mask_2: 0.06391  loss_dice_2: 0.4293  loss_bbox_2: 0.05129  loss_giou_2: 0.2269  loss_ce_dn_2: 0.2252  loss_mask_dn_2: 0.06056  loss_dice_dn_2: 0.392  loss_bbox_dn_2: 0.0443  loss_giou_dn_2: 0.203  loss_ce_3: 0.5965  loss_mask_3: 0.0591  loss_dice_3: 0.4225  loss_bbox_3: 0.04623  loss_giou_3: 0.2077  loss_ce_dn_3: 0.1907  loss_mask_dn_3: 0.05842  loss_dice_dn_3: 0.3676  loss_bbox_dn_3: 0.03418  loss_giou_dn_3: 0.182  loss_ce_4: 0.5545  loss_mask_4: 0.05979  loss_dice_4: 0.3766  loss_bbox_4: 0.04996  loss_giou_4: 0.1995  loss_ce_dn_4: 0.1669  loss_mask_dn_4: 0.05702  loss_dice_dn_4: 0.3741  loss_bbox_dn_4: 0.03303  loss_giou_dn_4: 0.1714  loss_ce_5: 0.535  loss_mask_5: 0.05928  loss_dice_5: 0.4106  loss_bbox_5: 0.05044  loss_giou_5: 0.2059  loss_ce_dn_5: 0.1559  loss_mask_dn_5: 0.05535  loss_dice_dn_5: 0.3665  loss_bbox_dn_5: 0.03164  loss_giou_dn_5: 0.1665  loss_ce_6: 0.5421  loss_mask_6: 0.05697  loss_dice_6: 0.3987  loss_bbox_6: 0.04763  loss_giou_6: 0.1996  loss_ce_dn_6: 0.1515  loss_mask_dn_6: 0.05379  loss_dice_dn_6: 0.3828  loss_bbox_dn_6: 0.03075  loss_giou_dn_6: 0.1648  loss_ce_7: 0.5147  loss_mask_7: 0.06254  loss_dice_7: 0.4097  loss_bbox_7: 0.04906  loss_giou_7: 0.2009  loss_ce_dn_7: 0.1463  loss_mask_dn_7: 0.05468  loss_dice_dn_7: 0.3658  loss_bbox_dn_7: 0.03074  loss_giou_dn_7: 0.1647  loss_ce_8: 0.5158  loss_mask_8: 0.05796  loss_dice_8: 0.3984  loss_bbox_8: 0.05044  loss_giou_8: 0.2023  loss_ce_dn_8: 0.1457  loss_mask_dn_8: 0.05402  loss_dice_dn_8: 0.3714  loss_bbox_dn_8: 0.03068  loss_giou_dn_8: 0.1626  loss_ce_interm: 0.9598  loss_mask_interm: 0.06215  loss_dice_interm: 0.4258  loss_bbox_interm: 0.07619  loss_giou_interm: 0.2798    time: 0.7903  last_time: 0.8026  data_time: 0.0120  last_data_time: 0.0291   lr: 1e-06  max_mem: 20850M\n",
      "\u001b[32m[07/13 11:33:32 d2.utils.events]: \u001b[0m eta: 0:09:56  iter: 1639  total_loss: 27.98  loss_ce: 0.4941  loss_mask: 0.05039  loss_dice: 0.3604  loss_bbox: 0.0318  loss_giou: 0.1896  loss_ce_dn: 0.09958  loss_mask_dn: 0.02783  loss_dice_dn: 0.3102  loss_bbox_dn: 0.02435  loss_giou_dn: 0.1508  loss_ce_0: 0.9047  loss_mask_0: 0.03835  loss_dice_0: 0.3864  loss_bbox_0: 0.05435  loss_giou_0: 0.3394  loss_ce_dn_0: 1.842  loss_mask_dn_0: 0.4647  loss_dice_dn_0: 2.645  loss_bbox_dn_0: 0.2797  loss_giou_dn_0: 0.852  loss_ce_1: 0.8085  loss_mask_1: 0.03969  loss_dice_1: 0.4374  loss_bbox_1: 0.0437  loss_giou_1: 0.2284  loss_ce_dn_1: 0.2427  loss_mask_dn_1: 0.03001  loss_dice_dn_1: 0.3874  loss_bbox_dn_1: 0.0554  loss_giou_dn_1: 0.2528  loss_ce_2: 0.6302  loss_mask_2: 0.04369  loss_dice_2: 0.3676  loss_bbox_2: 0.03436  loss_giou_2: 0.2061  loss_ce_dn_2: 0.1663  loss_mask_dn_2: 0.03052  loss_dice_dn_2: 0.326  loss_bbox_dn_2: 0.03871  loss_giou_dn_2: 0.1896  loss_ce_3: 0.5723  loss_mask_3: 0.04538  loss_dice_3: 0.3602  loss_bbox_3: 0.03105  loss_giou_3: 0.1973  loss_ce_dn_3: 0.1495  loss_mask_dn_3: 0.02818  loss_dice_dn_3: 0.3169  loss_bbox_dn_3: 0.03176  loss_giou_dn_3: 0.1679  loss_ce_4: 0.5394  loss_mask_4: 0.04834  loss_dice_4: 0.3612  loss_bbox_4: 0.03217  loss_giou_4: 0.2039  loss_ce_dn_4: 0.1343  loss_mask_dn_4: 0.02788  loss_dice_dn_4: 0.3118  loss_bbox_dn_4: 0.02928  loss_giou_dn_4: 0.1577  loss_ce_5: 0.5289  loss_mask_5: 0.0405  loss_dice_5: 0.3281  loss_bbox_5: 0.03119  loss_giou_5: 0.2047  loss_ce_dn_5: 0.118  loss_mask_dn_5: 0.02807  loss_dice_dn_5: 0.3224  loss_bbox_dn_5: 0.02667  loss_giou_dn_5: 0.1535  loss_ce_6: 0.5107  loss_mask_6: 0.04037  loss_dice_6: 0.3772  loss_bbox_6: 0.02914  loss_giou_6: 0.1849  loss_ce_dn_6: 0.1111  loss_mask_dn_6: 0.02733  loss_dice_dn_6: 0.3104  loss_bbox_dn_6: 0.02547  loss_giou_dn_6: 0.1519  loss_ce_7: 0.4806  loss_mask_7: 0.05006  loss_dice_7: 0.3385  loss_bbox_7: 0.03144  loss_giou_7: 0.1947  loss_ce_dn_7: 0.1032  loss_mask_dn_7: 0.02817  loss_dice_dn_7: 0.3159  loss_bbox_dn_7: 0.02475  loss_giou_dn_7: 0.1522  loss_ce_8: 0.5211  loss_mask_8: 0.04364  loss_dice_8: 0.3384  loss_bbox_8: 0.03172  loss_giou_8: 0.1884  loss_ce_dn_8: 0.102  loss_mask_dn_8: 0.0279  loss_dice_dn_8: 0.3005  loss_bbox_dn_8: 0.02436  loss_giou_dn_8: 0.1504  loss_ce_interm: 0.8999  loss_mask_interm: 0.03133  loss_dice_interm: 0.3346  loss_bbox_interm: 0.06543  loss_giou_interm: 0.31    time: 0.7903  last_time: 0.7975  data_time: 0.0153  last_data_time: 0.0060   lr: 1e-06  max_mem: 20850M\n",
      "\u001b[32m[07/13 11:33:48 d2.utils.events]: \u001b[0m eta: 0:09:41  iter: 1659  total_loss: 31.73  loss_ce: 0.5806  loss_mask: 0.04417  loss_dice: 0.3552  loss_bbox: 0.02908  loss_giou: 0.1883  loss_ce_dn: 0.1187  loss_mask_dn: 0.03844  loss_dice_dn: 0.3897  loss_bbox_dn: 0.03158  loss_giou_dn: 0.1416  loss_ce_0: 0.9876  loss_mask_0: 0.04632  loss_dice_0: 0.4409  loss_bbox_0: 0.05023  loss_giou_0: 0.2939  loss_ce_dn_0: 1.711  loss_mask_dn_0: 0.5164  loss_dice_dn_0: 2.811  loss_bbox_dn_0: 0.3709  loss_giou_dn_0: 0.855  loss_ce_1: 0.883  loss_mask_1: 0.04252  loss_dice_1: 0.3855  loss_bbox_1: 0.03994  loss_giou_1: 0.218  loss_ce_dn_1: 0.245  loss_mask_dn_1: 0.04416  loss_dice_dn_1: 0.3848  loss_bbox_dn_1: 0.07779  loss_giou_dn_1: 0.272  loss_ce_2: 0.7613  loss_mask_2: 0.04894  loss_dice_2: 0.3822  loss_bbox_2: 0.03684  loss_giou_2: 0.2065  loss_ce_dn_2: 0.1708  loss_mask_dn_2: 0.0408  loss_dice_dn_2: 0.3578  loss_bbox_dn_2: 0.04911  loss_giou_dn_2: 0.1872  loss_ce_3: 0.7283  loss_mask_3: 0.04773  loss_dice_3: 0.3977  loss_bbox_3: 0.03509  loss_giou_3: 0.1987  loss_ce_dn_3: 0.1399  loss_mask_dn_3: 0.0384  loss_dice_dn_3: 0.3754  loss_bbox_dn_3: 0.04205  loss_giou_dn_3: 0.1596  loss_ce_4: 0.7007  loss_mask_4: 0.04353  loss_dice_4: 0.3944  loss_bbox_4: 0.03028  loss_giou_4: 0.1917  loss_ce_dn_4: 0.1242  loss_mask_dn_4: 0.03813  loss_dice_dn_4: 0.3745  loss_bbox_dn_4: 0.03574  loss_giou_dn_4: 0.1498  loss_ce_5: 0.6386  loss_mask_5: 0.04471  loss_dice_5: 0.3545  loss_bbox_5: 0.03084  loss_giou_5: 0.1866  loss_ce_dn_5: 0.122  loss_mask_dn_5: 0.03826  loss_dice_dn_5: 0.371  loss_bbox_dn_5: 0.03531  loss_giou_dn_5: 0.1484  loss_ce_6: 0.6104  loss_mask_6: 0.04249  loss_dice_6: 0.3911  loss_bbox_6: 0.0282  loss_giou_6: 0.1798  loss_ce_dn_6: 0.1199  loss_mask_dn_6: 0.03954  loss_dice_dn_6: 0.3755  loss_bbox_dn_6: 0.03186  loss_giou_dn_6: 0.1443  loss_ce_7: 0.5871  loss_mask_7: 0.04408  loss_dice_7: 0.4061  loss_bbox_7: 0.03085  loss_giou_7: 0.1825  loss_ce_dn_7: 0.1126  loss_mask_dn_7: 0.03773  loss_dice_dn_7: 0.3863  loss_bbox_dn_7: 0.03137  loss_giou_dn_7: 0.1463  loss_ce_8: 0.584  loss_mask_8: 0.0416  loss_dice_8: 0.3845  loss_bbox_8: 0.0277  loss_giou_8: 0.1767  loss_ce_dn_8: 0.1156  loss_mask_dn_8: 0.03799  loss_dice_dn_8: 0.3703  loss_bbox_dn_8: 0.03167  loss_giou_dn_8: 0.1411  loss_ce_interm: 0.9591  loss_mask_interm: 0.03936  loss_dice_interm: 0.422  loss_bbox_interm: 0.07494  loss_giou_interm: 0.3018    time: 0.7903  last_time: 0.7793  data_time: 0.0152  last_data_time: 0.0073   lr: 1e-06  max_mem: 20850M\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[07/13 11:34:04 d2.utils.events]: \u001b[0m eta: 0:09:25  iter: 1679  total_loss: 28.68  loss_ce: 0.4642  loss_mask: 0.05873  loss_dice: 0.4371  loss_bbox: 0.03531  loss_giou: 0.2174  loss_ce_dn: 0.08989  loss_mask_dn: 0.05494  loss_dice_dn: 0.3566  loss_bbox_dn: 0.02617  loss_giou_dn: 0.1516  loss_ce_0: 0.9093  loss_mask_0: 0.07022  loss_dice_0: 0.4549  loss_bbox_0: 0.0506  loss_giou_0: 0.3097  loss_ce_dn_0: 1.785  loss_mask_dn_0: 0.447  loss_dice_dn_0: 2.632  loss_bbox_dn_0: 0.3203  loss_giou_dn_0: 0.8564  loss_ce_1: 0.7812  loss_mask_1: 0.06591  loss_dice_1: 0.4022  loss_bbox_1: 0.03749  loss_giou_1: 0.2499  loss_ce_dn_1: 0.242  loss_mask_dn_1: 0.05977  loss_dice_dn_1: 0.3573  loss_bbox_dn_1: 0.07089  loss_giou_dn_1: 0.2552  loss_ce_2: 0.6239  loss_mask_2: 0.0579  loss_dice_2: 0.3818  loss_bbox_2: 0.0384  loss_giou_2: 0.2243  loss_ce_dn_2: 0.1772  loss_mask_dn_2: 0.05699  loss_dice_dn_2: 0.3481  loss_bbox_dn_2: 0.04141  loss_giou_dn_2: 0.184  loss_ce_3: 0.5548  loss_mask_3: 0.0606  loss_dice_3: 0.4316  loss_bbox_3: 0.03356  loss_giou_3: 0.2286  loss_ce_dn_3: 0.1391  loss_mask_dn_3: 0.06  loss_dice_dn_3: 0.3511  loss_bbox_dn_3: 0.03436  loss_giou_dn_3: 0.1655  loss_ce_4: 0.5006  loss_mask_4: 0.06044  loss_dice_4: 0.3923  loss_bbox_4: 0.03749  loss_giou_4: 0.2355  loss_ce_dn_4: 0.1145  loss_mask_dn_4: 0.05661  loss_dice_dn_4: 0.3413  loss_bbox_dn_4: 0.03107  loss_giou_dn_4: 0.1584  loss_ce_5: 0.5044  loss_mask_5: 0.05923  loss_dice_5: 0.4012  loss_bbox_5: 0.03904  loss_giou_5: 0.2237  loss_ce_dn_5: 0.1004  loss_mask_dn_5: 0.05492  loss_dice_dn_5: 0.3368  loss_bbox_dn_5: 0.02875  loss_giou_dn_5: 0.1552  loss_ce_6: 0.4411  loss_mask_6: 0.05881  loss_dice_6: 0.3886  loss_bbox_6: 0.03791  loss_giou_6: 0.23  loss_ce_dn_6: 0.09327  loss_mask_dn_6: 0.05332  loss_dice_dn_6: 0.3499  loss_bbox_dn_6: 0.02735  loss_giou_dn_6: 0.1528  loss_ce_7: 0.4211  loss_mask_7: 0.06189  loss_dice_7: 0.4169  loss_bbox_7: 0.04049  loss_giou_7: 0.2213  loss_ce_dn_7: 0.09329  loss_mask_dn_7: 0.05464  loss_dice_dn_7: 0.3495  loss_bbox_dn_7: 0.02713  loss_giou_dn_7: 0.153  loss_ce_8: 0.4807  loss_mask_8: 0.0623  loss_dice_8: 0.4131  loss_bbox_8: 0.03741  loss_giou_8: 0.1982  loss_ce_dn_8: 0.09013  loss_mask_dn_8: 0.05475  loss_dice_dn_8: 0.3716  loss_bbox_dn_8: 0.0261  loss_giou_dn_8: 0.1505  loss_ce_interm: 0.9299  loss_mask_interm: 0.06416  loss_dice_interm: 0.4143  loss_bbox_interm: 0.08203  loss_giou_interm: 0.3028    time: 0.7903  last_time: 0.7748  data_time: 0.0114  last_data_time: 0.0073   lr: 1e-06  max_mem: 20850M\n",
      "\u001b[32m[07/13 11:34:20 d2.utils.events]: \u001b[0m eta: 0:09:09  iter: 1699  total_loss: 24.52  loss_ce: 0.4213  loss_mask: 0.03364  loss_dice: 0.3511  loss_bbox: 0.0355  loss_giou: 0.1665  loss_ce_dn: 0.1045  loss_mask_dn: 0.03262  loss_dice_dn: 0.319  loss_bbox_dn: 0.02468  loss_giou_dn: 0.136  loss_ce_0: 0.8476  loss_mask_0: 0.03729  loss_dice_0: 0.3667  loss_bbox_0: 0.05525  loss_giou_0: 0.3229  loss_ce_dn_0: 1.828  loss_mask_dn_0: 0.4649  loss_dice_dn_0: 2.818  loss_bbox_dn_0: 0.3568  loss_giou_dn_0: 0.8544  loss_ce_1: 0.7477  loss_mask_1: 0.03746  loss_dice_1: 0.3779  loss_bbox_1: 0.0403  loss_giou_1: 0.2183  loss_ce_dn_1: 0.2662  loss_mask_dn_1: 0.03785  loss_dice_dn_1: 0.3513  loss_bbox_dn_1: 0.06077  loss_giou_dn_1: 0.2438  loss_ce_2: 0.6449  loss_mask_2: 0.03572  loss_dice_2: 0.3386  loss_bbox_2: 0.03849  loss_giou_2: 0.1918  loss_ce_dn_2: 0.192  loss_mask_dn_2: 0.03611  loss_dice_dn_2: 0.3503  loss_bbox_dn_2: 0.0364  loss_giou_dn_2: 0.1729  loss_ce_3: 0.5364  loss_mask_3: 0.03869  loss_dice_3: 0.3508  loss_bbox_3: 0.03509  loss_giou_3: 0.1752  loss_ce_dn_3: 0.1465  loss_mask_dn_3: 0.03603  loss_dice_dn_3: 0.3317  loss_bbox_dn_3: 0.02958  loss_giou_dn_3: 0.1501  loss_ce_4: 0.5202  loss_mask_4: 0.03728  loss_dice_4: 0.3506  loss_bbox_4: 0.03897  loss_giou_4: 0.1746  loss_ce_dn_4: 0.1261  loss_mask_dn_4: 0.03533  loss_dice_dn_4: 0.3321  loss_bbox_dn_4: 0.0277  loss_giou_dn_4: 0.1448  loss_ce_5: 0.4652  loss_mask_5: 0.035  loss_dice_5: 0.3372  loss_bbox_5: 0.03802  loss_giou_5: 0.1736  loss_ce_dn_5: 0.1216  loss_mask_dn_5: 0.03457  loss_dice_dn_5: 0.332  loss_bbox_dn_5: 0.02585  loss_giou_dn_5: 0.1429  loss_ce_6: 0.4504  loss_mask_6: 0.03661  loss_dice_6: 0.343  loss_bbox_6: 0.03685  loss_giou_6: 0.1708  loss_ce_dn_6: 0.1122  loss_mask_dn_6: 0.03524  loss_dice_dn_6: 0.3354  loss_bbox_dn_6: 0.02534  loss_giou_dn_6: 0.137  loss_ce_7: 0.436  loss_mask_7: 0.03461  loss_dice_7: 0.3552  loss_bbox_7: 0.03611  loss_giou_7: 0.1691  loss_ce_dn_7: 0.1063  loss_mask_dn_7: 0.03242  loss_dice_dn_7: 0.3322  loss_bbox_dn_7: 0.02524  loss_giou_dn_7: 0.1374  loss_ce_8: 0.4245  loss_mask_8: 0.03322  loss_dice_8: 0.3421  loss_bbox_8: 0.03513  loss_giou_8: 0.1614  loss_ce_dn_8: 0.1052  loss_mask_dn_8: 0.03293  loss_dice_dn_8: 0.3333  loss_bbox_dn_8: 0.02466  loss_giou_dn_8: 0.1348  loss_ce_interm: 0.8495  loss_mask_interm: 0.03697  loss_dice_interm: 0.3422  loss_bbox_interm: 0.06515  loss_giou_interm: 0.2523    time: 0.7903  last_time: 0.7585  data_time: 0.0131  last_data_time: 0.0077   lr: 1e-06  max_mem: 20850M\n",
      "\u001b[32m[07/13 11:34:36 d2.utils.events]: \u001b[0m eta: 0:08:54  iter: 1719  total_loss: 28.97  loss_ce: 0.4561  loss_mask: 0.05012  loss_dice: 0.341  loss_bbox: 0.04352  loss_giou: 0.1883  loss_ce_dn: 0.09518  loss_mask_dn: 0.04669  loss_dice_dn: 0.3229  loss_bbox_dn: 0.03031  loss_giou_dn: 0.17  loss_ce_0: 1.011  loss_mask_0: 0.05305  loss_dice_0: 0.3941  loss_bbox_0: 0.06141  loss_giou_0: 0.3138  loss_ce_dn_0: 1.666  loss_mask_dn_0: 0.6404  loss_dice_dn_0: 2.791  loss_bbox_dn_0: 0.4001  loss_giou_dn_0: 0.8513  loss_ce_1: 0.8616  loss_mask_1: 0.05139  loss_dice_1: 0.375  loss_bbox_1: 0.0535  loss_giou_1: 0.2362  loss_ce_dn_1: 0.2666  loss_mask_dn_1: 0.05832  loss_dice_dn_1: 0.3963  loss_bbox_dn_1: 0.0817  loss_giou_dn_1: 0.2733  loss_ce_2: 0.6621  loss_mask_2: 0.05041  loss_dice_2: 0.3161  loss_bbox_2: 0.04854  loss_giou_2: 0.2532  loss_ce_dn_2: 0.1763  loss_mask_dn_2: 0.05173  loss_dice_dn_2: 0.3361  loss_bbox_dn_2: 0.05213  loss_giou_dn_2: 0.2056  loss_ce_3: 0.6131  loss_mask_3: 0.04639  loss_dice_3: 0.371  loss_bbox_3: 0.04515  loss_giou_3: 0.1913  loss_ce_dn_3: 0.13  loss_mask_dn_3: 0.04951  loss_dice_dn_3: 0.3166  loss_bbox_dn_3: 0.04001  loss_giou_dn_3: 0.1837  loss_ce_4: 0.5679  loss_mask_4: 0.04981  loss_dice_4: 0.3509  loss_bbox_4: 0.0444  loss_giou_4: 0.1868  loss_ce_dn_4: 0.1055  loss_mask_dn_4: 0.04767  loss_dice_dn_4: 0.3451  loss_bbox_dn_4: 0.0364  loss_giou_dn_4: 0.1718  loss_ce_5: 0.5007  loss_mask_5: 0.0483  loss_dice_5: 0.3679  loss_bbox_5: 0.03986  loss_giou_5: 0.2054  loss_ce_dn_5: 0.1009  loss_mask_dn_5: 0.04713  loss_dice_dn_5: 0.3422  loss_bbox_dn_5: 0.03466  loss_giou_dn_5: 0.1707  loss_ce_6: 0.4796  loss_mask_6: 0.05036  loss_dice_6: 0.296  loss_bbox_6: 0.04313  loss_giou_6: 0.1835  loss_ce_dn_6: 0.09701  loss_mask_dn_6: 0.04701  loss_dice_dn_6: 0.3357  loss_bbox_dn_6: 0.03354  loss_giou_dn_6: 0.1679  loss_ce_7: 0.4822  loss_mask_7: 0.0454  loss_dice_7: 0.3698  loss_bbox_7: 0.04213  loss_giou_7: 0.1853  loss_ce_dn_7: 0.09562  loss_mask_dn_7: 0.04612  loss_dice_dn_7: 0.3411  loss_bbox_dn_7: 0.03231  loss_giou_dn_7: 0.1668  loss_ce_8: 0.4626  loss_mask_8: 0.04801  loss_dice_8: 0.3482  loss_bbox_8: 0.03785  loss_giou_8: 0.1886  loss_ce_dn_8: 0.09465  loss_mask_dn_8: 0.04657  loss_dice_dn_8: 0.3385  loss_bbox_dn_8: 0.03021  loss_giou_dn_8: 0.1698  loss_ce_interm: 0.9847  loss_mask_interm: 0.05095  loss_dice_interm: 0.4161  loss_bbox_interm: 0.08074  loss_giou_interm: 0.324    time: 0.7903  last_time: 0.7804  data_time: 0.0166  last_data_time: 0.0070   lr: 1e-06  max_mem: 20850M\n",
      "\u001b[32m[07/13 11:34:52 d2.utils.events]: \u001b[0m eta: 0:08:38  iter: 1739  total_loss: 27.54  loss_ce: 0.5473  loss_mask: 0.03721  loss_dice: 0.384  loss_bbox: 0.04457  loss_giou: 0.1713  loss_ce_dn: 0.1352  loss_mask_dn: 0.03422  loss_dice_dn: 0.3419  loss_bbox_dn: 0.02919  loss_giou_dn: 0.1432  loss_ce_0: 0.9161  loss_mask_0: 0.03605  loss_dice_0: 0.3386  loss_bbox_0: 0.06556  loss_giou_0: 0.2659  loss_ce_dn_0: 1.871  loss_mask_dn_0: 0.5049  loss_dice_dn_0: 2.753  loss_bbox_dn_0: 0.347  loss_giou_dn_0: 0.86  loss_ce_1: 0.8386  loss_mask_1: 0.04172  loss_dice_1: 0.3732  loss_bbox_1: 0.06668  loss_giou_1: 0.2071  loss_ce_dn_1: 0.2703  loss_mask_dn_1: 0.04026  loss_dice_dn_1: 0.3829  loss_bbox_dn_1: 0.07273  loss_giou_dn_1: 0.2481  loss_ce_2: 0.7003  loss_mask_2: 0.03531  loss_dice_2: 0.3779  loss_bbox_2: 0.05183  loss_giou_2: 0.1885  loss_ce_dn_2: 0.1905  loss_mask_dn_2: 0.03743  loss_dice_dn_2: 0.3787  loss_bbox_dn_2: 0.04288  loss_giou_dn_2: 0.1844  loss_ce_3: 0.6698  loss_mask_3: 0.03587  loss_dice_3: 0.3604  loss_bbox_3: 0.04468  loss_giou_3: 0.1833  loss_ce_dn_3: 0.1623  loss_mask_dn_3: 0.03721  loss_dice_dn_3: 0.3386  loss_bbox_dn_3: 0.03364  loss_giou_dn_3: 0.1527  loss_ce_4: 0.5971  loss_mask_4: 0.03642  loss_dice_4: 0.3526  loss_bbox_4: 0.04594  loss_giou_4: 0.1755  loss_ce_dn_4: 0.1532  loss_mask_dn_4: 0.03508  loss_dice_dn_4: 0.3455  loss_bbox_dn_4: 0.03076  loss_giou_dn_4: 0.145  loss_ce_5: 0.5915  loss_mask_5: 0.03679  loss_dice_5: 0.3451  loss_bbox_5: 0.04078  loss_giou_5: 0.1736  loss_ce_dn_5: 0.1421  loss_mask_dn_5: 0.03366  loss_dice_dn_5: 0.3295  loss_bbox_dn_5: 0.03005  loss_giou_dn_5: 0.1447  loss_ce_6: 0.5635  loss_mask_6: 0.03567  loss_dice_6: 0.3563  loss_bbox_6: 0.0404  loss_giou_6: 0.1659  loss_ce_dn_6: 0.1385  loss_mask_dn_6: 0.03452  loss_dice_dn_6: 0.3404  loss_bbox_dn_6: 0.02933  loss_giou_dn_6: 0.1444  loss_ce_7: 0.5514  loss_mask_7: 0.03396  loss_dice_7: 0.3333  loss_bbox_7: 0.03809  loss_giou_7: 0.1675  loss_ce_dn_7: 0.1396  loss_mask_dn_7: 0.03482  loss_dice_dn_7: 0.3434  loss_bbox_dn_7: 0.02959  loss_giou_dn_7: 0.1446  loss_ce_8: 0.5423  loss_mask_8: 0.03696  loss_dice_8: 0.3483  loss_bbox_8: 0.04435  loss_giou_8: 0.1712  loss_ce_dn_8: 0.1335  loss_mask_dn_8: 0.03436  loss_dice_dn_8: 0.3332  loss_bbox_dn_8: 0.02852  loss_giou_dn_8: 0.1429  loss_ce_interm: 0.9265  loss_mask_interm: 0.04115  loss_dice_interm: 0.3597  loss_bbox_interm: 0.0895  loss_giou_interm: 0.2666    time: 0.7903  last_time: 0.8404  data_time: 0.0184  last_data_time: 0.0700   lr: 1e-06  max_mem: 20850M\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[07/13 11:35:07 d2.utils.events]: \u001b[0m eta: 0:08:22  iter: 1759  total_loss: 23.49  loss_ce: 0.4373  loss_mask: 0.0316  loss_dice: 0.2801  loss_bbox: 0.0319  loss_giou: 0.1368  loss_ce_dn: 0.08571  loss_mask_dn: 0.03011  loss_dice_dn: 0.293  loss_bbox_dn: 0.02962  loss_giou_dn: 0.1261  loss_ce_0: 0.8207  loss_mask_0: 0.03232  loss_dice_0: 0.2726  loss_bbox_0: 0.05226  loss_giou_0: 0.2747  loss_ce_dn_0: 1.734  loss_mask_dn_0: 0.3952  loss_dice_dn_0: 2.582  loss_bbox_dn_0: 0.295  loss_giou_dn_0: 0.8523  loss_ce_1: 0.7042  loss_mask_1: 0.03651  loss_dice_1: 0.2662  loss_bbox_1: 0.04029  loss_giou_1: 0.1679  loss_ce_dn_1: 0.2317  loss_mask_dn_1: 0.03669  loss_dice_dn_1: 0.3458  loss_bbox_dn_1: 0.05881  loss_giou_dn_1: 0.235  loss_ce_2: 0.5875  loss_mask_2: 0.03699  loss_dice_2: 0.272  loss_bbox_2: 0.04212  loss_giou_2: 0.1605  loss_ce_dn_2: 0.1458  loss_mask_dn_2: 0.0337  loss_dice_dn_2: 0.3239  loss_bbox_dn_2: 0.03722  loss_giou_dn_2: 0.1654  loss_ce_3: 0.523  loss_mask_3: 0.03238  loss_dice_3: 0.2856  loss_bbox_3: 0.0322  loss_giou_3: 0.1429  loss_ce_dn_3: 0.1207  loss_mask_dn_3: 0.03305  loss_dice_dn_3: 0.3067  loss_bbox_dn_3: 0.03231  loss_giou_dn_3: 0.1447  loss_ce_4: 0.4934  loss_mask_4: 0.03517  loss_dice_4: 0.2932  loss_bbox_4: 0.03221  loss_giou_4: 0.1418  loss_ce_dn_4: 0.1095  loss_mask_dn_4: 0.03178  loss_dice_dn_4: 0.318  loss_bbox_dn_4: 0.03143  loss_giou_dn_4: 0.1379  loss_ce_5: 0.4712  loss_mask_5: 0.03359  loss_dice_5: 0.2864  loss_bbox_5: 0.03219  loss_giou_5: 0.136  loss_ce_dn_5: 0.093  loss_mask_dn_5: 0.03066  loss_dice_dn_5: 0.3036  loss_bbox_dn_5: 0.03142  loss_giou_dn_5: 0.1311  loss_ce_6: 0.4456  loss_mask_6: 0.03475  loss_dice_6: 0.2931  loss_bbox_6: 0.03136  loss_giou_6: 0.1387  loss_ce_dn_6: 0.08891  loss_mask_dn_6: 0.03092  loss_dice_dn_6: 0.2988  loss_bbox_dn_6: 0.03044  loss_giou_dn_6: 0.1279  loss_ce_7: 0.4426  loss_mask_7: 0.03443  loss_dice_7: 0.2867  loss_bbox_7: 0.03158  loss_giou_7: 0.1381  loss_ce_dn_7: 0.08934  loss_mask_dn_7: 0.03131  loss_dice_dn_7: 0.2979  loss_bbox_dn_7: 0.03045  loss_giou_dn_7: 0.1269  loss_ce_8: 0.4321  loss_mask_8: 0.03195  loss_dice_8: 0.2733  loss_bbox_8: 0.03192  loss_giou_8: 0.1366  loss_ce_dn_8: 0.08747  loss_mask_dn_8: 0.03023  loss_dice_dn_8: 0.2869  loss_bbox_dn_8: 0.02965  loss_giou_dn_8: 0.1268  loss_ce_interm: 0.8213  loss_mask_interm: 0.03258  loss_dice_interm: 0.2947  loss_bbox_interm: 0.07112  loss_giou_interm: 0.2537    time: 0.7904  last_time: 0.7890  data_time: 0.0175  last_data_time: 0.0084   lr: 1e-06  max_mem: 20850M\n",
      "\u001b[32m[07/13 11:35:23 d2.utils.events]: \u001b[0m eta: 0:08:06  iter: 1779  total_loss: 27.18  loss_ce: 0.4739  loss_mask: 0.05908  loss_dice: 0.4141  loss_bbox: 0.03434  loss_giou: 0.1986  loss_ce_dn: 0.0981  loss_mask_dn: 0.0538  loss_dice_dn: 0.3345  loss_bbox_dn: 0.0286  loss_giou_dn: 0.1319  loss_ce_0: 0.8695  loss_mask_0: 0.05307  loss_dice_0: 0.3709  loss_bbox_0: 0.05598  loss_giou_0: 0.2931  loss_ce_dn_0: 1.848  loss_mask_dn_0: 0.6006  loss_dice_dn_0: 2.76  loss_bbox_dn_0: 0.3572  loss_giou_dn_0: 0.8547  loss_ce_1: 0.7843  loss_mask_1: 0.05939  loss_dice_1: 0.4093  loss_bbox_1: 0.04579  loss_giou_1: 0.2304  loss_ce_dn_1: 0.2587  loss_mask_dn_1: 0.05354  loss_dice_dn_1: 0.3556  loss_bbox_dn_1: 0.07516  loss_giou_dn_1: 0.2396  loss_ce_2: 0.6533  loss_mask_2: 0.06052  loss_dice_2: 0.4228  loss_bbox_2: 0.04126  loss_giou_2: 0.2171  loss_ce_dn_2: 0.1728  loss_mask_dn_2: 0.05459  loss_dice_dn_2: 0.3312  loss_bbox_dn_2: 0.04773  loss_giou_dn_2: 0.1771  loss_ce_3: 0.5844  loss_mask_3: 0.06135  loss_dice_3: 0.3776  loss_bbox_3: 0.03887  loss_giou_3: 0.2124  loss_ce_dn_3: 0.1409  loss_mask_dn_3: 0.05192  loss_dice_dn_3: 0.3328  loss_bbox_dn_3: 0.03733  loss_giou_dn_3: 0.1539  loss_ce_4: 0.572  loss_mask_4: 0.06002  loss_dice_4: 0.3997  loss_bbox_4: 0.03829  loss_giou_4: 0.205  loss_ce_dn_4: 0.1286  loss_mask_dn_4: 0.05241  loss_dice_dn_4: 0.3319  loss_bbox_dn_4: 0.03362  loss_giou_dn_4: 0.139  loss_ce_5: 0.5138  loss_mask_5: 0.05854  loss_dice_5: 0.3705  loss_bbox_5: 0.03771  loss_giou_5: 0.2065  loss_ce_dn_5: 0.1144  loss_mask_dn_5: 0.0539  loss_dice_dn_5: 0.3258  loss_bbox_dn_5: 0.03241  loss_giou_dn_5: 0.139  loss_ce_6: 0.5034  loss_mask_6: 0.06038  loss_dice_6: 0.3564  loss_bbox_6: 0.04011  loss_giou_6: 0.1988  loss_ce_dn_6: 0.104  loss_mask_dn_6: 0.05428  loss_dice_dn_6: 0.3242  loss_bbox_dn_6: 0.0285  loss_giou_dn_6: 0.1341  loss_ce_7: 0.5015  loss_mask_7: 0.05771  loss_dice_7: 0.3812  loss_bbox_7: 0.03755  loss_giou_7: 0.1973  loss_ce_dn_7: 0.09777  loss_mask_dn_7: 0.05354  loss_dice_dn_7: 0.3405  loss_bbox_dn_7: 0.02784  loss_giou_dn_7: 0.1341  loss_ce_8: 0.4732  loss_mask_8: 0.0592  loss_dice_8: 0.4159  loss_bbox_8: 0.03509  loss_giou_8: 0.1979  loss_ce_dn_8: 0.0974  loss_mask_dn_8: 0.0533  loss_dice_dn_8: 0.3325  loss_bbox_dn_8: 0.02812  loss_giou_dn_8: 0.1318  loss_ce_interm: 0.8682  loss_mask_interm: 0.05658  loss_dice_interm: 0.3531  loss_bbox_interm: 0.06643  loss_giou_interm: 0.2922    time: 0.7904  last_time: 0.7773  data_time: 0.0141  last_data_time: 0.0075   lr: 1e-06  max_mem: 20850M\n",
      "\u001b[32m[07/13 11:35:39 d2.utils.events]: \u001b[0m eta: 0:07:51  iter: 1799  total_loss: 32.68  loss_ce: 0.7256  loss_mask: 0.04426  loss_dice: 0.5107  loss_bbox: 0.04306  loss_giou: 0.271  loss_ce_dn: 0.1087  loss_mask_dn: 0.03888  loss_dice_dn: 0.4507  loss_bbox_dn: 0.02938  loss_giou_dn: 0.182  loss_ce_0: 1.063  loss_mask_0: 0.03789  loss_dice_0: 0.5409  loss_bbox_0: 0.06265  loss_giou_0: 0.4264  loss_ce_dn_0: 1.841  loss_mask_dn_0: 0.3625  loss_dice_dn_0: 2.825  loss_bbox_dn_0: 0.2864  loss_giou_dn_0: 0.8589  loss_ce_1: 0.9534  loss_mask_1: 0.04016  loss_dice_1: 0.4957  loss_bbox_1: 0.05119  loss_giou_1: 0.3404  loss_ce_dn_1: 0.2273  loss_mask_dn_1: 0.04384  loss_dice_dn_1: 0.4661  loss_bbox_dn_1: 0.06488  loss_giou_dn_1: 0.2934  loss_ce_2: 0.84  loss_mask_2: 0.04149  loss_dice_2: 0.536  loss_bbox_2: 0.04526  loss_giou_2: 0.2904  loss_ce_dn_2: 0.1609  loss_mask_dn_2: 0.04022  loss_dice_dn_2: 0.4458  loss_bbox_dn_2: 0.03875  loss_giou_dn_2: 0.2273  loss_ce_3: 0.7713  loss_mask_3: 0.03789  loss_dice_3: 0.4526  loss_bbox_3: 0.04715  loss_giou_3: 0.2645  loss_ce_dn_3: 0.1386  loss_mask_dn_3: 0.0386  loss_dice_dn_3: 0.4418  loss_bbox_dn_3: 0.03384  loss_giou_dn_3: 0.2037  loss_ce_4: 0.7616  loss_mask_4: 0.03943  loss_dice_4: 0.5213  loss_bbox_4: 0.04223  loss_giou_4: 0.2609  loss_ce_dn_4: 0.1232  loss_mask_dn_4: 0.04053  loss_dice_dn_4: 0.4209  loss_bbox_dn_4: 0.03001  loss_giou_dn_4: 0.1892  loss_ce_5: 0.7067  loss_mask_5: 0.04126  loss_dice_5: 0.4803  loss_bbox_5: 0.04316  loss_giou_5: 0.268  loss_ce_dn_5: 0.1158  loss_mask_dn_5: 0.03855  loss_dice_dn_5: 0.429  loss_bbox_dn_5: 0.02908  loss_giou_dn_5: 0.1846  loss_ce_6: 0.7128  loss_mask_6: 0.03817  loss_dice_6: 0.5291  loss_bbox_6: 0.04581  loss_giou_6: 0.2619  loss_ce_dn_6: 0.1102  loss_mask_dn_6: 0.03846  loss_dice_dn_6: 0.4283  loss_bbox_dn_6: 0.02927  loss_giou_dn_6: 0.183  loss_ce_7: 0.6749  loss_mask_7: 0.04044  loss_dice_7: 0.5476  loss_bbox_7: 0.04591  loss_giou_7: 0.2855  loss_ce_dn_7: 0.1089  loss_mask_dn_7: 0.03703  loss_dice_dn_7: 0.4428  loss_bbox_dn_7: 0.02939  loss_giou_dn_7: 0.1827  loss_ce_8: 0.7176  loss_mask_8: 0.04642  loss_dice_8: 0.5249  loss_bbox_8: 0.049  loss_giou_8: 0.2762  loss_ce_dn_8: 0.1085  loss_mask_dn_8: 0.03789  loss_dice_dn_8: 0.4592  loss_bbox_dn_8: 0.02932  loss_giou_dn_8: 0.1814  loss_ce_interm: 1.034  loss_mask_interm: 0.04409  loss_dice_interm: 0.4783  loss_bbox_interm: 0.07781  loss_giou_interm: 0.3629    time: 0.7903  last_time: 0.8032  data_time: 0.0159  last_data_time: 0.0374   lr: 1e-06  max_mem: 20850M\n",
      "\u001b[32m[07/13 11:35:55 d2.utils.events]: \u001b[0m eta: 0:07:35  iter: 1819  total_loss: 32.92  loss_ce: 0.595  loss_mask: 0.05465  loss_dice: 0.4602  loss_bbox: 0.03671  loss_giou: 0.2275  loss_ce_dn: 0.1393  loss_mask_dn: 0.04326  loss_dice_dn: 0.4  loss_bbox_dn: 0.03264  loss_giou_dn: 0.1638  loss_ce_0: 1.047  loss_mask_0: 0.05724  loss_dice_0: 0.503  loss_bbox_0: 0.05536  loss_giou_0: 0.3811  loss_ce_dn_0: 1.737  loss_mask_dn_0: 0.4446  loss_dice_dn_0: 2.85  loss_bbox_dn_0: 0.326  loss_giou_dn_0: 0.848  loss_ce_1: 0.9443  loss_mask_1: 0.0615  loss_dice_1: 0.4753  loss_bbox_1: 0.04746  loss_giou_1: 0.2852  loss_ce_dn_1: 0.2817  loss_mask_dn_1: 0.05323  loss_dice_dn_1: 0.4412  loss_bbox_dn_1: 0.06964  loss_giou_dn_1: 0.2836  loss_ce_2: 0.8052  loss_mask_2: 0.05457  loss_dice_2: 0.5006  loss_bbox_2: 0.04673  loss_giou_2: 0.2772  loss_ce_dn_2: 0.2048  loss_mask_dn_2: 0.0505  loss_dice_dn_2: 0.4272  loss_bbox_dn_2: 0.04601  loss_giou_dn_2: 0.2138  loss_ce_3: 0.7423  loss_mask_3: 0.05648  loss_dice_3: 0.4295  loss_bbox_3: 0.03998  loss_giou_3: 0.229  loss_ce_dn_3: 0.1699  loss_mask_dn_3: 0.04655  loss_dice_dn_3: 0.4183  loss_bbox_dn_3: 0.03982  loss_giou_dn_3: 0.1857  loss_ce_4: 0.6899  loss_mask_4: 0.05285  loss_dice_4: 0.498  loss_bbox_4: 0.03774  loss_giou_4: 0.2546  loss_ce_dn_4: 0.1564  loss_mask_dn_4: 0.04497  loss_dice_dn_4: 0.4127  loss_bbox_dn_4: 0.03708  loss_giou_dn_4: 0.1754  loss_ce_5: 0.6578  loss_mask_5: 0.0559  loss_dice_5: 0.4811  loss_bbox_5: 0.03564  loss_giou_5: 0.2263  loss_ce_dn_5: 0.1486  loss_mask_dn_5: 0.04386  loss_dice_dn_5: 0.4  loss_bbox_dn_5: 0.03608  loss_giou_dn_5: 0.1716  loss_ce_6: 0.6457  loss_mask_6: 0.05174  loss_dice_6: 0.5002  loss_bbox_6: 0.03876  loss_giou_6: 0.2236  loss_ce_dn_6: 0.1454  loss_mask_dn_6: 0.04283  loss_dice_dn_6: 0.3953  loss_bbox_dn_6: 0.0337  loss_giou_dn_6: 0.1678  loss_ce_7: 0.6134  loss_mask_7: 0.05648  loss_dice_7: 0.4855  loss_bbox_7: 0.03514  loss_giou_7: 0.2247  loss_ce_dn_7: 0.1409  loss_mask_dn_7: 0.04401  loss_dice_dn_7: 0.3983  loss_bbox_dn_7: 0.03366  loss_giou_dn_7: 0.1661  loss_ce_8: 0.6033  loss_mask_8: 0.0549  loss_dice_8: 0.4665  loss_bbox_8: 0.03496  loss_giou_8: 0.2271  loss_ce_dn_8: 0.1389  loss_mask_dn_8: 0.04352  loss_dice_dn_8: 0.3919  loss_bbox_dn_8: 0.03259  loss_giou_dn_8: 0.1641  loss_ce_interm: 1.042  loss_mask_interm: 0.05966  loss_dice_interm: 0.4825  loss_bbox_interm: 0.08287  loss_giou_interm: 0.3378    time: 0.7904  last_time: 0.7997  data_time: 0.0190  last_data_time: 0.0109   lr: 1e-06  max_mem: 20850M\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[07/13 11:36:11 d2.utils.events]: \u001b[0m eta: 0:07:19  iter: 1839  total_loss: 26.98  loss_ce: 0.5005  loss_mask: 0.03397  loss_dice: 0.354  loss_bbox: 0.03834  loss_giou: 0.1878  loss_ce_dn: 0.09622  loss_mask_dn: 0.0348  loss_dice_dn: 0.3001  loss_bbox_dn: 0.02972  loss_giou_dn: 0.1481  loss_ce_0: 0.9524  loss_mask_0: 0.03575  loss_dice_0: 0.3871  loss_bbox_0: 0.06781  loss_giou_0: 0.3175  loss_ce_dn_0: 1.868  loss_mask_dn_0: 0.5791  loss_dice_dn_0: 2.83  loss_bbox_dn_0: 0.3725  loss_giou_dn_0: 0.8617  loss_ce_1: 0.8038  loss_mask_1: 0.03486  loss_dice_1: 0.3569  loss_bbox_1: 0.05612  loss_giou_1: 0.2214  loss_ce_dn_1: 0.2387  loss_mask_dn_1: 0.03989  loss_dice_dn_1: 0.3601  loss_bbox_dn_1: 0.07591  loss_giou_dn_1: 0.268  loss_ce_2: 0.6592  loss_mask_2: 0.03607  loss_dice_2: 0.3838  loss_bbox_2: 0.04865  loss_giou_2: 0.2049  loss_ce_dn_2: 0.1693  loss_mask_dn_2: 0.03557  loss_dice_dn_2: 0.33  loss_bbox_dn_2: 0.04202  loss_giou_dn_2: 0.1845  loss_ce_3: 0.5742  loss_mask_3: 0.03607  loss_dice_3: 0.3778  loss_bbox_3: 0.04445  loss_giou_3: 0.188  loss_ce_dn_3: 0.1304  loss_mask_dn_3: 0.03563  loss_dice_dn_3: 0.3182  loss_bbox_dn_3: 0.03413  loss_giou_dn_3: 0.1657  loss_ce_4: 0.5122  loss_mask_4: 0.03532  loss_dice_4: 0.3672  loss_bbox_4: 0.03948  loss_giou_4: 0.1897  loss_ce_dn_4: 0.122  loss_mask_dn_4: 0.0348  loss_dice_dn_4: 0.3039  loss_bbox_dn_4: 0.03133  loss_giou_dn_4: 0.1532  loss_ce_5: 0.5357  loss_mask_5: 0.03746  loss_dice_5: 0.3372  loss_bbox_5: 0.04173  loss_giou_5: 0.194  loss_ce_dn_5: 0.116  loss_mask_dn_5: 0.0354  loss_dice_dn_5: 0.3139  loss_bbox_dn_5: 0.03192  loss_giou_dn_5: 0.1525  loss_ce_6: 0.4959  loss_mask_6: 0.0337  loss_dice_6: 0.3668  loss_bbox_6: 0.03988  loss_giou_6: 0.189  loss_ce_dn_6: 0.107  loss_mask_dn_6: 0.03464  loss_dice_dn_6: 0.2983  loss_bbox_dn_6: 0.02999  loss_giou_dn_6: 0.1487  loss_ce_7: 0.4822  loss_mask_7: 0.03267  loss_dice_7: 0.3945  loss_bbox_7: 0.03976  loss_giou_7: 0.185  loss_ce_dn_7: 0.09901  loss_mask_dn_7: 0.03636  loss_dice_dn_7: 0.3297  loss_bbox_dn_7: 0.02973  loss_giou_dn_7: 0.1489  loss_ce_8: 0.4723  loss_mask_8: 0.03252  loss_dice_8: 0.3551  loss_bbox_8: 0.03887  loss_giou_8: 0.1827  loss_ce_dn_8: 0.0962  loss_mask_dn_8: 0.03445  loss_dice_dn_8: 0.3069  loss_bbox_dn_8: 0.02948  loss_giou_dn_8: 0.1478  loss_ce_interm: 0.9185  loss_mask_interm: 0.03497  loss_dice_interm: 0.4091  loss_bbox_interm: 0.06983  loss_giou_interm: 0.2783    time: 0.7904  last_time: 0.7857  data_time: 0.0141  last_data_time: 0.0070   lr: 1e-06  max_mem: 20850M\n",
      "\u001b[32m[07/13 11:36:27 d2.utils.events]: \u001b[0m eta: 0:07:03  iter: 1859  total_loss: 33.99  loss_ce: 0.6717  loss_mask: 0.04258  loss_dice: 0.4641  loss_bbox: 0.04781  loss_giou: 0.2193  loss_ce_dn: 0.1393  loss_mask_dn: 0.03926  loss_dice_dn: 0.4028  loss_bbox_dn: 0.02977  loss_giou_dn: 0.1722  loss_ce_0: 1.031  loss_mask_0: 0.04457  loss_dice_0: 0.4531  loss_bbox_0: 0.07344  loss_giou_0: 0.3694  loss_ce_dn_0: 1.867  loss_mask_dn_0: 0.45  loss_dice_dn_0: 3.001  loss_bbox_dn_0: 0.3107  loss_giou_dn_0: 0.8516  loss_ce_1: 0.9277  loss_mask_1: 0.04315  loss_dice_1: 0.4425  loss_bbox_1: 0.06278  loss_giou_1: 0.2759  loss_ce_dn_1: 0.3036  loss_mask_dn_1: 0.04737  loss_dice_dn_1: 0.4351  loss_bbox_dn_1: 0.07291  loss_giou_dn_1: 0.2918  loss_ce_2: 0.8838  loss_mask_2: 0.04858  loss_dice_2: 0.4801  loss_bbox_2: 0.05265  loss_giou_2: 0.2496  loss_ce_dn_2: 0.2135  loss_mask_dn_2: 0.04004  loss_dice_dn_2: 0.4025  loss_bbox_dn_2: 0.04882  loss_giou_dn_2: 0.2139  loss_ce_3: 0.8275  loss_mask_3: 0.04456  loss_dice_3: 0.4529  loss_bbox_3: 0.05256  loss_giou_3: 0.232  loss_ce_dn_3: 0.1826  loss_mask_dn_3: 0.03912  loss_dice_dn_3: 0.4159  loss_bbox_dn_3: 0.04097  loss_giou_dn_3: 0.1901  loss_ce_4: 0.7419  loss_mask_4: 0.04229  loss_dice_4: 0.4554  loss_bbox_4: 0.04918  loss_giou_4: 0.2315  loss_ce_dn_4: 0.1685  loss_mask_dn_4: 0.03874  loss_dice_dn_4: 0.4102  loss_bbox_dn_4: 0.0377  loss_giou_dn_4: 0.1805  loss_ce_5: 0.7034  loss_mask_5: 0.04304  loss_dice_5: 0.4577  loss_bbox_5: 0.04822  loss_giou_5: 0.2269  loss_ce_dn_5: 0.1623  loss_mask_dn_5: 0.03883  loss_dice_dn_5: 0.4081  loss_bbox_dn_5: 0.03355  loss_giou_dn_5: 0.1772  loss_ce_6: 0.6993  loss_mask_6: 0.04086  loss_dice_6: 0.4714  loss_bbox_6: 0.04908  loss_giou_6: 0.2308  loss_ce_dn_6: 0.1514  loss_mask_dn_6: 0.03923  loss_dice_dn_6: 0.4012  loss_bbox_dn_6: 0.03168  loss_giou_dn_6: 0.174  loss_ce_7: 0.6739  loss_mask_7: 0.04218  loss_dice_7: 0.477  loss_bbox_7: 0.05007  loss_giou_7: 0.2258  loss_ce_dn_7: 0.1454  loss_mask_dn_7: 0.0386  loss_dice_dn_7: 0.4086  loss_bbox_dn_7: 0.03022  loss_giou_dn_7: 0.1734  loss_ce_8: 0.6812  loss_mask_8: 0.04153  loss_dice_8: 0.4502  loss_bbox_8: 0.04676  loss_giou_8: 0.2276  loss_ce_dn_8: 0.1408  loss_mask_dn_8: 0.03881  loss_dice_dn_8: 0.3985  loss_bbox_dn_8: 0.0296  loss_giou_dn_8: 0.172  loss_ce_interm: 1.046  loss_mask_interm: 0.04189  loss_dice_interm: 0.446  loss_bbox_interm: 0.0765  loss_giou_interm: 0.3334    time: 0.7904  last_time: 0.7764  data_time: 0.0221  last_data_time: 0.0073   lr: 1e-06  max_mem: 20850M\n",
      "\u001b[32m[07/13 11:36:43 d2.utils.events]: \u001b[0m eta: 0:06:48  iter: 1879  total_loss: 31.65  loss_ce: 0.6278  loss_mask: 0.03844  loss_dice: 0.4441  loss_bbox: 0.04564  loss_giou: 0.2306  loss_ce_dn: 0.1211  loss_mask_dn: 0.03512  loss_dice_dn: 0.3685  loss_bbox_dn: 0.02509  loss_giou_dn: 0.1907  loss_ce_0: 0.9995  loss_mask_0: 0.043  loss_dice_0: 0.4828  loss_bbox_0: 0.07287  loss_giou_0: 0.3919  loss_ce_dn_0: 1.76  loss_mask_dn_0: 0.4207  loss_dice_dn_0: 2.739  loss_bbox_dn_0: 0.2943  loss_giou_dn_0: 0.8561  loss_ce_1: 0.9005  loss_mask_1: 0.0417  loss_dice_1: 0.5157  loss_bbox_1: 0.06311  loss_giou_1: 0.3051  loss_ce_dn_1: 0.2417  loss_mask_dn_1: 0.03947  loss_dice_dn_1: 0.4681  loss_bbox_dn_1: 0.06324  loss_giou_dn_1: 0.2992  loss_ce_2: 0.7949  loss_mask_2: 0.03955  loss_dice_2: 0.5121  loss_bbox_2: 0.05832  loss_giou_2: 0.2635  loss_ce_dn_2: 0.1777  loss_mask_dn_2: 0.03635  loss_dice_dn_2: 0.4094  loss_bbox_dn_2: 0.03732  loss_giou_dn_2: 0.2328  loss_ce_3: 0.7076  loss_mask_3: 0.04053  loss_dice_3: 0.4471  loss_bbox_3: 0.05181  loss_giou_3: 0.2591  loss_ce_dn_3: 0.1494  loss_mask_dn_3: 0.0369  loss_dice_dn_3: 0.3801  loss_bbox_dn_3: 0.03054  loss_giou_dn_3: 0.213  loss_ce_4: 0.6947  loss_mask_4: 0.03937  loss_dice_4: 0.4428  loss_bbox_4: 0.05602  loss_giou_4: 0.2357  loss_ce_dn_4: 0.1387  loss_mask_dn_4: 0.03523  loss_dice_dn_4: 0.3741  loss_bbox_dn_4: 0.02838  loss_giou_dn_4: 0.2003  loss_ce_5: 0.6341  loss_mask_5: 0.03735  loss_dice_5: 0.5137  loss_bbox_5: 0.05432  loss_giou_5: 0.2504  loss_ce_dn_5: 0.1267  loss_mask_dn_5: 0.03492  loss_dice_dn_5: 0.3655  loss_bbox_dn_5: 0.02791  loss_giou_dn_5: 0.1949  loss_ce_6: 0.6527  loss_mask_6: 0.03689  loss_dice_6: 0.4399  loss_bbox_6: 0.04417  loss_giou_6: 0.2278  loss_ce_dn_6: 0.1246  loss_mask_dn_6: 0.03476  loss_dice_dn_6: 0.3706  loss_bbox_dn_6: 0.02683  loss_giou_dn_6: 0.1911  loss_ce_7: 0.6157  loss_mask_7: 0.03758  loss_dice_7: 0.5153  loss_bbox_7: 0.04956  loss_giou_7: 0.2235  loss_ce_dn_7: 0.123  loss_mask_dn_7: 0.03582  loss_dice_dn_7: 0.3676  loss_bbox_dn_7: 0.02662  loss_giou_dn_7: 0.1912  loss_ce_8: 0.6113  loss_mask_8: 0.03749  loss_dice_8: 0.4428  loss_bbox_8: 0.04832  loss_giou_8: 0.2261  loss_ce_dn_8: 0.1203  loss_mask_dn_8: 0.03498  loss_dice_dn_8: 0.3863  loss_bbox_dn_8: 0.02515  loss_giou_dn_8: 0.1906  loss_ce_interm: 1.005  loss_mask_interm: 0.04024  loss_dice_interm: 0.4645  loss_bbox_interm: 0.0852  loss_giou_interm: 0.343    time: 0.7903  last_time: 0.7744  data_time: 0.0164  last_data_time: 0.0074   lr: 1e-06  max_mem: 20850M\n",
      "\u001b[32m[07/13 11:36:58 d2.utils.events]: \u001b[0m eta: 0:06:32  iter: 1899  total_loss: 29.4  loss_ce: 0.5923  loss_mask: 0.05013  loss_dice: 0.3417  loss_bbox: 0.039  loss_giou: 0.1752  loss_ce_dn: 0.1212  loss_mask_dn: 0.03451  loss_dice_dn: 0.3156  loss_bbox_dn: 0.02783  loss_giou_dn: 0.1439  loss_ce_0: 0.9426  loss_mask_0: 0.04225  loss_dice_0: 0.3734  loss_bbox_0: 0.05763  loss_giou_0: 0.2889  loss_ce_dn_0: 1.889  loss_mask_dn_0: 0.5008  loss_dice_dn_0: 2.944  loss_bbox_dn_0: 0.3307  loss_giou_dn_0: 0.8509  loss_ce_1: 0.834  loss_mask_1: 0.05065  loss_dice_1: 0.393  loss_bbox_1: 0.04198  loss_giou_1: 0.2263  loss_ce_dn_1: 0.2616  loss_mask_dn_1: 0.04487  loss_dice_dn_1: 0.3789  loss_bbox_dn_1: 0.06647  loss_giou_dn_1: 0.2531  loss_ce_2: 0.7012  loss_mask_2: 0.04901  loss_dice_2: 0.3963  loss_bbox_2: 0.04294  loss_giou_2: 0.2031  loss_ce_dn_2: 0.1891  loss_mask_dn_2: 0.0433  loss_dice_dn_2: 0.3489  loss_bbox_dn_2: 0.04013  loss_giou_dn_2: 0.182  loss_ce_3: 0.6561  loss_mask_3: 0.05111  loss_dice_3: 0.3385  loss_bbox_3: 0.04064  loss_giou_3: 0.1887  loss_ce_dn_3: 0.1642  loss_mask_dn_3: 0.04563  loss_dice_dn_3: 0.3202  loss_bbox_dn_3: 0.03212  loss_giou_dn_3: 0.1592  loss_ce_4: 0.6213  loss_mask_4: 0.04729  loss_dice_4: 0.3636  loss_bbox_4: 0.03942  loss_giou_4: 0.1881  loss_ce_dn_4: 0.1466  loss_mask_dn_4: 0.04333  loss_dice_dn_4: 0.3201  loss_bbox_dn_4: 0.02973  loss_giou_dn_4: 0.1506  loss_ce_5: 0.6156  loss_mask_5: 0.04136  loss_dice_5: 0.3516  loss_bbox_5: 0.03813  loss_giou_5: 0.1824  loss_ce_dn_5: 0.1305  loss_mask_dn_5: 0.0391  loss_dice_dn_5: 0.3059  loss_bbox_dn_5: 0.02921  loss_giou_dn_5: 0.1456  loss_ce_6: 0.6054  loss_mask_6: 0.04796  loss_dice_6: 0.3861  loss_bbox_6: 0.03608  loss_giou_6: 0.1784  loss_ce_dn_6: 0.1264  loss_mask_dn_6: 0.03541  loss_dice_dn_6: 0.3037  loss_bbox_dn_6: 0.02944  loss_giou_dn_6: 0.1441  loss_ce_7: 0.5855  loss_mask_7: 0.04714  loss_dice_7: 0.4133  loss_bbox_7: 0.03577  loss_giou_7: 0.1801  loss_ce_dn_7: 0.1201  loss_mask_dn_7: 0.03462  loss_dice_dn_7: 0.3226  loss_bbox_dn_7: 0.0289  loss_giou_dn_7: 0.1438  loss_ce_8: 0.5828  loss_mask_8: 0.04774  loss_dice_8: 0.3813  loss_bbox_8: 0.03858  loss_giou_8: 0.1711  loss_ce_dn_8: 0.1195  loss_mask_dn_8: 0.0345  loss_dice_dn_8: 0.3123  loss_bbox_dn_8: 0.02789  loss_giou_dn_8: 0.1434  loss_ce_interm: 0.9314  loss_mask_interm: 0.04381  loss_dice_interm: 0.3798  loss_bbox_interm: 0.06901  loss_giou_interm: 0.2834    time: 0.7904  last_time: 0.8242  data_time: 0.0178  last_data_time: 0.0437   lr: 1e-06  max_mem: 20850M\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[07/13 11:37:14 d2.utils.events]: \u001b[0m eta: 0:06:16  iter: 1919  total_loss: 31.84  loss_ce: 0.591  loss_mask: 0.04117  loss_dice: 0.4338  loss_bbox: 0.05355  loss_giou: 0.2356  loss_ce_dn: 0.1117  loss_mask_dn: 0.03511  loss_dice_dn: 0.33  loss_bbox_dn: 0.026  loss_giou_dn: 0.1589  loss_ce_0: 0.9671  loss_mask_0: 0.04438  loss_dice_0: 0.4239  loss_bbox_0: 0.08055  loss_giou_0: 0.3478  loss_ce_dn_0: 1.79  loss_mask_dn_0: 0.3311  loss_dice_dn_0: 2.837  loss_bbox_dn_0: 0.3335  loss_giou_dn_0: 0.8548  loss_ce_1: 0.8971  loss_mask_1: 0.04541  loss_dice_1: 0.4311  loss_bbox_1: 0.07024  loss_giou_1: 0.2838  loss_ce_dn_1: 0.2555  loss_mask_dn_1: 0.03967  loss_dice_dn_1: 0.4067  loss_bbox_dn_1: 0.07065  loss_giou_dn_1: 0.2829  loss_ce_2: 0.8048  loss_mask_2: 0.04723  loss_dice_2: 0.4347  loss_bbox_2: 0.06217  loss_giou_2: 0.2647  loss_ce_dn_2: 0.1804  loss_mask_dn_2: 0.03798  loss_dice_dn_2: 0.3608  loss_bbox_dn_2: 0.04323  loss_giou_dn_2: 0.2037  loss_ce_3: 0.7256  loss_mask_3: 0.04404  loss_dice_3: 0.3994  loss_bbox_3: 0.05604  loss_giou_3: 0.2503  loss_ce_dn_3: 0.1461  loss_mask_dn_3: 0.03542  loss_dice_dn_3: 0.3494  loss_bbox_dn_3: 0.03468  loss_giou_dn_3: 0.1809  loss_ce_4: 0.663  loss_mask_4: 0.04478  loss_dice_4: 0.3741  loss_bbox_4: 0.05546  loss_giou_4: 0.2486  loss_ce_dn_4: 0.132  loss_mask_dn_4: 0.03524  loss_dice_dn_4: 0.3388  loss_bbox_dn_4: 0.03092  loss_giou_dn_4: 0.1675  loss_ce_5: 0.6976  loss_mask_5: 0.04226  loss_dice_5: 0.4403  loss_bbox_5: 0.05088  loss_giou_5: 0.2321  loss_ce_dn_5: 0.1172  loss_mask_dn_5: 0.0348  loss_dice_dn_5: 0.3385  loss_bbox_dn_5: 0.03056  loss_giou_dn_5: 0.1674  loss_ce_6: 0.6095  loss_mask_6: 0.04219  loss_dice_6: 0.4223  loss_bbox_6: 0.0477  loss_giou_6: 0.2358  loss_ce_dn_6: 0.1147  loss_mask_dn_6: 0.03448  loss_dice_dn_6: 0.352  loss_bbox_dn_6: 0.02702  loss_giou_dn_6: 0.1619  loss_ce_7: 0.656  loss_mask_7: 0.04142  loss_dice_7: 0.4033  loss_bbox_7: 0.05429  loss_giou_7: 0.2449  loss_ce_dn_7: 0.1135  loss_mask_dn_7: 0.03484  loss_dice_dn_7: 0.3289  loss_bbox_dn_7: 0.02679  loss_giou_dn_7: 0.1606  loss_ce_8: 0.6184  loss_mask_8: 0.04279  loss_dice_8: 0.4469  loss_bbox_8: 0.05106  loss_giou_8: 0.2403  loss_ce_dn_8: 0.1121  loss_mask_dn_8: 0.03526  loss_dice_dn_8: 0.3494  loss_bbox_dn_8: 0.02587  loss_giou_dn_8: 0.1589  loss_ce_interm: 0.9733  loss_mask_interm: 0.04466  loss_dice_interm: 0.4339  loss_bbox_interm: 0.08558  loss_giou_interm: 0.3464    time: 0.7904  last_time: 0.7759  data_time: 0.0135  last_data_time: 0.0074   lr: 1e-06  max_mem: 20850M\n",
      "\u001b[32m[07/13 11:37:30 d2.utils.events]: \u001b[0m eta: 0:06:01  iter: 1939  total_loss: 27.78  loss_ce: 0.5409  loss_mask: 0.03877  loss_dice: 0.4201  loss_bbox: 0.03731  loss_giou: 0.2043  loss_ce_dn: 0.1044  loss_mask_dn: 0.03214  loss_dice_dn: 0.3833  loss_bbox_dn: 0.02562  loss_giou_dn: 0.1502  loss_ce_0: 0.9073  loss_mask_0: 0.03775  loss_dice_0: 0.3909  loss_bbox_0: 0.06624  loss_giou_0: 0.3236  loss_ce_dn_0: 1.819  loss_mask_dn_0: 0.2785  loss_dice_dn_0: 2.599  loss_bbox_dn_0: 0.2864  loss_giou_dn_0: 0.8507  loss_ce_1: 0.8381  loss_mask_1: 0.04291  loss_dice_1: 0.4327  loss_bbox_1: 0.05093  loss_giou_1: 0.2617  loss_ce_dn_1: 0.2812  loss_mask_dn_1: 0.04096  loss_dice_dn_1: 0.3918  loss_bbox_dn_1: 0.06443  loss_giou_dn_1: 0.2685  loss_ce_2: 0.6658  loss_mask_2: 0.03841  loss_dice_2: 0.4446  loss_bbox_2: 0.04975  loss_giou_2: 0.2519  loss_ce_dn_2: 0.1975  loss_mask_dn_2: 0.03944  loss_dice_dn_2: 0.383  loss_bbox_dn_2: 0.04408  loss_giou_dn_2: 0.195  loss_ce_3: 0.5882  loss_mask_3: 0.04021  loss_dice_3: 0.3831  loss_bbox_3: 0.0445  loss_giou_3: 0.229  loss_ce_dn_3: 0.1557  loss_mask_dn_3: 0.03573  loss_dice_dn_3: 0.3696  loss_bbox_dn_3: 0.03569  loss_giou_dn_3: 0.1695  loss_ce_4: 0.5466  loss_mask_4: 0.0411  loss_dice_4: 0.4076  loss_bbox_4: 0.03761  loss_giou_4: 0.2181  loss_ce_dn_4: 0.1388  loss_mask_dn_4: 0.03525  loss_dice_dn_4: 0.3832  loss_bbox_dn_4: 0.03201  loss_giou_dn_4: 0.1612  loss_ce_5: 0.5341  loss_mask_5: 0.03856  loss_dice_5: 0.3946  loss_bbox_5: 0.03753  loss_giou_5: 0.2097  loss_ce_dn_5: 0.1229  loss_mask_dn_5: 0.03559  loss_dice_dn_5: 0.3665  loss_bbox_dn_5: 0.02916  loss_giou_dn_5: 0.1561  loss_ce_6: 0.5262  loss_mask_6: 0.03656  loss_dice_6: 0.3509  loss_bbox_6: 0.04204  loss_giou_6: 0.2051  loss_ce_dn_6: 0.1114  loss_mask_dn_6: 0.03382  loss_dice_dn_6: 0.3924  loss_bbox_dn_6: 0.02627  loss_giou_dn_6: 0.1517  loss_ce_7: 0.534  loss_mask_7: 0.03658  loss_dice_7: 0.3854  loss_bbox_7: 0.03745  loss_giou_7: 0.2085  loss_ce_dn_7: 0.1085  loss_mask_dn_7: 0.03114  loss_dice_dn_7: 0.395  loss_bbox_dn_7: 0.02628  loss_giou_dn_7: 0.1523  loss_ce_8: 0.5262  loss_mask_8: 0.03862  loss_dice_8: 0.4067  loss_bbox_8: 0.0378  loss_giou_8: 0.2054  loss_ce_dn_8: 0.1041  loss_mask_dn_8: 0.03137  loss_dice_dn_8: 0.394  loss_bbox_dn_8: 0.02552  loss_giou_dn_8: 0.1497  loss_ce_interm: 0.8991  loss_mask_interm: 0.03675  loss_dice_interm: 0.4185  loss_bbox_interm: 0.07722  loss_giou_interm: 0.3071    time: 0.7905  last_time: 0.7979  data_time: 0.0220  last_data_time: 0.0080   lr: 1e-06  max_mem: 20850M\n",
      "\u001b[32m[07/13 11:37:46 d2.utils.events]: \u001b[0m eta: 0:05:45  iter: 1959  total_loss: 33.61  loss_ce: 0.6209  loss_mask: 0.05716  loss_dice: 0.3812  loss_bbox: 0.05608  loss_giou: 0.1981  loss_ce_dn: 0.1542  loss_mask_dn: 0.04138  loss_dice_dn: 0.3518  loss_bbox_dn: 0.03244  loss_giou_dn: 0.1477  loss_ce_0: 1.001  loss_mask_0: 0.05061  loss_dice_0: 0.4423  loss_bbox_0: 0.07414  loss_giou_0: 0.3079  loss_ce_dn_0: 1.901  loss_mask_dn_0: 0.479  loss_dice_dn_0: 2.86  loss_bbox_dn_0: 0.354  loss_giou_dn_0: 0.8655  loss_ce_1: 0.929  loss_mask_1: 0.04375  loss_dice_1: 0.4224  loss_bbox_1: 0.06201  loss_giou_1: 0.2345  loss_ce_dn_1: 0.2804  loss_mask_dn_1: 0.04362  loss_dice_dn_1: 0.4122  loss_bbox_dn_1: 0.07282  loss_giou_dn_1: 0.2634  loss_ce_2: 0.8262  loss_mask_2: 0.04396  loss_dice_2: 0.3639  loss_bbox_2: 0.0564  loss_giou_2: 0.2153  loss_ce_dn_2: 0.2162  loss_mask_dn_2: 0.0427  loss_dice_dn_2: 0.3759  loss_bbox_dn_2: 0.04566  loss_giou_dn_2: 0.187  loss_ce_3: 0.7049  loss_mask_3: 0.04832  loss_dice_3: 0.373  loss_bbox_3: 0.05266  loss_giou_3: 0.1864  loss_ce_dn_3: 0.1812  loss_mask_dn_3: 0.04208  loss_dice_dn_3: 0.3677  loss_bbox_dn_3: 0.03678  loss_giou_dn_3: 0.1638  loss_ce_4: 0.6637  loss_mask_4: 0.04983  loss_dice_4: 0.4076  loss_bbox_4: 0.05347  loss_giou_4: 0.1843  loss_ce_dn_4: 0.1732  loss_mask_dn_4: 0.03989  loss_dice_dn_4: 0.3569  loss_bbox_dn_4: 0.03387  loss_giou_dn_4: 0.1527  loss_ce_5: 0.6365  loss_mask_5: 0.04393  loss_dice_5: 0.3639  loss_bbox_5: 0.05796  loss_giou_5: 0.1946  loss_ce_dn_5: 0.1624  loss_mask_dn_5: 0.03909  loss_dice_dn_5: 0.3479  loss_bbox_dn_5: 0.03253  loss_giou_dn_5: 0.1498  loss_ce_6: 0.6299  loss_mask_6: 0.05097  loss_dice_6: 0.4557  loss_bbox_6: 0.05264  loss_giou_6: 0.1891  loss_ce_dn_6: 0.1604  loss_mask_dn_6: 0.04129  loss_dice_dn_6: 0.3436  loss_bbox_dn_6: 0.03252  loss_giou_dn_6: 0.1474  loss_ce_7: 0.6398  loss_mask_7: 0.0556  loss_dice_7: 0.3708  loss_bbox_7: 0.0517  loss_giou_7: 0.1844  loss_ce_dn_7: 0.1595  loss_mask_dn_7: 0.04015  loss_dice_dn_7: 0.3451  loss_bbox_dn_7: 0.03227  loss_giou_dn_7: 0.1474  loss_ce_8: 0.62  loss_mask_8: 0.05311  loss_dice_8: 0.4146  loss_bbox_8: 0.05505  loss_giou_8: 0.1849  loss_ce_dn_8: 0.1548  loss_mask_dn_8: 0.04142  loss_dice_dn_8: 0.3492  loss_bbox_dn_8: 0.0324  loss_giou_dn_8: 0.1483  loss_ce_interm: 1.003  loss_mask_interm: 0.04548  loss_dice_interm: 0.4472  loss_bbox_interm: 0.08758  loss_giou_interm: 0.2877    time: 0.7905  last_time: 0.7876  data_time: 0.0230  last_data_time: 0.0097   lr: 1e-06  max_mem: 20850M\n",
      "\u001b[32m[07/13 11:38:02 d2.utils.events]: \u001b[0m eta: 0:05:29  iter: 1979  total_loss: 26.58  loss_ce: 0.4624  loss_mask: 0.05105  loss_dice: 0.3831  loss_bbox: 0.03619  loss_giou: 0.1704  loss_ce_dn: 0.0917  loss_mask_dn: 0.05279  loss_dice_dn: 0.3264  loss_bbox_dn: 0.0301  loss_giou_dn: 0.1414  loss_ce_0: 0.8491  loss_mask_0: 0.05894  loss_dice_0: 0.3494  loss_bbox_0: 0.04997  loss_giou_0: 0.2799  loss_ce_dn_0: 1.78  loss_mask_dn_0: 0.6313  loss_dice_dn_0: 2.708  loss_bbox_dn_0: 0.444  loss_giou_dn_0: 0.8561  loss_ce_1: 0.7537  loss_mask_1: 0.05468  loss_dice_1: 0.3749  loss_bbox_1: 0.0435  loss_giou_1: 0.1931  loss_ce_dn_1: 0.2522  loss_mask_dn_1: 0.05915  loss_dice_dn_1: 0.3589  loss_bbox_dn_1: 0.08365  loss_giou_dn_1: 0.2452  loss_ce_2: 0.6466  loss_mask_2: 0.05561  loss_dice_2: 0.3478  loss_bbox_2: 0.03979  loss_giou_2: 0.1768  loss_ce_dn_2: 0.1673  loss_mask_dn_2: 0.05475  loss_dice_dn_2: 0.3436  loss_bbox_dn_2: 0.05014  loss_giou_dn_2: 0.1717  loss_ce_3: 0.5727  loss_mask_3: 0.05285  loss_dice_3: 0.3394  loss_bbox_3: 0.0383  loss_giou_3: 0.1798  loss_ce_dn_3: 0.1262  loss_mask_dn_3: 0.05591  loss_dice_dn_3: 0.3257  loss_bbox_dn_3: 0.04067  loss_giou_dn_3: 0.1553  loss_ce_4: 0.5246  loss_mask_4: 0.04903  loss_dice_4: 0.3505  loss_bbox_4: 0.03541  loss_giou_4: 0.1735  loss_ce_dn_4: 0.1161  loss_mask_dn_4: 0.05272  loss_dice_dn_4: 0.3339  loss_bbox_dn_4: 0.03623  loss_giou_dn_4: 0.1477  loss_ce_5: 0.4704  loss_mask_5: 0.05109  loss_dice_5: 0.3878  loss_bbox_5: 0.03597  loss_giou_5: 0.1747  loss_ce_dn_5: 0.1014  loss_mask_dn_5: 0.05213  loss_dice_dn_5: 0.3292  loss_bbox_dn_5: 0.03304  loss_giou_dn_5: 0.1454  loss_ce_6: 0.4773  loss_mask_6: 0.04939  loss_dice_6: 0.3936  loss_bbox_6: 0.03458  loss_giou_6: 0.1716  loss_ce_dn_6: 0.1013  loss_mask_dn_6: 0.05188  loss_dice_dn_6: 0.3233  loss_bbox_dn_6: 0.03082  loss_giou_dn_6: 0.1466  loss_ce_7: 0.4527  loss_mask_7: 0.05026  loss_dice_7: 0.3677  loss_bbox_7: 0.03438  loss_giou_7: 0.171  loss_ce_dn_7: 0.09417  loss_mask_dn_7: 0.05151  loss_dice_dn_7: 0.3254  loss_bbox_dn_7: 0.03112  loss_giou_dn_7: 0.1446  loss_ce_8: 0.4623  loss_mask_8: 0.05331  loss_dice_8: 0.3904  loss_bbox_8: 0.03521  loss_giou_8: 0.1705  loss_ce_dn_8: 0.09155  loss_mask_dn_8: 0.05283  loss_dice_dn_8: 0.3166  loss_bbox_dn_8: 0.03019  loss_giou_dn_8: 0.1417  loss_ce_interm: 0.8478  loss_mask_interm: 0.06387  loss_dice_interm: 0.3552  loss_bbox_interm: 0.0826  loss_giou_interm: 0.257    time: 0.7906  last_time: 0.7914  data_time: 0.0172  last_data_time: 0.0088   lr: 1e-06  max_mem: 20850M\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[07/13 11:38:20 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
      "\u001b[32m[07/13 11:38:20 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001b[32m[07/13 11:38:20 d2.data.common]: \u001b[0mSerializing 58 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[07/13 11:38:20 d2.data.common]: \u001b[0mSerialized dataset takes 0.14 MiB\n",
      "\u001b[32m[07/13 11:38:20 d2.evaluation.evaluator]: \u001b[0mStart inference on 58 batches\n",
      "\u001b[32m[07/13 11:38:23 d2.evaluation.evaluator]: \u001b[0mInference done 11/58. Dataloading: 0.0009 s/iter. Inference: 0.1710 s/iter. Eval: 0.0975 s/iter. Total: 0.2694 s/iter. ETA=0:00:12\n",
      "\u001b[32m[07/13 11:38:28 d2.evaluation.evaluator]: \u001b[0mInference done 30/58. Dataloading: 0.0011 s/iter. Inference: 0.1714 s/iter. Eval: 0.0977 s/iter. Total: 0.2703 s/iter. ETA=0:00:07\n",
      "\u001b[32m[07/13 11:38:33 d2.evaluation.evaluator]: \u001b[0mInference done 49/58. Dataloading: 0.0012 s/iter. Inference: 0.1714 s/iter. Eval: 0.0975 s/iter. Total: 0.2701 s/iter. ETA=0:00:02\n",
      "\u001b[32m[07/13 11:38:36 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:14.392541 (0.271557 s / iter per device, on 1 devices)\n",
      "\u001b[32m[07/13 11:38:36 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:09 (0.171342 s / iter per device, on 1 devices)\n",
      "\u001b[32m[07/13 11:38:36 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
      "\u001b[32m[07/13 11:38:36 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/coco_instances_results.json\n",
      "\u001b[32m[07/13 11:38:36 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "\u001b[32m[07/13 11:38:36 d2.evaluation.fast_eval_api]: \u001b[0mEvaluate annotation type *bbox*\n",
      "\u001b[32m[07/13 11:38:36 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.evaluate() finished in 0.06 seconds.\n",
      "\u001b[32m[07/13 11:38:36 d2.evaluation.fast_eval_api]: \u001b[0mAccumulating evaluation results...\n",
      "\u001b[32m[07/13 11:38:36 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.accumulate() finished in 0.06 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.808\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.876\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.847\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.597\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.904\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.905\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.799\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.872\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.872\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.686\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.938\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.917\n",
      "\u001b[32m[07/13 11:38:36 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
      "|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl   |\n",
      "|:------:|:------:|:------:|:------:|:------:|:------:|\n",
      "| 80.837 | 87.617 | 84.668 | 59.712 | 90.413 | 90.546 |\n",
      "\u001b[32m[07/13 11:38:36 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
      "| category         | AP     | category       | AP     | category        | AP     |\n",
      "|:-----------------|:-------|:---------------|:-------|:----------------|:-------|\n",
      "| apple            | 87.763 | banana         | 93.485 | baseball        | 72.582 |\n",
      "| cereals          | 81.454 | cheezit        | 84.653 | chocolate_jello | 75.962 |\n",
      "| cleanser         | 78.965 | coffee_grounds | 74.668 | cola            | 87.187 |\n",
      "| couch_table      | 93.564 | dice           | 70.567 | fork            | 82.207 |\n",
      "| iced_tea         | 98.960 | juice_pack     | 88.473 | knife           | 76.570 |\n",
      "| lemon            | 90.615 | milk           | 85.933 | mustard         | 86.360 |\n",
      "| orange           | 68.220 | orange_juice   | 99.345 | peach           | 80.569 |\n",
      "| pear             | 85.445 | plum           | 88.232 | pringles        | 95.199 |\n",
      "| red_wine         | 90.958 | rubiks_cube    | 84.134 | shelf           | 92.131 |\n",
      "| shelf_door       | 60.180 | soccer_ball    | 86.885 | spam            | 77.992 |\n",
      "| sponge           | 75.285 | spoon          | 53.047 | strawberry      | 81.502 |\n",
      "| strawberry_jello | 72.287 | sugar          | 72.079 | tennis_ball     | 56.577 |\n",
      "| tomato_soup      | 54.995 | tropical_juice | 88.276 | tuna            | 79.334 |\n",
      "Loading and preparing results...\n",
      "DONE (t=0.03s)\n",
      "creating index...\n",
      "index created!\n",
      "\u001b[32m[07/13 11:38:36 d2.evaluation.fast_eval_api]: \u001b[0mEvaluate annotation type *segm*\n",
      "\u001b[32m[07/13 11:38:36 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.evaluate() finished in 0.08 seconds.\n",
      "\u001b[32m[07/13 11:38:36 d2.evaluation.fast_eval_api]: \u001b[0mAccumulating evaluation results...\n",
      "\u001b[32m[07/13 11:38:36 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.accumulate() finished in 0.06 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.767\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.869\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.832\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.487\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.891\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.877\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.763\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.839\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.839\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.610\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.917\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.912\n",
      "\u001b[32m[07/13 11:38:36 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for segm: \n",
      "|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl   |\n",
      "|:------:|:------:|:------:|:------:|:------:|:------:|\n",
      "| 76.697 | 86.921 | 83.191 | 48.651 | 89.116 | 87.721 |\n",
      "\u001b[32m[07/13 11:38:36 d2.evaluation.coco_evaluation]: \u001b[0mPer-category segm AP: \n",
      "| category         | AP     | category       | AP     | category        | AP     |\n",
      "|:-----------------|:-------|:---------------|:-------|:----------------|:-------|\n",
      "| apple            | 86.040 | banana         | 89.129 | baseball        | 72.414 |\n",
      "| cereals          | 79.608 | cheezit        | 84.653 | chocolate_jello | 75.856 |\n",
      "| cleanser         | 70.902 | coffee_grounds | 79.213 | cola            | 80.089 |\n",
      "| couch_table      | 49.020 | dice           | 64.219 | fork            | 70.438 |\n",
      "| iced_tea         | 98.193 | juice_pack     | 84.684 | knife           | 71.218 |\n",
      "| lemon            | 89.023 | milk           | 83.374 | mustard         | 85.835 |\n",
      "| orange           | 67.089 | orange_juice   | 99.345 | peach           | 74.010 |\n",
      "| pear             | 86.717 | plum           | 84.873 | pringles        | 86.802 |\n",
      "| red_wine         | 89.950 | rubiks_cube    | 82.102 | shelf           | 76.195 |\n",
      "| shelf_door       | 66.597 | soccer_ball    | 87.390 | spam            | 79.738 |\n",
      "| sponge           | 73.294 | spoon          | 43.260 | strawberry      | 78.102 |\n",
      "| strawberry_jello | 67.170 | sugar          | 71.024 | tennis_ball     | 47.784 |\n",
      "| tomato_soup      | 50.530 | tropical_juice | 86.609 | tuna            | 78.704 |\n",
      "\u001b[32m[07/13 11:38:36 d2.engine.defaults]: \u001b[0mEvaluation results for fiftyone_valid in csv format:\n",
      "\u001b[32m[07/13 11:38:36 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
      "\u001b[32m[07/13 11:38:36 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
      "\u001b[32m[07/13 11:38:36 d2.evaluation.testing]: \u001b[0mcopypaste: 80.8369,87.6174,84.6678,59.7118,90.4126,90.5458\n",
      "\u001b[32m[07/13 11:38:36 d2.evaluation.testing]: \u001b[0mcopypaste: Task: segm\n",
      "\u001b[32m[07/13 11:38:36 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[07/13 11:38:36 d2.evaluation.testing]: \u001b[0mcopypaste: 76.6973,86.9207,83.1905,48.6512,89.1165,87.7209\n",
      "\u001b[32m[07/13 11:38:36 d2.utils.events]: \u001b[0m eta: 0:05:13  iter: 1999  total_loss: 27.89  loss_ce: 0.42  loss_mask: 0.06687  loss_dice: 0.3675  loss_bbox: 0.03435  loss_giou: 0.1562  loss_ce_dn: 0.08987  loss_mask_dn: 0.05798  loss_dice_dn: 0.3414  loss_bbox_dn: 0.02906  loss_giou_dn: 0.1349  loss_ce_0: 0.8905  loss_mask_0: 0.07144  loss_dice_0: 0.3634  loss_bbox_0: 0.05899  loss_giou_0: 0.3189  loss_ce_dn_0: 1.752  loss_mask_dn_0: 0.4722  loss_dice_dn_0: 2.903  loss_bbox_dn_0: 0.3296  loss_giou_dn_0: 0.8522  loss_ce_1: 0.7729  loss_mask_1: 0.06873  loss_dice_1: 0.3338  loss_bbox_1: 0.04728  loss_giou_1: 0.235  loss_ce_dn_1: 0.239  loss_mask_dn_1: 0.07798  loss_dice_dn_1: 0.3722  loss_bbox_dn_1: 0.08199  loss_giou_dn_1: 0.2645  loss_ce_2: 0.6089  loss_mask_2: 0.07073  loss_dice_2: 0.3626  loss_bbox_2: 0.04109  loss_giou_2: 0.2211  loss_ce_dn_2: 0.1615  loss_mask_dn_2: 0.07032  loss_dice_dn_2: 0.353  loss_bbox_dn_2: 0.04776  loss_giou_dn_2: 0.1773  loss_ce_3: 0.5876  loss_mask_3: 0.07025  loss_dice_3: 0.3801  loss_bbox_3: 0.03802  loss_giou_3: 0.2061  loss_ce_dn_3: 0.1291  loss_mask_dn_3: 0.06641  loss_dice_dn_3: 0.33  loss_bbox_dn_3: 0.03934  loss_giou_dn_3: 0.1581  loss_ce_4: 0.5068  loss_mask_4: 0.0685  loss_dice_4: 0.3627  loss_bbox_4: 0.03719  loss_giou_4: 0.1748  loss_ce_dn_4: 0.114  loss_mask_dn_4: 0.06838  loss_dice_dn_4: 0.3357  loss_bbox_dn_4: 0.03334  loss_giou_dn_4: 0.1445  loss_ce_5: 0.4625  loss_mask_5: 0.06668  loss_dice_5: 0.3552  loss_bbox_5: 0.03796  loss_giou_5: 0.1712  loss_ce_dn_5: 0.1063  loss_mask_dn_5: 0.06838  loss_dice_dn_5: 0.3454  loss_bbox_dn_5: 0.03218  loss_giou_dn_5: 0.1403  loss_ce_6: 0.4453  loss_mask_6: 0.06679  loss_dice_6: 0.3482  loss_bbox_6: 0.03482  loss_giou_6: 0.1606  loss_ce_dn_6: 0.1011  loss_mask_dn_6: 0.06348  loss_dice_dn_6: 0.3281  loss_bbox_dn_6: 0.03043  loss_giou_dn_6: 0.1383  loss_ce_7: 0.4186  loss_mask_7: 0.07012  loss_dice_7: 0.3701  loss_bbox_7: 0.0362  loss_giou_7: 0.1571  loss_ce_dn_7: 0.09626  loss_mask_dn_7: 0.05963  loss_dice_dn_7: 0.3402  loss_bbox_dn_7: 0.03035  loss_giou_dn_7: 0.1372  loss_ce_8: 0.4337  loss_mask_8: 0.06719  loss_dice_8: 0.3517  loss_bbox_8: 0.03778  loss_giou_8: 0.1557  loss_ce_dn_8: 0.09143  loss_mask_dn_8: 0.05867  loss_dice_dn_8: 0.336  loss_bbox_dn_8: 0.02846  loss_giou_dn_8: 0.1347  loss_ce_interm: 0.8769  loss_mask_interm: 0.07297  loss_dice_interm: 0.3473  loss_bbox_interm: 0.0689  loss_giou_interm: 0.2729    time: 0.7906  last_time: 0.7973  data_time: 0.0128  last_data_time: 0.0102   lr: 1e-06  max_mem: 20850M\n",
      "\u001b[32m[07/13 11:38:52 d2.utils.events]: \u001b[0m eta: 0:04:58  iter: 2019  total_loss: 31.58  loss_ce: 0.5331  loss_mask: 0.04161  loss_dice: 0.467  loss_bbox: 0.0376  loss_giou: 0.2028  loss_ce_dn: 0.1339  loss_mask_dn: 0.03319  loss_dice_dn: 0.3474  loss_bbox_dn: 0.02678  loss_giou_dn: 0.1658  loss_ce_0: 1.004  loss_mask_0: 0.03897  loss_dice_0: 0.4352  loss_bbox_0: 0.05636  loss_giou_0: 0.3648  loss_ce_dn_0: 1.989  loss_mask_dn_0: 0.4387  loss_dice_dn_0: 2.888  loss_bbox_dn_0: 0.3013  loss_giou_dn_0: 0.8542  loss_ce_1: 0.8891  loss_mask_1: 0.04681  loss_dice_1: 0.4709  loss_bbox_1: 0.05551  loss_giou_1: 0.2568  loss_ce_dn_1: 0.285  loss_mask_dn_1: 0.03935  loss_dice_dn_1: 0.4178  loss_bbox_dn_1: 0.06662  loss_giou_dn_1: 0.2807  loss_ce_2: 0.737  loss_mask_2: 0.04157  loss_dice_2: 0.4436  loss_bbox_2: 0.04029  loss_giou_2: 0.2218  loss_ce_dn_2: 0.2021  loss_mask_dn_2: 0.0335  loss_dice_dn_2: 0.3588  loss_bbox_dn_2: 0.03759  loss_giou_dn_2: 0.2103  loss_ce_3: 0.6588  loss_mask_3: 0.04581  loss_dice_3: 0.442  loss_bbox_3: 0.0389  loss_giou_3: 0.2092  loss_ce_dn_3: 0.161  loss_mask_dn_3: 0.03335  loss_dice_dn_3: 0.3437  loss_bbox_dn_3: 0.03032  loss_giou_dn_3: 0.1831  loss_ce_4: 0.6051  loss_mask_4: 0.04268  loss_dice_4: 0.3967  loss_bbox_4: 0.03836  loss_giou_4: 0.2053  loss_ce_dn_4: 0.15  loss_mask_dn_4: 0.03355  loss_dice_dn_4: 0.3771  loss_bbox_dn_4: 0.02835  loss_giou_dn_4: 0.1706  loss_ce_5: 0.602  loss_mask_5: 0.03989  loss_dice_5: 0.4354  loss_bbox_5: 0.03624  loss_giou_5: 0.2026  loss_ce_dn_5: 0.1418  loss_mask_dn_5: 0.03298  loss_dice_dn_5: 0.3659  loss_bbox_dn_5: 0.0278  loss_giou_dn_5: 0.1708  loss_ce_6: 0.5692  loss_mask_6: 0.03903  loss_dice_6: 0.4336  loss_bbox_6: 0.03544  loss_giou_6: 0.2001  loss_ce_dn_6: 0.1319  loss_mask_dn_6: 0.03251  loss_dice_dn_6: 0.3522  loss_bbox_dn_6: 0.02784  loss_giou_dn_6: 0.1658  loss_ce_7: 0.5476  loss_mask_7: 0.03811  loss_dice_7: 0.4337  loss_bbox_7: 0.03736  loss_giou_7: 0.196  loss_ce_dn_7: 0.1311  loss_mask_dn_7: 0.03349  loss_dice_dn_7: 0.3568  loss_bbox_dn_7: 0.02777  loss_giou_dn_7: 0.1664  loss_ce_8: 0.5498  loss_mask_8: 0.04214  loss_dice_8: 0.4559  loss_bbox_8: 0.044  loss_giou_8: 0.2034  loss_ce_dn_8: 0.1313  loss_mask_dn_8: 0.03351  loss_dice_dn_8: 0.3478  loss_bbox_dn_8: 0.02673  loss_giou_dn_8: 0.1651  loss_ce_interm: 0.9598  loss_mask_interm: 0.04213  loss_dice_interm: 0.4838  loss_bbox_interm: 0.07317  loss_giou_interm: 0.3288    time: 0.7906  last_time: 0.7929  data_time: 0.0157  last_data_time: 0.0085   lr: 1e-07  max_mem: 20850M\n",
      "\u001b[32m[07/13 11:39:08 d2.utils.events]: \u001b[0m eta: 0:04:42  iter: 2039  total_loss: 29.44  loss_ce: 0.5094  loss_mask: 0.04655  loss_dice: 0.3584  loss_bbox: 0.04044  loss_giou: 0.1848  loss_ce_dn: 0.1064  loss_mask_dn: 0.04079  loss_dice_dn: 0.3274  loss_bbox_dn: 0.03599  loss_giou_dn: 0.1628  loss_ce_0: 0.8741  loss_mask_0: 0.04583  loss_dice_0: 0.3488  loss_bbox_0: 0.05639  loss_giou_0: 0.3172  loss_ce_dn_0: 1.792  loss_mask_dn_0: 0.3543  loss_dice_dn_0: 2.592  loss_bbox_dn_0: 0.3339  loss_giou_dn_0: 0.8532  loss_ce_1: 0.7626  loss_mask_1: 0.04594  loss_dice_1: 0.3588  loss_bbox_1: 0.04591  loss_giou_1: 0.2155  loss_ce_dn_1: 0.2551  loss_mask_dn_1: 0.04424  loss_dice_dn_1: 0.3576  loss_bbox_dn_1: 0.07836  loss_giou_dn_1: 0.2644  loss_ce_2: 0.6602  loss_mask_2: 0.04649  loss_dice_2: 0.3512  loss_bbox_2: 0.03945  loss_giou_2: 0.2054  loss_ce_dn_2: 0.186  loss_mask_dn_2: 0.04242  loss_dice_dn_2: 0.3375  loss_bbox_dn_2: 0.05182  loss_giou_dn_2: 0.2  loss_ce_3: 0.5981  loss_mask_3: 0.04561  loss_dice_3: 0.3633  loss_bbox_3: 0.03967  loss_giou_3: 0.2085  loss_ce_dn_3: 0.1513  loss_mask_dn_3: 0.04287  loss_dice_dn_3: 0.324  loss_bbox_dn_3: 0.04385  loss_giou_dn_3: 0.1794  loss_ce_4: 0.5627  loss_mask_4: 0.04301  loss_dice_4: 0.347  loss_bbox_4: 0.03974  loss_giou_4: 0.1903  loss_ce_dn_4: 0.1277  loss_mask_dn_4: 0.04244  loss_dice_dn_4: 0.3366  loss_bbox_dn_4: 0.04075  loss_giou_dn_4: 0.1643  loss_ce_5: 0.5251  loss_mask_5: 0.04478  loss_dice_5: 0.3481  loss_bbox_5: 0.03943  loss_giou_5: 0.1955  loss_ce_dn_5: 0.1212  loss_mask_dn_5: 0.04265  loss_dice_dn_5: 0.307  loss_bbox_dn_5: 0.0397  loss_giou_dn_5: 0.1632  loss_ce_6: 0.5309  loss_mask_6: 0.04434  loss_dice_6: 0.3718  loss_bbox_6: 0.04072  loss_giou_6: 0.1904  loss_ce_dn_6: 0.1125  loss_mask_dn_6: 0.04102  loss_dice_dn_6: 0.3318  loss_bbox_dn_6: 0.03754  loss_giou_dn_6: 0.164  loss_ce_7: 0.516  loss_mask_7: 0.04427  loss_dice_7: 0.3301  loss_bbox_7: 0.04055  loss_giou_7: 0.1927  loss_ce_dn_7: 0.1098  loss_mask_dn_7: 0.04225  loss_dice_dn_7: 0.339  loss_bbox_dn_7: 0.03837  loss_giou_dn_7: 0.1654  loss_ce_8: 0.514  loss_mask_8: 0.04534  loss_dice_8: 0.3153  loss_bbox_8: 0.04252  loss_giou_8: 0.1908  loss_ce_dn_8: 0.107  loss_mask_dn_8: 0.04177  loss_dice_dn_8: 0.3274  loss_bbox_dn_8: 0.0359  loss_giou_dn_8: 0.1618  loss_ce_interm: 0.8711  loss_mask_interm: 0.04551  loss_dice_interm: 0.345  loss_bbox_interm: 0.08601  loss_giou_interm: 0.3037    time: 0.7906  last_time: 0.7879  data_time: 0.0143  last_data_time: 0.0079   lr: 1e-07  max_mem: 20850M\n",
      "\u001b[32m[07/13 11:39:24 d2.utils.events]: \u001b[0m eta: 0:04:26  iter: 2059  total_loss: 29.53  loss_ce: 0.5183  loss_mask: 0.05893  loss_dice: 0.4541  loss_bbox: 0.03793  loss_giou: 0.2175  loss_ce_dn: 0.1032  loss_mask_dn: 0.04672  loss_dice_dn: 0.4288  loss_bbox_dn: 0.02799  loss_giou_dn: 0.1867  loss_ce_0: 1.023  loss_mask_0: 0.0683  loss_dice_0: 0.4282  loss_bbox_0: 0.0503  loss_giou_0: 0.3624  loss_ce_dn_0: 1.759  loss_mask_dn_0: 0.4364  loss_dice_dn_0: 2.808  loss_bbox_dn_0: 0.3434  loss_giou_dn_0: 0.8591  loss_ce_1: 0.7787  loss_mask_1: 0.06017  loss_dice_1: 0.454  loss_bbox_1: 0.0437  loss_giou_1: 0.2647  loss_ce_dn_1: 0.2564  loss_mask_dn_1: 0.04668  loss_dice_dn_1: 0.4321  loss_bbox_dn_1: 0.07395  loss_giou_dn_1: 0.2862  loss_ce_2: 0.6874  loss_mask_2: 0.06564  loss_dice_2: 0.4333  loss_bbox_2: 0.04441  loss_giou_2: 0.2501  loss_ce_dn_2: 0.1857  loss_mask_dn_2: 0.04598  loss_dice_dn_2: 0.4411  loss_bbox_dn_2: 0.04015  loss_giou_dn_2: 0.2234  loss_ce_3: 0.6394  loss_mask_3: 0.06143  loss_dice_3: 0.4502  loss_bbox_3: 0.03811  loss_giou_3: 0.2398  loss_ce_dn_3: 0.1447  loss_mask_dn_3: 0.04366  loss_dice_dn_3: 0.4153  loss_bbox_dn_3: 0.03459  loss_giou_dn_3: 0.2036  loss_ce_4: 0.594  loss_mask_4: 0.06024  loss_dice_4: 0.422  loss_bbox_4: 0.03988  loss_giou_4: 0.2352  loss_ce_dn_4: 0.1327  loss_mask_dn_4: 0.04353  loss_dice_dn_4: 0.4382  loss_bbox_dn_4: 0.03251  loss_giou_dn_4: 0.1987  loss_ce_5: 0.547  loss_mask_5: 0.06385  loss_dice_5: 0.4677  loss_bbox_5: 0.04085  loss_giou_5: 0.2211  loss_ce_dn_5: 0.1175  loss_mask_dn_5: 0.04557  loss_dice_dn_5: 0.4156  loss_bbox_dn_5: 0.03045  loss_giou_dn_5: 0.1931  loss_ce_6: 0.5224  loss_mask_6: 0.06265  loss_dice_6: 0.4115  loss_bbox_6: 0.04179  loss_giou_6: 0.2189  loss_ce_dn_6: 0.1102  loss_mask_dn_6: 0.04624  loss_dice_dn_6: 0.4219  loss_bbox_dn_6: 0.02916  loss_giou_dn_6: 0.1873  loss_ce_7: 0.5224  loss_mask_7: 0.05897  loss_dice_7: 0.4242  loss_bbox_7: 0.03719  loss_giou_7: 0.2195  loss_ce_dn_7: 0.1063  loss_mask_dn_7: 0.04535  loss_dice_dn_7: 0.4376  loss_bbox_dn_7: 0.02856  loss_giou_dn_7: 0.1876  loss_ce_8: 0.5164  loss_mask_8: 0.0612  loss_dice_8: 0.4293  loss_bbox_8: 0.0342  loss_giou_8: 0.2105  loss_ce_dn_8: 0.1043  loss_mask_dn_8: 0.04572  loss_dice_dn_8: 0.4138  loss_bbox_dn_8: 0.0278  loss_giou_dn_8: 0.186  loss_ce_interm: 1.014  loss_mask_interm: 0.06107  loss_dice_interm: 0.4311  loss_bbox_interm: 0.06932  loss_giou_interm: 0.3506    time: 0.7906  last_time: 0.7845  data_time: 0.0180  last_data_time: 0.0085   lr: 1e-07  max_mem: 20850M\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[07/13 11:39:40 d2.utils.events]: \u001b[0m eta: 0:04:11  iter: 2079  total_loss: 31.78  loss_ce: 0.6404  loss_mask: 0.04633  loss_dice: 0.3494  loss_bbox: 0.03883  loss_giou: 0.1846  loss_ce_dn: 0.1051  loss_mask_dn: 0.04357  loss_dice_dn: 0.3721  loss_bbox_dn: 0.03375  loss_giou_dn: 0.1658  loss_ce_0: 0.9723  loss_mask_0: 0.04983  loss_dice_0: 0.397  loss_bbox_0: 0.06236  loss_giou_0: 0.4095  loss_ce_dn_0: 1.862  loss_mask_dn_0: 0.5399  loss_dice_dn_0: 2.878  loss_bbox_dn_0: 0.3356  loss_giou_dn_0: 0.8491  loss_ce_1: 0.8534  loss_mask_1: 0.05035  loss_dice_1: 0.3122  loss_bbox_1: 0.05051  loss_giou_1: 0.2304  loss_ce_dn_1: 0.2659  loss_mask_dn_1: 0.05129  loss_dice_dn_1: 0.4204  loss_bbox_dn_1: 0.08252  loss_giou_dn_1: 0.2794  loss_ce_2: 0.7328  loss_mask_2: 0.0519  loss_dice_2: 0.3649  loss_bbox_2: 0.04497  loss_giou_2: 0.196  loss_ce_dn_2: 0.1859  loss_mask_dn_2: 0.04571  loss_dice_dn_2: 0.4058  loss_bbox_dn_2: 0.04993  loss_giou_dn_2: 0.2134  loss_ce_3: 0.6911  loss_mask_3: 0.0503  loss_dice_3: 0.4237  loss_bbox_3: 0.04298  loss_giou_3: 0.2018  loss_ce_dn_3: 0.1594  loss_mask_dn_3: 0.04303  loss_dice_dn_3: 0.3721  loss_bbox_dn_3: 0.04031  loss_giou_dn_3: 0.1838  loss_ce_4: 0.68  loss_mask_4: 0.05067  loss_dice_4: 0.3675  loss_bbox_4: 0.04199  loss_giou_4: 0.1789  loss_ce_dn_4: 0.1372  loss_mask_dn_4: 0.04228  loss_dice_dn_4: 0.3719  loss_bbox_dn_4: 0.03697  loss_giou_dn_4: 0.1758  loss_ce_5: 0.6896  loss_mask_5: 0.04446  loss_dice_5: 0.3969  loss_bbox_5: 0.04072  loss_giou_5: 0.1826  loss_ce_dn_5: 0.1222  loss_mask_dn_5: 0.04175  loss_dice_dn_5: 0.3461  loss_bbox_dn_5: 0.03365  loss_giou_dn_5: 0.173  loss_ce_6: 0.6748  loss_mask_6: 0.04518  loss_dice_6: 0.3604  loss_bbox_6: 0.03976  loss_giou_6: 0.1906  loss_ce_dn_6: 0.1152  loss_mask_dn_6: 0.04341  loss_dice_dn_6: 0.3569  loss_bbox_dn_6: 0.03282  loss_giou_dn_6: 0.1667  loss_ce_7: 0.6316  loss_mask_7: 0.04638  loss_dice_7: 0.3164  loss_bbox_7: 0.03987  loss_giou_7: 0.1901  loss_ce_dn_7: 0.1098  loss_mask_dn_7: 0.04284  loss_dice_dn_7: 0.3632  loss_bbox_dn_7: 0.03367  loss_giou_dn_7: 0.1679  loss_ce_8: 0.6282  loss_mask_8: 0.04507  loss_dice_8: 0.3924  loss_bbox_8: 0.03907  loss_giou_8: 0.1816  loss_ce_dn_8: 0.1078  loss_mask_dn_8: 0.04413  loss_dice_dn_8: 0.3879  loss_bbox_dn_8: 0.03338  loss_giou_dn_8: 0.1648  loss_ce_interm: 0.9835  loss_mask_interm: 0.04973  loss_dice_interm: 0.409  loss_bbox_interm: 0.07976  loss_giou_interm: 0.3283    time: 0.7907  last_time: 0.7875  data_time: 0.0182  last_data_time: 0.0084   lr: 1e-07  max_mem: 20850M\n",
      "\u001b[32m[07/13 11:39:56 d2.utils.events]: \u001b[0m eta: 0:03:55  iter: 2099  total_loss: 28.07  loss_ce: 0.5112  loss_mask: 0.03592  loss_dice: 0.384  loss_bbox: 0.03752  loss_giou: 0.2111  loss_ce_dn: 0.1153  loss_mask_dn: 0.03379  loss_dice_dn: 0.4052  loss_bbox_dn: 0.02789  loss_giou_dn: 0.1494  loss_ce_0: 0.9454  loss_mask_0: 0.04392  loss_dice_0: 0.4405  loss_bbox_0: 0.05283  loss_giou_0: 0.3342  loss_ce_dn_0: 1.865  loss_mask_dn_0: 0.4502  loss_dice_dn_0: 2.829  loss_bbox_dn_0: 0.3025  loss_giou_dn_0: 0.8561  loss_ce_1: 0.8962  loss_mask_1: 0.04195  loss_dice_1: 0.4108  loss_bbox_1: 0.03884  loss_giou_1: 0.2536  loss_ce_dn_1: 0.266  loss_mask_dn_1: 0.05035  loss_dice_dn_1: 0.4501  loss_bbox_dn_1: 0.07577  loss_giou_dn_1: 0.2803  loss_ce_2: 0.6946  loss_mask_2: 0.03667  loss_dice_2: 0.4178  loss_bbox_2: 0.04022  loss_giou_2: 0.1985  loss_ce_dn_2: 0.1842  loss_mask_dn_2: 0.03528  loss_dice_dn_2: 0.4004  loss_bbox_dn_2: 0.04464  loss_giou_dn_2: 0.1957  loss_ce_3: 0.6326  loss_mask_3: 0.03824  loss_dice_3: 0.3756  loss_bbox_3: 0.03921  loss_giou_3: 0.2094  loss_ce_dn_3: 0.1411  loss_mask_dn_3: 0.03486  loss_dice_dn_3: 0.3963  loss_bbox_dn_3: 0.03372  loss_giou_dn_3: 0.1673  loss_ce_4: 0.5806  loss_mask_4: 0.03339  loss_dice_4: 0.3969  loss_bbox_4: 0.03941  loss_giou_4: 0.2172  loss_ce_dn_4: 0.1255  loss_mask_dn_4: 0.03357  loss_dice_dn_4: 0.3821  loss_bbox_dn_4: 0.03082  loss_giou_dn_4: 0.1596  loss_ce_5: 0.5592  loss_mask_5: 0.03241  loss_dice_5: 0.3959  loss_bbox_5: 0.0372  loss_giou_5: 0.2166  loss_ce_dn_5: 0.1158  loss_mask_dn_5: 0.0337  loss_dice_dn_5: 0.3866  loss_bbox_dn_5: 0.02926  loss_giou_dn_5: 0.1515  loss_ce_6: 0.5641  loss_mask_6: 0.03562  loss_dice_6: 0.4039  loss_bbox_6: 0.03558  loss_giou_6: 0.1927  loss_ce_dn_6: 0.1162  loss_mask_dn_6: 0.03278  loss_dice_dn_6: 0.3796  loss_bbox_dn_6: 0.02863  loss_giou_dn_6: 0.1499  loss_ce_7: 0.5333  loss_mask_7: 0.03259  loss_dice_7: 0.3777  loss_bbox_7: 0.03765  loss_giou_7: 0.2044  loss_ce_dn_7: 0.1163  loss_mask_dn_7: 0.03341  loss_dice_dn_7: 0.3869  loss_bbox_dn_7: 0.02853  loss_giou_dn_7: 0.1509  loss_ce_8: 0.5242  loss_mask_8: 0.03444  loss_dice_8: 0.3745  loss_bbox_8: 0.03377  loss_giou_8: 0.1945  loss_ce_dn_8: 0.1139  loss_mask_dn_8: 0.03371  loss_dice_dn_8: 0.3837  loss_bbox_dn_8: 0.02797  loss_giou_dn_8: 0.1488  loss_ce_interm: 0.9551  loss_mask_interm: 0.03934  loss_dice_interm: 0.3968  loss_bbox_interm: 0.08193  loss_giou_interm: 0.3216    time: 0.7907  last_time: 0.7794  data_time: 0.0190  last_data_time: 0.0057   lr: 1e-07  max_mem: 20850M\n",
      "\u001b[32m[07/13 11:40:12 d2.utils.events]: \u001b[0m eta: 0:03:39  iter: 2119  total_loss: 32.54  loss_ce: 0.5036  loss_mask: 0.03589  loss_dice: 0.5219  loss_bbox: 0.04349  loss_giou: 0.2903  loss_ce_dn: 0.1082  loss_mask_dn: 0.02908  loss_dice_dn: 0.3903  loss_bbox_dn: 0.03065  loss_giou_dn: 0.1575  loss_ce_0: 1.012  loss_mask_0: 0.03039  loss_dice_0: 0.4603  loss_bbox_0: 0.0621  loss_giou_0: 0.3718  loss_ce_dn_0: 1.914  loss_mask_dn_0: 0.3301  loss_dice_dn_0: 2.698  loss_bbox_dn_0: 0.2821  loss_giou_dn_0: 0.8527  loss_ce_1: 0.8742  loss_mask_1: 0.0424  loss_dice_1: 0.4931  loss_bbox_1: 0.04769  loss_giou_1: 0.2954  loss_ce_dn_1: 0.2665  loss_mask_dn_1: 0.02937  loss_dice_dn_1: 0.4257  loss_bbox_dn_1: 0.05835  loss_giou_dn_1: 0.2604  loss_ce_2: 0.7036  loss_mask_2: 0.03697  loss_dice_2: 0.4833  loss_bbox_2: 0.04962  loss_giou_2: 0.2891  loss_ce_dn_2: 0.1816  loss_mask_dn_2: 0.02861  loss_dice_dn_2: 0.3993  loss_bbox_dn_2: 0.03966  loss_giou_dn_2: 0.1942  loss_ce_3: 0.6524  loss_mask_3: 0.03131  loss_dice_3: 0.4822  loss_bbox_3: 0.04655  loss_giou_3: 0.2728  loss_ce_dn_3: 0.1447  loss_mask_dn_3: 0.02841  loss_dice_dn_3: 0.4269  loss_bbox_dn_3: 0.03331  loss_giou_dn_3: 0.1726  loss_ce_4: 0.5893  loss_mask_4: 0.03367  loss_dice_4: 0.4374  loss_bbox_4: 0.04583  loss_giou_4: 0.2831  loss_ce_dn_4: 0.1283  loss_mask_dn_4: 0.02855  loss_dice_dn_4: 0.3788  loss_bbox_dn_4: 0.03154  loss_giou_dn_4: 0.1628  loss_ce_5: 0.5245  loss_mask_5: 0.04142  loss_dice_5: 0.4535  loss_bbox_5: 0.04606  loss_giou_5: 0.2865  loss_ce_dn_5: 0.118  loss_mask_dn_5: 0.02959  loss_dice_dn_5: 0.386  loss_bbox_dn_5: 0.032  loss_giou_dn_5: 0.1593  loss_ce_6: 0.5697  loss_mask_6: 0.0369  loss_dice_6: 0.4744  loss_bbox_6: 0.04796  loss_giou_6: 0.2936  loss_ce_dn_6: 0.1153  loss_mask_dn_6: 0.0285  loss_dice_dn_6: 0.3868  loss_bbox_dn_6: 0.03064  loss_giou_dn_6: 0.1576  loss_ce_7: 0.5176  loss_mask_7: 0.03738  loss_dice_7: 0.449  loss_bbox_7: 0.04798  loss_giou_7: 0.2904  loss_ce_dn_7: 0.1095  loss_mask_dn_7: 0.02963  loss_dice_dn_7: 0.3873  loss_bbox_dn_7: 0.03081  loss_giou_dn_7: 0.1582  loss_ce_8: 0.5114  loss_mask_8: 0.03512  loss_dice_8: 0.4626  loss_bbox_8: 0.04259  loss_giou_8: 0.2891  loss_ce_dn_8: 0.1069  loss_mask_dn_8: 0.02879  loss_dice_dn_8: 0.3873  loss_bbox_dn_8: 0.03058  loss_giou_dn_8: 0.157  loss_ce_interm: 1.002  loss_mask_interm: 0.03154  loss_dice_interm: 0.4466  loss_bbox_interm: 0.06732  loss_giou_interm: 0.3131    time: 0.7910  last_time: 0.8266  data_time: 0.0498  last_data_time: 0.0536   lr: 1e-07  max_mem: 20850M\n",
      "\u001b[32m[07/13 11:40:28 d2.utils.events]: \u001b[0m eta: 0:03:24  iter: 2139  total_loss: 27.23  loss_ce: 0.4376  loss_mask: 0.05848  loss_dice: 0.3833  loss_bbox: 0.03106  loss_giou: 0.1653  loss_ce_dn: 0.1164  loss_mask_dn: 0.04685  loss_dice_dn: 0.3466  loss_bbox_dn: 0.03215  loss_giou_dn: 0.1482  loss_ce_0: 0.9605  loss_mask_0: 0.05394  loss_dice_0: 0.3353  loss_bbox_0: 0.04987  loss_giou_0: 0.2651  loss_ce_dn_0: 1.803  loss_mask_dn_0: 0.4124  loss_dice_dn_0: 2.737  loss_bbox_dn_0: 0.3025  loss_giou_dn_0: 0.8493  loss_ce_1: 0.8032  loss_mask_1: 0.05674  loss_dice_1: 0.3377  loss_bbox_1: 0.03707  loss_giou_1: 0.1998  loss_ce_dn_1: 0.2683  loss_mask_dn_1: 0.05521  loss_dice_dn_1: 0.3716  loss_bbox_dn_1: 0.08015  loss_giou_dn_1: 0.2498  loss_ce_2: 0.6024  loss_mask_2: 0.04559  loss_dice_2: 0.3301  loss_bbox_2: 0.03453  loss_giou_2: 0.1734  loss_ce_dn_2: 0.1874  loss_mask_dn_2: 0.05411  loss_dice_dn_2: 0.3616  loss_bbox_dn_2: 0.0516  loss_giou_dn_2: 0.1867  loss_ce_3: 0.5269  loss_mask_3: 0.0439  loss_dice_3: 0.3461  loss_bbox_3: 0.03347  loss_giou_3: 0.1758  loss_ce_dn_3: 0.1601  loss_mask_dn_3: 0.05037  loss_dice_dn_3: 0.356  loss_bbox_dn_3: 0.04377  loss_giou_dn_3: 0.1629  loss_ce_4: 0.5028  loss_mask_4: 0.0474  loss_dice_4: 0.3455  loss_bbox_4: 0.03225  loss_giou_4: 0.167  loss_ce_dn_4: 0.1436  loss_mask_dn_4: 0.05274  loss_dice_dn_4: 0.3605  loss_bbox_dn_4: 0.03837  loss_giou_dn_4: 0.1511  loss_ce_5: 0.4871  loss_mask_5: 0.05432  loss_dice_5: 0.348  loss_bbox_5: 0.03242  loss_giou_5: 0.1652  loss_ce_dn_5: 0.1328  loss_mask_dn_5: 0.04818  loss_dice_dn_5: 0.3515  loss_bbox_dn_5: 0.03432  loss_giou_dn_5: 0.1497  loss_ce_6: 0.4578  loss_mask_6: 0.05588  loss_dice_6: 0.3582  loss_bbox_6: 0.03272  loss_giou_6: 0.163  loss_ce_dn_6: 0.1247  loss_mask_dn_6: 0.04578  loss_dice_dn_6: 0.346  loss_bbox_dn_6: 0.03228  loss_giou_dn_6: 0.1475  loss_ce_7: 0.4464  loss_mask_7: 0.05333  loss_dice_7: 0.3648  loss_bbox_7: 0.03264  loss_giou_7: 0.1637  loss_ce_dn_7: 0.1192  loss_mask_dn_7: 0.04772  loss_dice_dn_7: 0.3402  loss_bbox_dn_7: 0.03233  loss_giou_dn_7: 0.1485  loss_ce_8: 0.4474  loss_mask_8: 0.05546  loss_dice_8: 0.3424  loss_bbox_8: 0.03257  loss_giou_8: 0.1646  loss_ce_dn_8: 0.1186  loss_mask_dn_8: 0.04769  loss_dice_dn_8: 0.3375  loss_bbox_dn_8: 0.03187  loss_giou_dn_8: 0.1473  loss_ce_interm: 0.9555  loss_mask_interm: 0.05018  loss_dice_interm: 0.3809  loss_bbox_interm: 0.07344  loss_giou_interm: 0.289    time: 0.7910  last_time: 0.7709  data_time: 0.0175  last_data_time: 0.0068   lr: 1e-07  max_mem: 20850M\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[07/13 11:40:44 d2.utils.events]: \u001b[0m eta: 0:03:08  iter: 2159  total_loss: 31.07  loss_ce: 0.5557  loss_mask: 0.04721  loss_dice: 0.3548  loss_bbox: 0.04642  loss_giou: 0.2185  loss_ce_dn: 0.146  loss_mask_dn: 0.03953  loss_dice_dn: 0.399  loss_bbox_dn: 0.03131  loss_giou_dn: 0.1585  loss_ce_0: 0.9197  loss_mask_0: 0.05283  loss_dice_0: 0.4478  loss_bbox_0: 0.0576  loss_giou_0: 0.3352  loss_ce_dn_0: 1.753  loss_mask_dn_0: 0.5189  loss_dice_dn_0: 2.625  loss_bbox_dn_0: 0.3341  loss_giou_dn_0: 0.8559  loss_ce_1: 0.8172  loss_mask_1: 0.04773  loss_dice_1: 0.4306  loss_bbox_1: 0.04938  loss_giou_1: 0.2667  loss_ce_dn_1: 0.2787  loss_mask_dn_1: 0.04339  loss_dice_dn_1: 0.4685  loss_bbox_dn_1: 0.073  loss_giou_dn_1: 0.2742  loss_ce_2: 0.7154  loss_mask_2: 0.04675  loss_dice_2: 0.4357  loss_bbox_2: 0.05106  loss_giou_2: 0.2572  loss_ce_dn_2: 0.2174  loss_mask_dn_2: 0.0453  loss_dice_dn_2: 0.424  loss_bbox_dn_2: 0.04673  loss_giou_dn_2: 0.199  loss_ce_3: 0.6437  loss_mask_3: 0.04395  loss_dice_3: 0.4559  loss_bbox_3: 0.04782  loss_giou_3: 0.2277  loss_ce_dn_3: 0.1863  loss_mask_dn_3: 0.04095  loss_dice_dn_3: 0.4079  loss_bbox_dn_3: 0.038  loss_giou_dn_3: 0.1712  loss_ce_4: 0.6236  loss_mask_4: 0.04597  loss_dice_4: 0.4849  loss_bbox_4: 0.04744  loss_giou_4: 0.2248  loss_ce_dn_4: 0.1662  loss_mask_dn_4: 0.04103  loss_dice_dn_4: 0.3932  loss_bbox_dn_4: 0.0353  loss_giou_dn_4: 0.1679  loss_ce_5: 0.5905  loss_mask_5: 0.0433  loss_dice_5: 0.3939  loss_bbox_5: 0.04676  loss_giou_5: 0.24  loss_ce_dn_5: 0.1521  loss_mask_dn_5: 0.04285  loss_dice_dn_5: 0.3944  loss_bbox_dn_5: 0.03311  loss_giou_dn_5: 0.1612  loss_ce_6: 0.5687  loss_mask_6: 0.04299  loss_dice_6: 0.3948  loss_bbox_6: 0.05286  loss_giou_6: 0.2254  loss_ce_dn_6: 0.1549  loss_mask_dn_6: 0.04103  loss_dice_dn_6: 0.3917  loss_bbox_dn_6: 0.03226  loss_giou_dn_6: 0.1608  loss_ce_7: 0.5372  loss_mask_7: 0.04463  loss_dice_7: 0.4416  loss_bbox_7: 0.04652  loss_giou_7: 0.2264  loss_ce_dn_7: 0.1467  loss_mask_dn_7: 0.03957  loss_dice_dn_7: 0.3919  loss_bbox_dn_7: 0.03201  loss_giou_dn_7: 0.1597  loss_ce_8: 0.523  loss_mask_8: 0.04458  loss_dice_8: 0.3842  loss_bbox_8: 0.0461  loss_giou_8: 0.218  loss_ce_dn_8: 0.146  loss_mask_dn_8: 0.04  loss_dice_dn_8: 0.3932  loss_bbox_dn_8: 0.03118  loss_giou_dn_8: 0.1584  loss_ce_interm: 0.8976  loss_mask_interm: 0.04903  loss_dice_interm: 0.4938  loss_bbox_interm: 0.08796  loss_giou_interm: 0.3593    time: 0.7910  last_time: 0.7817  data_time: 0.0132  last_data_time: 0.0057   lr: 1e-07  max_mem: 20850M\n",
      "\u001b[32m[07/13 11:41:00 d2.utils.events]: \u001b[0m eta: 0:02:52  iter: 2179  total_loss: 27.5  loss_ce: 0.4691  loss_mask: 0.04734  loss_dice: 0.3045  loss_bbox: 0.03604  loss_giou: 0.1617  loss_ce_dn: 0.1098  loss_mask_dn: 0.04048  loss_dice_dn: 0.32  loss_bbox_dn: 0.03192  loss_giou_dn: 0.1415  loss_ce_0: 0.8601  loss_mask_0: 0.04325  loss_dice_0: 0.3763  loss_bbox_0: 0.0585  loss_giou_0: 0.2879  loss_ce_dn_0: 1.888  loss_mask_dn_0: 0.5399  loss_dice_dn_0: 2.829  loss_bbox_dn_0: 0.3331  loss_giou_dn_0: 0.8536  loss_ce_1: 0.7562  loss_mask_1: 0.05347  loss_dice_1: 0.3443  loss_bbox_1: 0.0465  loss_giou_1: 0.2089  loss_ce_dn_1: 0.2889  loss_mask_dn_1: 0.04375  loss_dice_dn_1: 0.3409  loss_bbox_dn_1: 0.0724  loss_giou_dn_1: 0.2561  loss_ce_2: 0.6378  loss_mask_2: 0.04465  loss_dice_2: 0.3462  loss_bbox_2: 0.04119  loss_giou_2: 0.2005  loss_ce_dn_2: 0.2045  loss_mask_dn_2: 0.03979  loss_dice_dn_2: 0.3472  loss_bbox_dn_2: 0.04474  loss_giou_dn_2: 0.1842  loss_ce_3: 0.5816  loss_mask_3: 0.04387  loss_dice_3: 0.3713  loss_bbox_3: 0.04067  loss_giou_3: 0.1838  loss_ce_dn_3: 0.1644  loss_mask_dn_3: 0.03981  loss_dice_dn_3: 0.3271  loss_bbox_dn_3: 0.03935  loss_giou_dn_3: 0.1599  loss_ce_4: 0.5662  loss_mask_4: 0.04115  loss_dice_4: 0.3406  loss_bbox_4: 0.03928  loss_giou_4: 0.1661  loss_ce_dn_4: 0.142  loss_mask_dn_4: 0.03805  loss_dice_dn_4: 0.3192  loss_bbox_dn_4: 0.03666  loss_giou_dn_4: 0.151  loss_ce_5: 0.5444  loss_mask_5: 0.0424  loss_dice_5: 0.3349  loss_bbox_5: 0.03835  loss_giou_5: 0.1641  loss_ce_dn_5: 0.1302  loss_mask_dn_5: 0.04006  loss_dice_dn_5: 0.3067  loss_bbox_dn_5: 0.03426  loss_giou_dn_5: 0.146  loss_ce_6: 0.5408  loss_mask_6: 0.03889  loss_dice_6: 0.3223  loss_bbox_6: 0.03652  loss_giou_6: 0.1629  loss_ce_dn_6: 0.1211  loss_mask_dn_6: 0.03972  loss_dice_dn_6: 0.3027  loss_bbox_dn_6: 0.03195  loss_giou_dn_6: 0.1437  loss_ce_7: 0.5213  loss_mask_7: 0.04133  loss_dice_7: 0.3107  loss_bbox_7: 0.03697  loss_giou_7: 0.1638  loss_ce_dn_7: 0.1101  loss_mask_dn_7: 0.04027  loss_dice_dn_7: 0.3171  loss_bbox_dn_7: 0.0322  loss_giou_dn_7: 0.1432  loss_ce_8: 0.467  loss_mask_8: 0.04549  loss_dice_8: 0.3306  loss_bbox_8: 0.03498  loss_giou_8: 0.1615  loss_ce_dn_8: 0.1088  loss_mask_dn_8: 0.03873  loss_dice_dn_8: 0.3138  loss_bbox_dn_8: 0.03204  loss_giou_dn_8: 0.1414  loss_ce_interm: 0.8878  loss_mask_interm: 0.04425  loss_dice_interm: 0.349  loss_bbox_interm: 0.07628  loss_giou_interm: 0.3098    time: 0.7909  last_time: 0.7797  data_time: 0.0135  last_data_time: 0.0072   lr: 1e-07  max_mem: 21007M\n",
      "\u001b[32m[07/13 11:41:16 d2.utils.events]: \u001b[0m eta: 0:02:36  iter: 2199  total_loss: 26.13  loss_ce: 0.4822  loss_mask: 0.04193  loss_dice: 0.3809  loss_bbox: 0.03532  loss_giou: 0.1569  loss_ce_dn: 0.1154  loss_mask_dn: 0.04084  loss_dice_dn: 0.324  loss_bbox_dn: 0.02852  loss_giou_dn: 0.1425  loss_ce_0: 0.9337  loss_mask_0: 0.04507  loss_dice_0: 0.4087  loss_bbox_0: 0.05781  loss_giou_0: 0.3198  loss_ce_dn_0: 1.806  loss_mask_dn_0: 0.4306  loss_dice_dn_0: 2.925  loss_bbox_dn_0: 0.3242  loss_giou_dn_0: 0.8461  loss_ce_1: 0.7902  loss_mask_1: 0.04173  loss_dice_1: 0.3571  loss_bbox_1: 0.04912  loss_giou_1: 0.2312  loss_ce_dn_1: 0.2534  loss_mask_dn_1: 0.04469  loss_dice_dn_1: 0.3731  loss_bbox_dn_1: 0.06879  loss_giou_dn_1: 0.2573  loss_ce_2: 0.6751  loss_mask_2: 0.0451  loss_dice_2: 0.3999  loss_bbox_2: 0.04426  loss_giou_2: 0.1941  loss_ce_dn_2: 0.182  loss_mask_dn_2: 0.0409  loss_dice_dn_2: 0.3306  loss_bbox_dn_2: 0.0439  loss_giou_dn_2: 0.1766  loss_ce_3: 0.5908  loss_mask_3: 0.0392  loss_dice_3: 0.3619  loss_bbox_3: 0.03779  loss_giou_3: 0.1871  loss_ce_dn_3: 0.1666  loss_mask_dn_3: 0.03903  loss_dice_dn_3: 0.331  loss_bbox_dn_3: 0.03644  loss_giou_dn_3: 0.1558  loss_ce_4: 0.5456  loss_mask_4: 0.0413  loss_dice_4: 0.3686  loss_bbox_4: 0.04216  loss_giou_4: 0.1812  loss_ce_dn_4: 0.1447  loss_mask_dn_4: 0.04126  loss_dice_dn_4: 0.3239  loss_bbox_dn_4: 0.03258  loss_giou_dn_4: 0.148  loss_ce_5: 0.5305  loss_mask_5: 0.04055  loss_dice_5: 0.3731  loss_bbox_5: 0.04219  loss_giou_5: 0.1794  loss_ce_dn_5: 0.1353  loss_mask_dn_5: 0.04025  loss_dice_dn_5: 0.3193  loss_bbox_dn_5: 0.03071  loss_giou_dn_5: 0.1465  loss_ce_6: 0.5045  loss_mask_6: 0.0414  loss_dice_6: 0.3769  loss_bbox_6: 0.03674  loss_giou_6: 0.1573  loss_ce_dn_6: 0.1224  loss_mask_dn_6: 0.04094  loss_dice_dn_6: 0.3453  loss_bbox_dn_6: 0.02945  loss_giou_dn_6: 0.1439  loss_ce_7: 0.5016  loss_mask_7: 0.04183  loss_dice_7: 0.3649  loss_bbox_7: 0.03694  loss_giou_7: 0.158  loss_ce_dn_7: 0.1195  loss_mask_dn_7: 0.04135  loss_dice_dn_7: 0.3343  loss_bbox_dn_7: 0.02838  loss_giou_dn_7: 0.143  loss_ce_8: 0.4925  loss_mask_8: 0.04018  loss_dice_8: 0.3673  loss_bbox_8: 0.03657  loss_giou_8: 0.157  loss_ce_dn_8: 0.1168  loss_mask_dn_8: 0.04062  loss_dice_dn_8: 0.3165  loss_bbox_dn_8: 0.02849  loss_giou_dn_8: 0.1428  loss_ce_interm: 0.9317  loss_mask_interm: 0.04565  loss_dice_interm: 0.388  loss_bbox_interm: 0.06863  loss_giou_interm: 0.2684    time: 0.7909  last_time: 0.7830  data_time: 0.0156  last_data_time: 0.0047   lr: 1e-07  max_mem: 21007M\n",
      "\u001b[32m[07/13 11:41:31 d2.utils.events]: \u001b[0m eta: 0:02:21  iter: 2219  total_loss: 29.14  loss_ce: 0.5177  loss_mask: 0.03594  loss_dice: 0.362  loss_bbox: 0.03717  loss_giou: 0.2085  loss_ce_dn: 0.1248  loss_mask_dn: 0.03914  loss_dice_dn: 0.3611  loss_bbox_dn: 0.03151  loss_giou_dn: 0.1458  loss_ce_0: 0.9904  loss_mask_0: 0.03736  loss_dice_0: 0.3767  loss_bbox_0: 0.06135  loss_giou_0: 0.3297  loss_ce_dn_0: 1.727  loss_mask_dn_0: 0.4791  loss_dice_dn_0: 2.857  loss_bbox_dn_0: 0.3202  loss_giou_dn_0: 0.8547  loss_ce_1: 0.8284  loss_mask_1: 0.03966  loss_dice_1: 0.4102  loss_bbox_1: 0.04851  loss_giou_1: 0.2511  loss_ce_dn_1: 0.274  loss_mask_dn_1: 0.04186  loss_dice_dn_1: 0.4315  loss_bbox_dn_1: 0.06668  loss_giou_dn_1: 0.2667  loss_ce_2: 0.7127  loss_mask_2: 0.03978  loss_dice_2: 0.4175  loss_bbox_2: 0.04399  loss_giou_2: 0.2228  loss_ce_dn_2: 0.1947  loss_mask_dn_2: 0.04365  loss_dice_dn_2: 0.3868  loss_bbox_dn_2: 0.04545  loss_giou_dn_2: 0.1879  loss_ce_3: 0.6107  loss_mask_3: 0.03715  loss_dice_3: 0.3749  loss_bbox_3: 0.04071  loss_giou_3: 0.2198  loss_ce_dn_3: 0.1619  loss_mask_dn_3: 0.0412  loss_dice_dn_3: 0.3828  loss_bbox_dn_3: 0.03896  loss_giou_dn_3: 0.1656  loss_ce_4: 0.5597  loss_mask_4: 0.03809  loss_dice_4: 0.4165  loss_bbox_4: 0.03799  loss_giou_4: 0.2138  loss_ce_dn_4: 0.1397  loss_mask_dn_4: 0.0383  loss_dice_dn_4: 0.3702  loss_bbox_dn_4: 0.0349  loss_giou_dn_4: 0.1532  loss_ce_5: 0.57  loss_mask_5: 0.03572  loss_dice_5: 0.4058  loss_bbox_5: 0.04326  loss_giou_5: 0.2175  loss_ce_dn_5: 0.1338  loss_mask_dn_5: 0.03917  loss_dice_dn_5: 0.3747  loss_bbox_dn_5: 0.03315  loss_giou_dn_5: 0.1507  loss_ce_6: 0.5327  loss_mask_6: 0.04088  loss_dice_6: 0.4021  loss_bbox_6: 0.03784  loss_giou_6: 0.2113  loss_ce_dn_6: 0.1365  loss_mask_dn_6: 0.03939  loss_dice_dn_6: 0.3653  loss_bbox_dn_6: 0.0325  loss_giou_dn_6: 0.1475  loss_ce_7: 0.5202  loss_mask_7: 0.03372  loss_dice_7: 0.3387  loss_bbox_7: 0.03642  loss_giou_7: 0.2071  loss_ce_dn_7: 0.1282  loss_mask_dn_7: 0.03918  loss_dice_dn_7: 0.3586  loss_bbox_dn_7: 0.03274  loss_giou_dn_7: 0.1478  loss_ce_8: 0.5184  loss_mask_8: 0.0378  loss_dice_8: 0.4253  loss_bbox_8: 0.0381  loss_giou_8: 0.2088  loss_ce_dn_8: 0.1243  loss_mask_dn_8: 0.03929  loss_dice_dn_8: 0.3701  loss_bbox_dn_8: 0.03161  loss_giou_dn_8: 0.1457  loss_ce_interm: 0.9955  loss_mask_interm: 0.03868  loss_dice_interm: 0.4306  loss_bbox_interm: 0.07728  loss_giou_interm: 0.2835    time: 0.7909  last_time: 0.7795  data_time: 0.0112  last_data_time: 0.0067   lr: 1e-07  max_mem: 21007M\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[07/13 11:41:47 d2.utils.events]: \u001b[0m eta: 0:02:05  iter: 2239  total_loss: 29.46  loss_ce: 0.5051  loss_mask: 0.05084  loss_dice: 0.3832  loss_bbox: 0.03742  loss_giou: 0.2049  loss_ce_dn: 0.1143  loss_mask_dn: 0.0432  loss_dice_dn: 0.3901  loss_bbox_dn: 0.02567  loss_giou_dn: 0.1624  loss_ce_0: 0.9104  loss_mask_0: 0.04914  loss_dice_0: 0.4205  loss_bbox_0: 0.06299  loss_giou_0: 0.3372  loss_ce_dn_0: 1.874  loss_mask_dn_0: 0.474  loss_dice_dn_0: 2.768  loss_bbox_dn_0: 0.3176  loss_giou_dn_0: 0.8564  loss_ce_1: 0.784  loss_mask_1: 0.04801  loss_dice_1: 0.4218  loss_bbox_1: 0.04958  loss_giou_1: 0.2401  loss_ce_dn_1: 0.2708  loss_mask_dn_1: 0.05419  loss_dice_dn_1: 0.4177  loss_bbox_dn_1: 0.07139  loss_giou_dn_1: 0.2676  loss_ce_2: 0.6458  loss_mask_2: 0.04449  loss_dice_2: 0.3963  loss_bbox_2: 0.04393  loss_giou_2: 0.2185  loss_ce_dn_2: 0.1756  loss_mask_dn_2: 0.04687  loss_dice_dn_2: 0.3944  loss_bbox_dn_2: 0.04166  loss_giou_dn_2: 0.1952  loss_ce_3: 0.5827  loss_mask_3: 0.05264  loss_dice_3: 0.4388  loss_bbox_3: 0.04287  loss_giou_3: 0.2357  loss_ce_dn_3: 0.1463  loss_mask_dn_3: 0.04758  loss_dice_dn_3: 0.3793  loss_bbox_dn_3: 0.0315  loss_giou_dn_3: 0.1799  loss_ce_4: 0.5329  loss_mask_4: 0.05116  loss_dice_4: 0.4383  loss_bbox_4: 0.04458  loss_giou_4: 0.2246  loss_ce_dn_4: 0.1326  loss_mask_dn_4: 0.04766  loss_dice_dn_4: 0.3925  loss_bbox_dn_4: 0.02832  loss_giou_dn_4: 0.175  loss_ce_5: 0.5369  loss_mask_5: 0.04519  loss_dice_5: 0.4171  loss_bbox_5: 0.04625  loss_giou_5: 0.2204  loss_ce_dn_5: 0.1188  loss_mask_dn_5: 0.04608  loss_dice_dn_5: 0.3916  loss_bbox_dn_5: 0.02767  loss_giou_dn_5: 0.1675  loss_ce_6: 0.5204  loss_mask_6: 0.04736  loss_dice_6: 0.4004  loss_bbox_6: 0.0395  loss_giou_6: 0.2083  loss_ce_dn_6: 0.1142  loss_mask_dn_6: 0.04455  loss_dice_dn_6: 0.3911  loss_bbox_dn_6: 0.02602  loss_giou_dn_6: 0.1641  loss_ce_7: 0.5109  loss_mask_7: 0.04816  loss_dice_7: 0.4094  loss_bbox_7: 0.04045  loss_giou_7: 0.2078  loss_ce_dn_7: 0.1178  loss_mask_dn_7: 0.04385  loss_dice_dn_7: 0.3917  loss_bbox_dn_7: 0.02625  loss_giou_dn_7: 0.1654  loss_ce_8: 0.4779  loss_mask_8: 0.05178  loss_dice_8: 0.4346  loss_bbox_8: 0.03961  loss_giou_8: 0.2055  loss_ce_dn_8: 0.1142  loss_mask_dn_8: 0.04366  loss_dice_dn_8: 0.3824  loss_bbox_dn_8: 0.02552  loss_giou_dn_8: 0.1623  loss_ce_interm: 0.9058  loss_mask_interm: 0.05133  loss_dice_interm: 0.3796  loss_bbox_interm: 0.07412  loss_giou_interm: 0.3102    time: 0.7909  last_time: 0.7908  data_time: 0.0134  last_data_time: 0.0087   lr: 1e-07  max_mem: 21007M\n",
      "\u001b[32m[07/13 11:42:03 d2.utils.events]: \u001b[0m eta: 0:01:49  iter: 2259  total_loss: 25.55  loss_ce: 0.3464  loss_mask: 0.04382  loss_dice: 0.2978  loss_bbox: 0.02981  loss_giou: 0.1472  loss_ce_dn: 0.07579  loss_mask_dn: 0.04529  loss_dice_dn: 0.2941  loss_bbox_dn: 0.02957  loss_giou_dn: 0.1325  loss_ce_0: 0.8291  loss_mask_0: 0.04814  loss_dice_0: 0.3173  loss_bbox_0: 0.04912  loss_giou_0: 0.273  loss_ce_dn_0: 1.739  loss_mask_dn_0: 0.4857  loss_dice_dn_0: 2.669  loss_bbox_dn_0: 0.4085  loss_giou_dn_0: 0.8484  loss_ce_1: 0.7019  loss_mask_1: 0.04834  loss_dice_1: 0.3218  loss_bbox_1: 0.04037  loss_giou_1: 0.1809  loss_ce_dn_1: 0.2298  loss_mask_dn_1: 0.04975  loss_dice_dn_1: 0.3065  loss_bbox_dn_1: 0.07236  loss_giou_dn_1: 0.2354  loss_ce_2: 0.5724  loss_mask_2: 0.04647  loss_dice_2: 0.3066  loss_bbox_2: 0.03601  loss_giou_2: 0.1653  loss_ce_dn_2: 0.144  loss_mask_dn_2: 0.05082  loss_dice_dn_2: 0.2965  loss_bbox_dn_2: 0.04317  loss_giou_dn_2: 0.1735  loss_ce_3: 0.4877  loss_mask_3: 0.04652  loss_dice_3: 0.3227  loss_bbox_3: 0.03269  loss_giou_3: 0.1507  loss_ce_dn_3: 0.1144  loss_mask_dn_3: 0.04902  loss_dice_dn_3: 0.3008  loss_bbox_dn_3: 0.03783  loss_giou_dn_3: 0.1499  loss_ce_4: 0.402  loss_mask_4: 0.04538  loss_dice_4: 0.2976  loss_bbox_4: 0.03229  loss_giou_4: 0.1534  loss_ce_dn_4: 0.09707  loss_mask_dn_4: 0.04864  loss_dice_dn_4: 0.2978  loss_bbox_dn_4: 0.03434  loss_giou_dn_4: 0.1388  loss_ce_5: 0.3681  loss_mask_5: 0.0462  loss_dice_5: 0.3347  loss_bbox_5: 0.03059  loss_giou_5: 0.1519  loss_ce_dn_5: 0.08887  loss_mask_dn_5: 0.04847  loss_dice_dn_5: 0.2977  loss_bbox_dn_5: 0.03284  loss_giou_dn_5: 0.1363  loss_ce_6: 0.372  loss_mask_6: 0.04602  loss_dice_6: 0.3143  loss_bbox_6: 0.03009  loss_giou_6: 0.1463  loss_ce_dn_6: 0.08047  loss_mask_dn_6: 0.04805  loss_dice_dn_6: 0.2849  loss_bbox_dn_6: 0.0304  loss_giou_dn_6: 0.1335  loss_ce_7: 0.3574  loss_mask_7: 0.04487  loss_dice_7: 0.3207  loss_bbox_7: 0.03  loss_giou_7: 0.148  loss_ce_dn_7: 0.07716  loss_mask_dn_7: 0.04679  loss_dice_dn_7: 0.2897  loss_bbox_dn_7: 0.03087  loss_giou_dn_7: 0.1336  loss_ce_8: 0.3458  loss_mask_8: 0.04503  loss_dice_8: 0.3208  loss_bbox_8: 0.0323  loss_giou_8: 0.1468  loss_ce_dn_8: 0.0773  loss_mask_dn_8: 0.04695  loss_dice_dn_8: 0.2938  loss_bbox_dn_8: 0.0294  loss_giou_dn_8: 0.133  loss_ce_interm: 0.8271  loss_mask_interm: 0.05079  loss_dice_interm: 0.286  loss_bbox_interm: 0.06638  loss_giou_interm: 0.2315    time: 0.7909  last_time: 0.7831  data_time: 0.0157  last_data_time: 0.0112   lr: 1e-07  max_mem: 21007M\n",
      "\u001b[32m[07/13 11:42:19 d2.utils.events]: \u001b[0m eta: 0:01:34  iter: 2279  total_loss: 26.79  loss_ce: 0.5165  loss_mask: 0.02641  loss_dice: 0.343  loss_bbox: 0.03096  loss_giou: 0.1748  loss_ce_dn: 0.09462  loss_mask_dn: 0.02715  loss_dice_dn: 0.2895  loss_bbox_dn: 0.02312  loss_giou_dn: 0.14  loss_ce_0: 0.9099  loss_mask_0: 0.02977  loss_dice_0: 0.3618  loss_bbox_0: 0.06107  loss_giou_0: 0.2699  loss_ce_dn_0: 1.811  loss_mask_dn_0: 0.3213  loss_dice_dn_0: 2.743  loss_bbox_dn_0: 0.2798  loss_giou_dn_0: 0.8568  loss_ce_1: 0.7657  loss_mask_1: 0.02749  loss_dice_1: 0.3677  loss_bbox_1: 0.04732  loss_giou_1: 0.2187  loss_ce_dn_1: 0.2678  loss_mask_dn_1: 0.02999  loss_dice_dn_1: 0.3615  loss_bbox_dn_1: 0.05154  loss_giou_dn_1: 0.2542  loss_ce_2: 0.6919  loss_mask_2: 0.02599  loss_dice_2: 0.3515  loss_bbox_2: 0.04459  loss_giou_2: 0.1847  loss_ce_dn_2: 0.1818  loss_mask_dn_2: 0.02769  loss_dice_dn_2: 0.3175  loss_bbox_dn_2: 0.03262  loss_giou_dn_2: 0.1869  loss_ce_3: 0.6325  loss_mask_3: 0.02698  loss_dice_3: 0.3205  loss_bbox_3: 0.03728  loss_giou_3: 0.1888  loss_ce_dn_3: 0.1327  loss_mask_dn_3: 0.02707  loss_dice_dn_3: 0.3088  loss_bbox_dn_3: 0.02554  loss_giou_dn_3: 0.156  loss_ce_4: 0.6002  loss_mask_4: 0.02474  loss_dice_4: 0.3568  loss_bbox_4: 0.03571  loss_giou_4: 0.1806  loss_ce_dn_4: 0.115  loss_mask_dn_4: 0.02638  loss_dice_dn_4: 0.3122  loss_bbox_dn_4: 0.02434  loss_giou_dn_4: 0.1436  loss_ce_5: 0.5643  loss_mask_5: 0.02584  loss_dice_5: 0.3408  loss_bbox_5: 0.03728  loss_giou_5: 0.1792  loss_ce_dn_5: 0.1019  loss_mask_dn_5: 0.02619  loss_dice_dn_5: 0.3079  loss_bbox_dn_5: 0.02351  loss_giou_dn_5: 0.1395  loss_ce_6: 0.5325  loss_mask_6: 0.02579  loss_dice_6: 0.3343  loss_bbox_6: 0.036  loss_giou_6: 0.1801  loss_ce_dn_6: 0.09521  loss_mask_dn_6: 0.02669  loss_dice_dn_6: 0.3034  loss_bbox_dn_6: 0.02327  loss_giou_dn_6: 0.1388  loss_ce_7: 0.557  loss_mask_7: 0.02657  loss_dice_7: 0.3363  loss_bbox_7: 0.03889  loss_giou_7: 0.1747  loss_ce_dn_7: 0.09477  loss_mask_dn_7: 0.02678  loss_dice_dn_7: 0.2958  loss_bbox_dn_7: 0.02318  loss_giou_dn_7: 0.1394  loss_ce_8: 0.5174  loss_mask_8: 0.02507  loss_dice_8: 0.3314  loss_bbox_8: 0.03143  loss_giou_8: 0.1747  loss_ce_dn_8: 0.09242  loss_mask_dn_8: 0.02635  loss_dice_dn_8: 0.2911  loss_bbox_dn_8: 0.02291  loss_giou_dn_8: 0.1394  loss_ce_interm: 0.8799  loss_mask_interm: 0.03007  loss_dice_interm: 0.3424  loss_bbox_interm: 0.06347  loss_giou_interm: 0.2684    time: 0.7909  last_time: 0.7929  data_time: 0.0149  last_data_time: 0.0089   lr: 1e-07  max_mem: 21007M\n",
      "\u001b[32m[07/13 11:42:35 d2.utils.events]: \u001b[0m eta: 0:01:18  iter: 2299  total_loss: 29.91  loss_ce: 0.4758  loss_mask: 0.03086  loss_dice: 0.4203  loss_bbox: 0.03657  loss_giou: 0.2017  loss_ce_dn: 0.1066  loss_mask_dn: 0.02675  loss_dice_dn: 0.3931  loss_bbox_dn: 0.02598  loss_giou_dn: 0.1654  loss_ce_0: 0.9513  loss_mask_0: 0.03506  loss_dice_0: 0.4158  loss_bbox_0: 0.05912  loss_giou_0: 0.3381  loss_ce_dn_0: 1.645  loss_mask_dn_0: 0.3359  loss_dice_dn_0: 2.717  loss_bbox_dn_0: 0.3013  loss_giou_dn_0: 0.853  loss_ce_1: 0.8016  loss_mask_1: 0.03664  loss_dice_1: 0.4754  loss_bbox_1: 0.04265  loss_giou_1: 0.2369  loss_ce_dn_1: 0.2567  loss_mask_dn_1: 0.03068  loss_dice_dn_1: 0.4064  loss_bbox_dn_1: 0.06169  loss_giou_dn_1: 0.2686  loss_ce_2: 0.6582  loss_mask_2: 0.04092  loss_dice_2: 0.4207  loss_bbox_2: 0.04078  loss_giou_2: 0.2145  loss_ce_dn_2: 0.1856  loss_mask_dn_2: 0.02863  loss_dice_dn_2: 0.3844  loss_bbox_dn_2: 0.03664  loss_giou_dn_2: 0.2049  loss_ce_3: 0.5853  loss_mask_3: 0.0355  loss_dice_3: 0.4469  loss_bbox_3: 0.03896  loss_giou_3: 0.2057  loss_ce_dn_3: 0.1515  loss_mask_dn_3: 0.02713  loss_dice_dn_3: 0.3835  loss_bbox_dn_3: 0.03011  loss_giou_dn_3: 0.1836  loss_ce_4: 0.5442  loss_mask_4: 0.03461  loss_dice_4: 0.4006  loss_bbox_4: 0.03699  loss_giou_4: 0.2077  loss_ce_dn_4: 0.135  loss_mask_dn_4: 0.02706  loss_dice_dn_4: 0.3813  loss_bbox_dn_4: 0.02829  loss_giou_dn_4: 0.1718  loss_ce_5: 0.5159  loss_mask_5: 0.03265  loss_dice_5: 0.4034  loss_bbox_5: 0.03987  loss_giou_5: 0.2134  loss_ce_dn_5: 0.1237  loss_mask_dn_5: 0.02701  loss_dice_dn_5: 0.3751  loss_bbox_dn_5: 0.02632  loss_giou_dn_5: 0.1676  loss_ce_6: 0.5043  loss_mask_6: 0.0291  loss_dice_6: 0.4275  loss_bbox_6: 0.04102  loss_giou_6: 0.2053  loss_ce_dn_6: 0.1139  loss_mask_dn_6: 0.02711  loss_dice_dn_6: 0.3855  loss_bbox_dn_6: 0.02602  loss_giou_dn_6: 0.1662  loss_ce_7: 0.5002  loss_mask_7: 0.03054  loss_dice_7: 0.3978  loss_bbox_7: 0.04113  loss_giou_7: 0.207  loss_ce_dn_7: 0.1063  loss_mask_dn_7: 0.02727  loss_dice_dn_7: 0.3686  loss_bbox_dn_7: 0.02598  loss_giou_dn_7: 0.1675  loss_ce_8: 0.4638  loss_mask_8: 0.02776  loss_dice_8: 0.4224  loss_bbox_8: 0.03899  loss_giou_8: 0.2079  loss_ce_dn_8: 0.1064  loss_mask_dn_8: 0.02714  loss_dice_dn_8: 0.3912  loss_bbox_dn_8: 0.02581  loss_giou_dn_8: 0.1646  loss_ce_interm: 0.8968  loss_mask_interm: 0.03471  loss_dice_interm: 0.4562  loss_bbox_interm: 0.07396  loss_giou_interm: 0.284    time: 0.7910  last_time: 0.7835  data_time: 0.0162  last_data_time: 0.0110   lr: 1e-07  max_mem: 21007M\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[07/13 11:42:51 d2.utils.events]: \u001b[0m eta: 0:01:02  iter: 2319  total_loss: 30.72  loss_ce: 0.5373  loss_mask: 0.03038  loss_dice: 0.4391  loss_bbox: 0.04873  loss_giou: 0.2592  loss_ce_dn: 0.125  loss_mask_dn: 0.02873  loss_dice_dn: 0.4049  loss_bbox_dn: 0.02482  loss_giou_dn: 0.1723  loss_ce_0: 0.9794  loss_mask_0: 0.02902  loss_dice_0: 0.4785  loss_bbox_0: 0.06858  loss_giou_0: 0.4207  loss_ce_dn_0: 1.931  loss_mask_dn_0: 0.325  loss_dice_dn_0: 2.861  loss_bbox_dn_0: 0.2517  loss_giou_dn_0: 0.8578  loss_ce_1: 0.8569  loss_mask_1: 0.0313  loss_dice_1: 0.4741  loss_bbox_1: 0.05551  loss_giou_1: 0.2961  loss_ce_dn_1: 0.2691  loss_mask_dn_1: 0.02982  loss_dice_dn_1: 0.4245  loss_bbox_dn_1: 0.05456  loss_giou_dn_1: 0.2936  loss_ce_2: 0.7387  loss_mask_2: 0.03088  loss_dice_2: 0.4312  loss_bbox_2: 0.05365  loss_giou_2: 0.2816  loss_ce_dn_2: 0.19  loss_mask_dn_2: 0.02944  loss_dice_dn_2: 0.405  loss_bbox_dn_2: 0.03781  loss_giou_dn_2: 0.2202  loss_ce_3: 0.6518  loss_mask_3: 0.03087  loss_dice_3: 0.4494  loss_bbox_3: 0.05228  loss_giou_3: 0.2585  loss_ce_dn_3: 0.1603  loss_mask_dn_3: 0.02977  loss_dice_dn_3: 0.4126  loss_bbox_dn_3: 0.03016  loss_giou_dn_3: 0.1961  loss_ce_4: 0.6281  loss_mask_4: 0.02799  loss_dice_4: 0.433  loss_bbox_4: 0.04814  loss_giou_4: 0.2525  loss_ce_dn_4: 0.1459  loss_mask_dn_4: 0.02655  loss_dice_dn_4: 0.4073  loss_bbox_dn_4: 0.02854  loss_giou_dn_4: 0.1834  loss_ce_5: 0.5903  loss_mask_5: 0.02697  loss_dice_5: 0.4552  loss_bbox_5: 0.04447  loss_giou_5: 0.2618  loss_ce_dn_5: 0.1377  loss_mask_dn_5: 0.02796  loss_dice_dn_5: 0.3971  loss_bbox_dn_5: 0.02677  loss_giou_dn_5: 0.1773  loss_ce_6: 0.5615  loss_mask_6: 0.0285  loss_dice_6: 0.4487  loss_bbox_6: 0.04572  loss_giou_6: 0.2351  loss_ce_dn_6: 0.1278  loss_mask_dn_6: 0.02783  loss_dice_dn_6: 0.3925  loss_bbox_dn_6: 0.02606  loss_giou_dn_6: 0.1747  loss_ce_7: 0.5709  loss_mask_7: 0.02976  loss_dice_7: 0.4349  loss_bbox_7: 0.0464  loss_giou_7: 0.2386  loss_ce_dn_7: 0.1288  loss_mask_dn_7: 0.02835  loss_dice_dn_7: 0.3832  loss_bbox_dn_7: 0.02567  loss_giou_dn_7: 0.1755  loss_ce_8: 0.5467  loss_mask_8: 0.02795  loss_dice_8: 0.4796  loss_bbox_8: 0.04975  loss_giou_8: 0.2507  loss_ce_dn_8: 0.1283  loss_mask_dn_8: 0.02842  loss_dice_dn_8: 0.3892  loss_bbox_dn_8: 0.02529  loss_giou_dn_8: 0.1708  loss_ce_interm: 0.969  loss_mask_interm: 0.02773  loss_dice_interm: 0.4928  loss_bbox_interm: 0.07133  loss_giou_interm: 0.3455    time: 0.7909  last_time: 0.7773  data_time: 0.0113  last_data_time: 0.0087   lr: 1e-07  max_mem: 21007M\n",
      "\u001b[32m[07/13 11:43:07 d2.utils.events]: \u001b[0m eta: 0:00:47  iter: 2339  total_loss: 31.77  loss_ce: 0.6711  loss_mask: 0.03298  loss_dice: 0.4015  loss_bbox: 0.04354  loss_giou: 0.2348  loss_ce_dn: 0.1009  loss_mask_dn: 0.03215  loss_dice_dn: 0.3869  loss_bbox_dn: 0.02573  loss_giou_dn: 0.1655  loss_ce_0: 1.024  loss_mask_0: 0.03333  loss_dice_0: 0.4332  loss_bbox_0: 0.06946  loss_giou_0: 0.3804  loss_ce_dn_0: 1.717  loss_mask_dn_0: 0.3734  loss_dice_dn_0: 2.893  loss_bbox_dn_0: 0.2852  loss_giou_dn_0: 0.8519  loss_ce_1: 0.8809  loss_mask_1: 0.03457  loss_dice_1: 0.4514  loss_bbox_1: 0.0557  loss_giou_1: 0.2672  loss_ce_dn_1: 0.2367  loss_mask_dn_1: 0.03501  loss_dice_dn_1: 0.3846  loss_bbox_dn_1: 0.05643  loss_giou_dn_1: 0.2752  loss_ce_2: 0.8432  loss_mask_2: 0.03492  loss_dice_2: 0.4553  loss_bbox_2: 0.04874  loss_giou_2: 0.2546  loss_ce_dn_2: 0.1689  loss_mask_dn_2: 0.03457  loss_dice_dn_2: 0.3885  loss_bbox_dn_2: 0.03539  loss_giou_dn_2: 0.2105  loss_ce_3: 0.7569  loss_mask_3: 0.03287  loss_dice_3: 0.4456  loss_bbox_3: 0.04968  loss_giou_3: 0.2568  loss_ce_dn_3: 0.1456  loss_mask_dn_3: 0.03364  loss_dice_dn_3: 0.3945  loss_bbox_dn_3: 0.03027  loss_giou_dn_3: 0.1814  loss_ce_4: 0.7368  loss_mask_4: 0.03266  loss_dice_4: 0.4512  loss_bbox_4: 0.04902  loss_giou_4: 0.237  loss_ce_dn_4: 0.1249  loss_mask_dn_4: 0.03382  loss_dice_dn_4: 0.3729  loss_bbox_dn_4: 0.02859  loss_giou_dn_4: 0.1733  loss_ce_5: 0.7061  loss_mask_5: 0.03201  loss_dice_5: 0.4188  loss_bbox_5: 0.04612  loss_giou_5: 0.2475  loss_ce_dn_5: 0.1179  loss_mask_dn_5: 0.03386  loss_dice_dn_5: 0.3662  loss_bbox_dn_5: 0.02724  loss_giou_dn_5: 0.1716  loss_ce_6: 0.696  loss_mask_6: 0.03231  loss_dice_6: 0.4696  loss_bbox_6: 0.04739  loss_giou_6: 0.2373  loss_ce_dn_6: 0.1054  loss_mask_dn_6: 0.03264  loss_dice_dn_6: 0.3814  loss_bbox_dn_6: 0.02602  loss_giou_dn_6: 0.169  loss_ce_7: 0.6914  loss_mask_7: 0.03284  loss_dice_7: 0.4289  loss_bbox_7: 0.04625  loss_giou_7: 0.2415  loss_ce_dn_7: 0.101  loss_mask_dn_7: 0.0325  loss_dice_dn_7: 0.3724  loss_bbox_dn_7: 0.02639  loss_giou_dn_7: 0.1661  loss_ce_8: 0.6703  loss_mask_8: 0.03137  loss_dice_8: 0.4465  loss_bbox_8: 0.04232  loss_giou_8: 0.227  loss_ce_dn_8: 0.09711  loss_mask_dn_8: 0.03212  loss_dice_dn_8: 0.3857  loss_bbox_dn_8: 0.0256  loss_giou_dn_8: 0.1658  loss_ce_interm: 0.9721  loss_mask_interm: 0.03544  loss_dice_interm: 0.4338  loss_bbox_interm: 0.07003  loss_giou_interm: 0.3412    time: 0.7910  last_time: 0.8427  data_time: 0.0179  last_data_time: 0.0617   lr: 1e-07  max_mem: 21007M\n",
      "\u001b[32m[07/13 11:43:22 d2.utils.events]: \u001b[0m eta: 0:00:31  iter: 2359  total_loss: 31.52  loss_ce: 0.4983  loss_mask: 0.04462  loss_dice: 0.4953  loss_bbox: 0.04239  loss_giou: 0.2457  loss_ce_dn: 0.1311  loss_mask_dn: 0.03906  loss_dice_dn: 0.388  loss_bbox_dn: 0.02685  loss_giou_dn: 0.1754  loss_ce_0: 0.9993  loss_mask_0: 0.0461  loss_dice_0: 0.5185  loss_bbox_0: 0.05132  loss_giou_0: 0.3842  loss_ce_dn_0: 1.803  loss_mask_dn_0: 0.4516  loss_dice_dn_0: 2.692  loss_bbox_dn_0: 0.372  loss_giou_dn_0: 0.8594  loss_ce_1: 0.879  loss_mask_1: 0.04478  loss_dice_1: 0.4758  loss_bbox_1: 0.03964  loss_giou_1: 0.2873  loss_ce_dn_1: 0.2512  loss_mask_dn_1: 0.04044  loss_dice_dn_1: 0.4209  loss_bbox_dn_1: 0.06641  loss_giou_dn_1: 0.2767  loss_ce_2: 0.7238  loss_mask_2: 0.04646  loss_dice_2: 0.4253  loss_bbox_2: 0.04768  loss_giou_2: 0.2708  loss_ce_dn_2: 0.1768  loss_mask_dn_2: 0.03538  loss_dice_dn_2: 0.3734  loss_bbox_dn_2: 0.04129  loss_giou_dn_2: 0.2096  loss_ce_3: 0.6316  loss_mask_3: 0.04994  loss_dice_3: 0.4462  loss_bbox_3: 0.0443  loss_giou_3: 0.2393  loss_ce_dn_3: 0.1529  loss_mask_dn_3: 0.03504  loss_dice_dn_3: 0.3993  loss_bbox_dn_3: 0.03468  loss_giou_dn_3: 0.1953  loss_ce_4: 0.5953  loss_mask_4: 0.05335  loss_dice_4: 0.4481  loss_bbox_4: 0.04322  loss_giou_4: 0.248  loss_ce_dn_4: 0.1489  loss_mask_dn_4: 0.03825  loss_dice_dn_4: 0.3815  loss_bbox_dn_4: 0.02978  loss_giou_dn_4: 0.1819  loss_ce_5: 0.5469  loss_mask_5: 0.04902  loss_dice_5: 0.4223  loss_bbox_5: 0.04285  loss_giou_5: 0.233  loss_ce_dn_5: 0.1333  loss_mask_dn_5: 0.03719  loss_dice_dn_5: 0.371  loss_bbox_dn_5: 0.02846  loss_giou_dn_5: 0.1772  loss_ce_6: 0.5079  loss_mask_6: 0.04433  loss_dice_6: 0.4619  loss_bbox_6: 0.04752  loss_giou_6: 0.2379  loss_ce_dn_6: 0.1278  loss_mask_dn_6: 0.03896  loss_dice_dn_6: 0.3891  loss_bbox_dn_6: 0.02789  loss_giou_dn_6: 0.1749  loss_ce_7: 0.5159  loss_mask_7: 0.04686  loss_dice_7: 0.4405  loss_bbox_7: 0.04425  loss_giou_7: 0.2383  loss_ce_dn_7: 0.1245  loss_mask_dn_7: 0.03778  loss_dice_dn_7: 0.3954  loss_bbox_dn_7: 0.02771  loss_giou_dn_7: 0.1758  loss_ce_8: 0.4858  loss_mask_8: 0.04496  loss_dice_8: 0.4636  loss_bbox_8: 0.04323  loss_giou_8: 0.2572  loss_ce_dn_8: 0.1305  loss_mask_dn_8: 0.03724  loss_dice_dn_8: 0.3836  loss_bbox_dn_8: 0.02663  loss_giou_dn_8: 0.1748  loss_ce_interm: 0.9785  loss_mask_interm: 0.0466  loss_dice_interm: 0.4823  loss_bbox_interm: 0.07241  loss_giou_interm: 0.3404    time: 0.7909  last_time: 0.7822  data_time: 0.0108  last_data_time: 0.0072   lr: 1e-07  max_mem: 21007M\n",
      "\u001b[32m[07/13 11:43:38 d2.utils.events]: \u001b[0m eta: 0:00:15  iter: 2379  total_loss: 31.35  loss_ce: 0.5819  loss_mask: 0.03934  loss_dice: 0.3596  loss_bbox: 0.04058  loss_giou: 0.1945  loss_ce_dn: 0.1303  loss_mask_dn: 0.03521  loss_dice_dn: 0.3134  loss_bbox_dn: 0.02772  loss_giou_dn: 0.1601  loss_ce_0: 1.03  loss_mask_0: 0.0383  loss_dice_0: 0.4201  loss_bbox_0: 0.06369  loss_giou_0: 0.3784  loss_ce_dn_0: 2.017  loss_mask_dn_0: 0.4538  loss_dice_dn_0: 2.921  loss_bbox_dn_0: 0.3052  loss_giou_dn_0: 0.8553  loss_ce_1: 0.9088  loss_mask_1: 0.03848  loss_dice_1: 0.3709  loss_bbox_1: 0.04725  loss_giou_1: 0.2273  loss_ce_dn_1: 0.277  loss_mask_dn_1: 0.04172  loss_dice_dn_1: 0.3424  loss_bbox_dn_1: 0.06634  loss_giou_dn_1: 0.265  loss_ce_2: 0.8097  loss_mask_2: 0.03765  loss_dice_2: 0.4118  loss_bbox_2: 0.0451  loss_giou_2: 0.2358  loss_ce_dn_2: 0.1926  loss_mask_dn_2: 0.0421  loss_dice_dn_2: 0.3174  loss_bbox_dn_2: 0.04112  loss_giou_dn_2: 0.1943  loss_ce_3: 0.7413  loss_mask_3: 0.03964  loss_dice_3: 0.3424  loss_bbox_3: 0.04403  loss_giou_3: 0.2058  loss_ce_dn_3: 0.1713  loss_mask_dn_3: 0.03701  loss_dice_dn_3: 0.3104  loss_bbox_dn_3: 0.03233  loss_giou_dn_3: 0.1723  loss_ce_4: 0.7019  loss_mask_4: 0.03727  loss_dice_4: 0.3517  loss_bbox_4: 0.04252  loss_giou_4: 0.2068  loss_ce_dn_4: 0.1641  loss_mask_dn_4: 0.03465  loss_dice_dn_4: 0.3232  loss_bbox_dn_4: 0.03035  loss_giou_dn_4: 0.1655  loss_ce_5: 0.5989  loss_mask_5: 0.03836  loss_dice_5: 0.3456  loss_bbox_5: 0.04024  loss_giou_5: 0.2048  loss_ce_dn_5: 0.1506  loss_mask_dn_5: 0.03428  loss_dice_dn_5: 0.3217  loss_bbox_dn_5: 0.02843  loss_giou_dn_5: 0.1628  loss_ce_6: 0.5882  loss_mask_6: 0.03738  loss_dice_6: 0.3877  loss_bbox_6: 0.0395  loss_giou_6: 0.2014  loss_ce_dn_6: 0.1443  loss_mask_dn_6: 0.03472  loss_dice_dn_6: 0.3235  loss_bbox_dn_6: 0.02756  loss_giou_dn_6: 0.1605  loss_ce_7: 0.5731  loss_mask_7: 0.03863  loss_dice_7: 0.3439  loss_bbox_7: 0.04159  loss_giou_7: 0.2012  loss_ce_dn_7: 0.1368  loss_mask_dn_7: 0.03478  loss_dice_dn_7: 0.3208  loss_bbox_dn_7: 0.0275  loss_giou_dn_7: 0.1596  loss_ce_8: 0.6101  loss_mask_8: 0.03803  loss_dice_8: 0.3581  loss_bbox_8: 0.03859  loss_giou_8: 0.1967  loss_ce_dn_8: 0.1317  loss_mask_dn_8: 0.03454  loss_dice_dn_8: 0.3214  loss_bbox_dn_8: 0.02754  loss_giou_dn_8: 0.1604  loss_ce_interm: 1.07  loss_mask_interm: 0.03896  loss_dice_interm: 0.4169  loss_bbox_interm: 0.0698  loss_giou_interm: 0.2946    time: 0.7909  last_time: 0.7767  data_time: 0.0217  last_data_time: 0.0068   lr: 1e-07  max_mem: 21007M\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[07/13 11:44:01 d2.utils.events]: \u001b[0m eta: 0:00:00  iter: 2399  total_loss: 25.16  loss_ce: 0.4077  loss_mask: 0.05206  loss_dice: 0.3056  loss_bbox: 0.04354  loss_giou: 0.1606  loss_ce_dn: 0.07873  loss_mask_dn: 0.05005  loss_dice_dn: 0.3178  loss_bbox_dn: 0.03255  loss_giou_dn: 0.1447  loss_ce_0: 0.8513  loss_mask_0: 0.05454  loss_dice_0: 0.3151  loss_bbox_0: 0.0685  loss_giou_0: 0.301  loss_ce_dn_0: 1.843  loss_mask_dn_0: 0.5186  loss_dice_dn_0: 2.667  loss_bbox_dn_0: 0.4041  loss_giou_dn_0: 0.8579  loss_ce_1: 0.7317  loss_mask_1: 0.05406  loss_dice_1: 0.3176  loss_bbox_1: 0.05487  loss_giou_1: 0.2004  loss_ce_dn_1: 0.2595  loss_mask_dn_1: 0.05548  loss_dice_dn_1: 0.3079  loss_bbox_dn_1: 0.09274  loss_giou_dn_1: 0.2534  loss_ce_2: 0.5933  loss_mask_2: 0.04833  loss_dice_2: 0.2975  loss_bbox_2: 0.04899  loss_giou_2: 0.179  loss_ce_dn_2: 0.1685  loss_mask_dn_2: 0.05068  loss_dice_dn_2: 0.3149  loss_bbox_dn_2: 0.04847  loss_giou_dn_2: 0.186  loss_ce_3: 0.4951  loss_mask_3: 0.04786  loss_dice_3: 0.3438  loss_bbox_3: 0.04476  loss_giou_3: 0.1694  loss_ce_dn_3: 0.1213  loss_mask_dn_3: 0.04894  loss_dice_dn_3: 0.3122  loss_bbox_dn_3: 0.04176  loss_giou_dn_3: 0.1628  loss_ce_4: 0.4656  loss_mask_4: 0.05002  loss_dice_4: 0.3215  loss_bbox_4: 0.04519  loss_giou_4: 0.1673  loss_ce_dn_4: 0.1042  loss_mask_dn_4: 0.04926  loss_dice_dn_4: 0.2972  loss_bbox_dn_4: 0.03661  loss_giou_dn_4: 0.1537  loss_ce_5: 0.4269  loss_mask_5: 0.05425  loss_dice_5: 0.2812  loss_bbox_5: 0.04639  loss_giou_5: 0.1521  loss_ce_dn_5: 0.09316  loss_mask_dn_5: 0.05054  loss_dice_dn_5: 0.3283  loss_bbox_dn_5: 0.03446  loss_giou_dn_5: 0.1503  loss_ce_6: 0.4126  loss_mask_6: 0.04964  loss_dice_6: 0.2841  loss_bbox_6: 0.04702  loss_giou_6: 0.1494  loss_ce_dn_6: 0.08424  loss_mask_dn_6: 0.05021  loss_dice_dn_6: 0.3085  loss_bbox_dn_6: 0.0328  loss_giou_dn_6: 0.1469  loss_ce_7: 0.4158  loss_mask_7: 0.04984  loss_dice_7: 0.3203  loss_bbox_7: 0.04492  loss_giou_7: 0.1498  loss_ce_dn_7: 0.08036  loss_mask_dn_7: 0.05011  loss_dice_dn_7: 0.3096  loss_bbox_dn_7: 0.03283  loss_giou_dn_7: 0.1478  loss_ce_8: 0.4183  loss_mask_8: 0.05004  loss_dice_8: 0.3267  loss_bbox_8: 0.04352  loss_giou_8: 0.1615  loss_ce_dn_8: 0.07789  loss_mask_dn_8: 0.05055  loss_dice_dn_8: 0.3204  loss_bbox_dn_8: 0.0324  loss_giou_dn_8: 0.1449  loss_ce_interm: 0.8392  loss_mask_interm: 0.05315  loss_dice_interm: 0.3364  loss_bbox_interm: 0.109  loss_giou_interm: 0.2877    time: 0.7909  last_time: 0.7820  data_time: 0.0116  last_data_time: 0.0090   lr: 1e-07  max_mem: 21007M\n",
      "\u001b[32m[07/13 11:44:01 d2.engine.hooks]: \u001b[0mOverall training speed: 2398 iterations in 0:31:36 (0.7909 s / it)\n",
      "\u001b[32m[07/13 11:44:01 d2.engine.hooks]: \u001b[0mTotal training time: 0:32:27 (0:00:50 on hooks)\n",
      "\u001b[32m[07/13 11:44:03 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
      "\u001b[32m[07/13 11:44:03 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001b[32m[07/13 11:44:03 d2.data.common]: \u001b[0mSerializing 58 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[07/13 11:44:03 d2.data.common]: \u001b[0mSerialized dataset takes 0.14 MiB\n",
      "\u001b[32m[07/13 11:44:03 d2.evaluation.evaluator]: \u001b[0mStart inference on 58 batches\n",
      "\u001b[32m[07/13 11:44:06 d2.evaluation.evaluator]: \u001b[0mInference done 11/58. Dataloading: 0.0009 s/iter. Inference: 0.1701 s/iter. Eval: 0.0975 s/iter. Total: 0.2685 s/iter. ETA=0:00:12\n",
      "\u001b[32m[07/13 11:44:11 d2.evaluation.evaluator]: \u001b[0mInference done 30/58. Dataloading: 0.0010 s/iter. Inference: 0.1699 s/iter. Eval: 0.0976 s/iter. Total: 0.2686 s/iter. ETA=0:00:07\n",
      "\u001b[32m[07/13 11:44:16 d2.evaluation.evaluator]: \u001b[0mInference done 49/58. Dataloading: 0.0010 s/iter. Inference: 0.1699 s/iter. Eval: 0.0975 s/iter. Total: 0.2685 s/iter. ETA=0:00:02\n",
      "\u001b[32m[07/13 11:44:19 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:14.306841 (0.269940 s / iter per device, on 1 devices)\n",
      "\u001b[32m[07/13 11:44:19 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:09 (0.169844 s / iter per device, on 1 devices)\n",
      "\u001b[32m[07/13 11:44:19 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
      "\u001b[32m[07/13 11:44:19 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/inference/coco_instances_results.json\n",
      "\u001b[32m[07/13 11:44:19 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "\u001b[32m[07/13 11:44:19 d2.evaluation.fast_eval_api]: \u001b[0mEvaluate annotation type *bbox*\n",
      "\u001b[32m[07/13 11:44:19 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.evaluate() finished in 0.06 seconds.\n",
      "\u001b[32m[07/13 11:44:19 d2.evaluation.fast_eval_api]: \u001b[0mAccumulating evaluation results...\n",
      "\u001b[32m[07/13 11:44:19 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.accumulate() finished in 0.07 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.809\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.876\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.848\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.598\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.904\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.906\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.799\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.872\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.872\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.687\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.937\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.917\n",
      "\u001b[32m[07/13 11:44:19 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
      "|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl   |\n",
      "|:------:|:------:|:------:|:------:|:------:|:------:|\n",
      "| 80.861 | 87.609 | 84.837 | 59.821 | 90.385 | 90.564 |\n",
      "\u001b[32m[07/13 11:44:19 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
      "| category         | AP     | category       | AP     | category        | AP     |\n",
      "|:-----------------|:-------|:---------------|:-------|:----------------|:-------|\n",
      "| apple            | 87.763 | banana         | 93.321 | baseball        | 72.582 |\n",
      "| cereals          | 81.530 | cheezit        | 84.653 | chocolate_jello | 75.962 |\n",
      "| cleanser         | 78.965 | coffee_grounds | 72.248 | cola            | 87.921 |\n",
      "| couch_table      | 93.564 | dice           | 73.441 | fork            | 82.207 |\n",
      "| iced_tea         | 98.960 | juice_pack     | 88.473 | knife           | 76.570 |\n",
      "| lemon            | 90.148 | milk           | 85.171 | mustard         | 86.360 |\n",
      "| orange           | 68.220 | orange_juice   | 99.345 | peach           | 80.569 |\n",
      "| pear             | 85.445 | plum           | 88.232 | pringles        | 95.199 |\n",
      "| red_wine         | 90.924 | rubiks_cube    | 84.134 | shelf           | 92.244 |\n",
      "| shelf_door       | 60.342 | soccer_ball    | 86.885 | spam            | 77.992 |\n",
      "| sponge           | 75.285 | spoon          | 53.260 | strawberry      | 81.502 |\n",
      "| strawberry_jello | 72.447 | sugar          | 72.762 | tennis_ball     | 56.577 |\n",
      "| tomato_soup      | 54.977 | tropical_juice | 88.344 | tuna            | 79.044 |\n",
      "Loading and preparing results...\n",
      "DONE (t=0.03s)\n",
      "creating index...\n",
      "index created!\n",
      "\u001b[32m[07/13 11:44:19 d2.evaluation.fast_eval_api]: \u001b[0mEvaluate annotation type *segm*\n",
      "\u001b[32m[07/13 11:44:19 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.evaluate() finished in 0.22 seconds.\n",
      "\u001b[32m[07/13 11:44:19 d2.evaluation.fast_eval_api]: \u001b[0mAccumulating evaluation results...\n",
      "\u001b[32m[07/13 11:44:19 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.accumulate() finished in 0.06 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.767\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.869\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.832\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.486\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.892\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.878\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.763\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.839\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.839\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.608\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.918\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.912\n",
      "\u001b[32m[07/13 11:44:19 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for segm: \n",
      "|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl   |\n",
      "|:------:|:------:|:------:|:------:|:------:|:------:|\n",
      "| 76.721 | 86.915 | 83.210 | 48.630 | 89.168 | 87.763 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[07/13 11:44:19 d2.evaluation.coco_evaluation]: \u001b[0mPer-category segm AP: \n",
      "| category         | AP     | category       | AP     | category        | AP     |\n",
      "|:-----------------|:-------|:---------------|:-------|:----------------|:-------|\n",
      "| apple            | 86.040 | banana         | 88.995 | baseball        | 72.414 |\n",
      "| cereals          | 79.675 | cheezit        | 84.653 | chocolate_jello | 75.856 |\n",
      "| cleanser         | 70.902 | coffee_grounds | 79.213 | cola            | 80.089 |\n",
      "| couch_table      | 49.020 | dice           | 64.219 | fork            | 70.438 |\n",
      "| iced_tea         | 98.193 | juice_pack     | 84.684 | knife           | 71.218 |\n",
      "| lemon            | 88.575 | milk           | 83.384 | mustard         | 86.197 |\n",
      "| orange           | 67.089 | orange_juice   | 99.345 | peach           | 74.010 |\n",
      "| pear             | 86.717 | plum           | 84.873 | pringles        | 86.802 |\n",
      "| red_wine         | 89.916 | rubiks_cube    | 82.102 | shelf           | 76.834 |\n",
      "| shelf_door       | 66.597 | soccer_ball    | 87.390 | spam            | 79.738 |\n",
      "| sponge           | 73.294 | spoon          | 43.515 | strawberry      | 78.102 |\n",
      "| strawberry_jello | 67.186 | sugar          | 71.461 | tennis_ball     | 47.784 |\n",
      "| tomato_soup      | 50.520 | tropical_juice | 86.679 | tuna            | 78.413 |\n",
      "\u001b[32m[07/13 11:44:19 d2.engine.defaults]: \u001b[0mEvaluation results for fiftyone_valid in csv format:\n",
      "\u001b[32m[07/13 11:44:19 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
      "\u001b[32m[07/13 11:44:19 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
      "\u001b[32m[07/13 11:44:19 d2.evaluation.testing]: \u001b[0mcopypaste: 80.8607,87.6091,84.8366,59.8215,90.3846,90.5642\n",
      "\u001b[32m[07/13 11:44:19 d2.evaluation.testing]: \u001b[0mcopypaste: Task: segm\n",
      "\u001b[32m[07/13 11:44:19 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
      "\u001b[32m[07/13 11:44:19 d2.evaluation.testing]: \u001b[0mcopypaste: 76.7214,86.9153,83.2101,48.6302,89.1682,87.7632\n"
     ]
    }
   ],
   "source": [
    "# setup and launch the trainer\n",
    "from helpers.model_trainer import Trainer\n",
    "import maskdino\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning) \n",
    "\n",
    "# in case our dataset was changed discard previous cache\n",
    "try:\n",
    "    for item in os.listdir(\"output/inference\"):\n",
    "        os.remove(\"output/inference/\" + item)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "cfg = get_cfg()\n",
    "add_deeplab_config(cfg)\n",
    "maskdino.add_maskdino_config(cfg)\n",
    "if model_type == 'swin':\n",
    "    cfg.merge_from_file('data/robocup_bordeaux_2023/robocup_data/trained_models/conf/swin_config.yaml')\n",
    "    cfg.MODEL.WEIGHTS = \"data/robocup_bordeaux_2023/robocup_data/trained_models/coco_pretrained_maskdino/swin/maskdino_swinl_50ep_300q_hid2048_3sd1_instance_maskenhanced_mask52.3ap_box59.0ap.pth\" \n",
    "    cfg.SOLVER.IMS_PER_BATCH = 4 # batch size\n",
    "elif model_type == 'r50':\n",
    "    cfg.merge_from_file('data/robocup_bordeaux_2023/robocup_data/trained_models/conf/r50_config.yaml')\n",
    "    cfg.MODEL.WEIGHTS = \"data/robocup_bordeaux_2023/robocup_data/trained_models/coco_pretrained_maskdino/resnet/maskdino_r50_50ep_300q_hid2048_3sd1_instance_maskenhanced_mask46.3ap_box51.7ap.pth\"\n",
    "    cfg.SOLVER.IMS_PER_BATCH = 4\n",
    "cfg.DATASETS.TRAIN = ('fiftyone_train',)\n",
    "cfg.DATASETS.TEST = ('fiftyone_valid',)\n",
    "cfg.DATALOADER.NUM_WORKERS = 2\n",
    "cfg.MODEL.SEM_SEG_HEAD.NUM_CLASSES = len(classes)\n",
    "cfg.SOLVER.MAX_ITER = 2400  # corresponds to about 30 minutes of finetuning on an RTX4090\n",
    "cfg.SOLVER.STEPS = (800, 1400, 2000) # when to decay learning rate\n",
    "\n",
    "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
    "TRAIN = True\n",
    "if TRAIN:\n",
    "    trainer = Trainer(cfg)\n",
    "    trainer.resume_or_load(resume=False)\n",
    "    trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914308ad",
   "metadata": {},
   "source": [
    "#### Exporting the Detector\n",
    "\n",
    "The detector is fully described by its model definition in the form of a detectron2 config, its weights after training and a file enumerating the object class labels used in the particular training session. These are gathered and zipped in the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f7e06f3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copy result files...\n",
      "ZIP result files...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# save the model in a format suitable for deployment on the robot\n",
    "# this is just a zip file containing weights, class label descriptions and a model description\n",
    "if TRAIN:\n",
    "    model_path = 'output/model_final.pth'\n",
    "    conf_path = 'data/robocup_bordeaux_2023/robocup_data/trained_models/conf/swin_config.yaml' if model_type == 'swin' else 'data/robocup_bordeaux_2023/robocup_data/trained_models/conf/r50_config.yaml'\n",
    "    classes_path = 'output/classes.json'\n",
    "\n",
    "\n",
    "    time_stamp = get_timestamp()\n",
    "    result_path = 'output/' + experiment_name + '_' + time_stamp\n",
    "\n",
    "    try:\n",
    "        shutil.rmtree(result_path)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    os.mkdir(result_path)\n",
    "\n",
    "    print('Copy result files...')\n",
    "    shutil.copy(model_path, result_path + '/model.pth')\n",
    "    shutil.copy(conf_path, result_path + '/detectron2_config.yaml') \n",
    "    shutil.copy(classes_path, result_path + '/class_labels.json') \n",
    "    print('ZIP result files...')\n",
    "    make_archive(result_path, result_path+'.zip')\n",
    "    print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e79495e",
   "metadata": {},
   "source": [
    "#### Evaluating the Detector\n",
    "\n",
    "To make sure that the detector is well equipped to find objects relevant to the task at hand, its outputs on the validation set are brought back from detectron2 to fiftyone for visualization. In addition, precision, recall and f1-scores are computed class-wise to help identify any issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "33deb801",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "criterion.weight_dict  {'loss_ce': 4.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_bbox': 5.0, 'loss_giou': 2.0, 'loss_ce_interm': 4.0, 'loss_mask_interm': 5.0, 'loss_dice_interm': 5.0, 'loss_bbox_interm': 5.0, 'loss_giou_interm': 2.0, 'loss_ce_dn': 4.0, 'loss_mask_dn': 5.0, 'loss_dice_dn': 5.0, 'loss_bbox_dn': 5.0, 'loss_giou_dn': 2.0, 'loss_ce_interm_dn': 4.0, 'loss_mask_interm_dn': 5.0, 'loss_dice_interm_dn': 5.0, 'loss_bbox_interm_dn': 5.0, 'loss_giou_interm_dn': 2.0, 'loss_ce_0': 4.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_bbox_0': 5.0, 'loss_giou_0': 2.0, 'loss_ce_interm_0': 4.0, 'loss_mask_interm_0': 5.0, 'loss_dice_interm_0': 5.0, 'loss_bbox_interm_0': 5.0, 'loss_giou_interm_0': 2.0, 'loss_ce_dn_0': 4.0, 'loss_mask_dn_0': 5.0, 'loss_dice_dn_0': 5.0, 'loss_bbox_dn_0': 5.0, 'loss_giou_dn_0': 2.0, 'loss_ce_interm_dn_0': 4.0, 'loss_mask_interm_dn_0': 5.0, 'loss_dice_interm_dn_0': 5.0, 'loss_bbox_interm_dn_0': 5.0, 'loss_giou_interm_dn_0': 2.0, 'loss_ce_1': 4.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_bbox_1': 5.0, 'loss_giou_1': 2.0, 'loss_ce_interm_1': 4.0, 'loss_mask_interm_1': 5.0, 'loss_dice_interm_1': 5.0, 'loss_bbox_interm_1': 5.0, 'loss_giou_interm_1': 2.0, 'loss_ce_dn_1': 4.0, 'loss_mask_dn_1': 5.0, 'loss_dice_dn_1': 5.0, 'loss_bbox_dn_1': 5.0, 'loss_giou_dn_1': 2.0, 'loss_ce_interm_dn_1': 4.0, 'loss_mask_interm_dn_1': 5.0, 'loss_dice_interm_dn_1': 5.0, 'loss_bbox_interm_dn_1': 5.0, 'loss_giou_interm_dn_1': 2.0, 'loss_ce_2': 4.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_bbox_2': 5.0, 'loss_giou_2': 2.0, 'loss_ce_interm_2': 4.0, 'loss_mask_interm_2': 5.0, 'loss_dice_interm_2': 5.0, 'loss_bbox_interm_2': 5.0, 'loss_giou_interm_2': 2.0, 'loss_ce_dn_2': 4.0, 'loss_mask_dn_2': 5.0, 'loss_dice_dn_2': 5.0, 'loss_bbox_dn_2': 5.0, 'loss_giou_dn_2': 2.0, 'loss_ce_interm_dn_2': 4.0, 'loss_mask_interm_dn_2': 5.0, 'loss_dice_interm_dn_2': 5.0, 'loss_bbox_interm_dn_2': 5.0, 'loss_giou_interm_dn_2': 2.0, 'loss_ce_3': 4.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_bbox_3': 5.0, 'loss_giou_3': 2.0, 'loss_ce_interm_3': 4.0, 'loss_mask_interm_3': 5.0, 'loss_dice_interm_3': 5.0, 'loss_bbox_interm_3': 5.0, 'loss_giou_interm_3': 2.0, 'loss_ce_dn_3': 4.0, 'loss_mask_dn_3': 5.0, 'loss_dice_dn_3': 5.0, 'loss_bbox_dn_3': 5.0, 'loss_giou_dn_3': 2.0, 'loss_ce_interm_dn_3': 4.0, 'loss_mask_interm_dn_3': 5.0, 'loss_dice_interm_dn_3': 5.0, 'loss_bbox_interm_dn_3': 5.0, 'loss_giou_interm_dn_3': 2.0, 'loss_ce_4': 4.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_bbox_4': 5.0, 'loss_giou_4': 2.0, 'loss_ce_interm_4': 4.0, 'loss_mask_interm_4': 5.0, 'loss_dice_interm_4': 5.0, 'loss_bbox_interm_4': 5.0, 'loss_giou_interm_4': 2.0, 'loss_ce_dn_4': 4.0, 'loss_mask_dn_4': 5.0, 'loss_dice_dn_4': 5.0, 'loss_bbox_dn_4': 5.0, 'loss_giou_dn_4': 2.0, 'loss_ce_interm_dn_4': 4.0, 'loss_mask_interm_dn_4': 5.0, 'loss_dice_interm_dn_4': 5.0, 'loss_bbox_interm_dn_4': 5.0, 'loss_giou_interm_dn_4': 2.0, 'loss_ce_5': 4.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_bbox_5': 5.0, 'loss_giou_5': 2.0, 'loss_ce_interm_5': 4.0, 'loss_mask_interm_5': 5.0, 'loss_dice_interm_5': 5.0, 'loss_bbox_interm_5': 5.0, 'loss_giou_interm_5': 2.0, 'loss_ce_dn_5': 4.0, 'loss_mask_dn_5': 5.0, 'loss_dice_dn_5': 5.0, 'loss_bbox_dn_5': 5.0, 'loss_giou_dn_5': 2.0, 'loss_ce_interm_dn_5': 4.0, 'loss_mask_interm_dn_5': 5.0, 'loss_dice_interm_dn_5': 5.0, 'loss_bbox_interm_dn_5': 5.0, 'loss_giou_interm_dn_5': 2.0, 'loss_ce_6': 4.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_bbox_6': 5.0, 'loss_giou_6': 2.0, 'loss_ce_interm_6': 4.0, 'loss_mask_interm_6': 5.0, 'loss_dice_interm_6': 5.0, 'loss_bbox_interm_6': 5.0, 'loss_giou_interm_6': 2.0, 'loss_ce_dn_6': 4.0, 'loss_mask_dn_6': 5.0, 'loss_dice_dn_6': 5.0, 'loss_bbox_dn_6': 5.0, 'loss_giou_dn_6': 2.0, 'loss_ce_interm_dn_6': 4.0, 'loss_mask_interm_dn_6': 5.0, 'loss_dice_interm_dn_6': 5.0, 'loss_bbox_interm_dn_6': 5.0, 'loss_giou_interm_dn_6': 2.0, 'loss_ce_7': 4.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_bbox_7': 5.0, 'loss_giou_7': 2.0, 'loss_ce_interm_7': 4.0, 'loss_mask_interm_7': 5.0, 'loss_dice_interm_7': 5.0, 'loss_bbox_interm_7': 5.0, 'loss_giou_interm_7': 2.0, 'loss_ce_dn_7': 4.0, 'loss_mask_dn_7': 5.0, 'loss_dice_dn_7': 5.0, 'loss_bbox_dn_7': 5.0, 'loss_giou_dn_7': 2.0, 'loss_ce_interm_dn_7': 4.0, 'loss_mask_interm_dn_7': 5.0, 'loss_dice_interm_dn_7': 5.0, 'loss_bbox_interm_dn_7': 5.0, 'loss_giou_interm_dn_7': 2.0, 'loss_ce_8': 4.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0, 'loss_bbox_8': 5.0, 'loss_giou_8': 2.0, 'loss_ce_interm_8': 4.0, 'loss_mask_interm_8': 5.0, 'loss_dice_interm_8': 5.0, 'loss_bbox_interm_8': 5.0, 'loss_giou_interm_8': 2.0, 'loss_ce_dn_8': 4.0, 'loss_mask_dn_8': 5.0, 'loss_dice_dn_8': 5.0, 'loss_bbox_dn_8': 5.0, 'loss_giou_dn_8': 2.0, 'loss_ce_interm_dn_8': 4.0, 'loss_mask_interm_dn_8': 5.0, 'loss_dice_interm_dn_8': 5.0, 'loss_bbox_interm_dn_8': 5.0, 'loss_giou_interm_dn_8': 2.0}\n",
      "\u001b[32m[07/13 11:44:58 d2.checkpoint.detection_checkpoint]: \u001b[0m[DetectionCheckpointer] Loading from ./output/model_final.pth ...\n"
     ]
    }
   ],
   "source": [
    "# if we just want to load the results of a previous model use that models experiment_name here\n",
    "# else leave model_name at None to use the most recently trained model\n",
    "model_name = None\n",
    "weight_path = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\") if model_name is None else os.path.join(cfg.OUTPUT_DIR, model_name, 'model.pth')\n",
    "cfg.MODEL.WEIGHTS =  weight_path\n",
    "predictor = DefaultPredictor(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "17915646",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bridge back from detectron2 to fiftyone to annotate the validation data with model predictions\n",
    "\n",
    "val_view = valid_dataset_processed\n",
    "dataset_dicts = get_fiftyone_dicts(val_view, labels_dict)\n",
    "predictions = {}\n",
    "for d in dataset_dicts:\n",
    "    img_w = d[\"width\"]\n",
    "    img_h = d[\"height\"]\n",
    "    img = cv2.imread(d[\"file_name\"])\n",
    "    outputs = predictor(img)\n",
    "    detections, instances = detectron_to_fo(outputs, img_w, img_h, classes)\n",
    "    predictions[d[\"image_id\"]] = detections\n",
    "\n",
    "valid_dataset_processed.set_values(\"predictions\", predictions, key_field=\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "61967626",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"800\"\n",
       "            src=\"http://localhost:5151/?notebook=True&subscription=b3df6721-82eb-4052-a4eb-452bab8466f6\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f368d160100>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# launch a fiftyone session to view the model predictions after training\n",
    "session = fo.launch_app(valid_dataset_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e1eafb1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating detections...\n",
      " 100% |███████████████████| 58/58 [4.3s elapsed, 0s remaining, 17.7 samples/s]      \n",
      "Performing IoU sweep...\n",
      " 100% |███████████████████| 58/58 [3.3s elapsed, 0s remaining, 23.5 samples/s]      \n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "           apple       0.85      0.85      0.85        13\n",
      "          banana       1.00      0.91      0.95        11\n",
      "        baseball       0.58      0.70      0.64        10\n",
      "         cereals       0.73      0.79      0.76        34\n",
      "         cheezit       1.00      0.70      0.82        10\n",
      " chocolate_jello       0.80      0.92      0.86        13\n",
      "        cleanser       0.91      0.83      0.87        12\n",
      "  coffee_grounds       0.80      1.00      0.89         8\n",
      "            cola       0.79      1.00      0.88        15\n",
      "     couch_table       1.00      0.60      0.75         5\n",
      "            dice       1.00      1.00      1.00         7\n",
      "            fork       0.64      0.70      0.67        10\n",
      "        iced_tea       1.00      0.94      0.97        16\n",
      "      juice_pack       0.89      0.84      0.86        19\n",
      "           knife       0.71      0.83      0.77        12\n",
      "           lemon       0.83      0.91      0.87        11\n",
      "            milk       0.91      0.88      0.89        33\n",
      "         mustard       1.00      0.90      0.95        10\n",
      "          orange       0.67      0.75      0.71         8\n",
      "    orange_juice       0.75      1.00      0.86        18\n",
      "           peach       1.00      0.80      0.89         5\n",
      "            pear       1.00      1.00      1.00        13\n",
      "            plum       1.00      1.00      1.00        12\n",
      "        pringles       0.91      1.00      0.95        10\n",
      "        red_wine       0.82      0.90      0.86        10\n",
      "     rubiks_cube       1.00      0.75      0.86        12\n",
      "           shelf       1.00      0.95      0.98        22\n",
      "      shelf_door       0.55      0.50      0.52        12\n",
      "     soccer_ball       1.00      0.80      0.89        10\n",
      "            spam       0.59      0.83      0.69        12\n",
      "          sponge       0.93      0.93      0.93        15\n",
      "           spoon       0.53      0.67      0.59        12\n",
      "      strawberry       1.00      1.00      1.00         8\n",
      "strawberry_jello       0.64      0.70      0.67        10\n",
      "           sugar       1.00      0.67      0.80         9\n",
      "     tennis_ball       0.43      0.60      0.50         5\n",
      "     tomato_soup       0.78      0.50      0.61        14\n",
      "  tropical_juice       0.82      0.96      0.89        28\n",
      "            tuna       0.69      0.85      0.76        13\n",
      "\n",
      "       micro avg       0.82      0.85      0.83       507\n",
      "       macro avg       0.83      0.83      0.83       507\n",
      "    weighted avg       0.84      0.85      0.84       507\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# this is just a quick summary of model performance as a final sanity-check before deployment\n",
    "# in addition, the training process above yields validation map scores as a side result\n",
    "results = valid_dataset_processed.evaluate_detections(\n",
    "    \"predictions\",\n",
    "    gt_field=\"segmentations\",\n",
    "    eval_key=\"eval\",\n",
    "    use_masks=True,\n",
    "    compute_mAP=True,\n",
    ")\n",
    "results.print_report()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
